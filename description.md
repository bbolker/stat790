Monday 3:30-5:30, Weds 9:30-10:30.

## Themes

* bias-variance tradeoff/preventing overfitting
   * shrinkage
   * regularization
   * penalization
   * dropouts (NN)
   * constraining tree depth 
   * learning rate (boosting)
   * early stopping
   * priors
   * (double descent?)
* basis construction/feature engineering
   * splines
   * GPs
   * tree splits
   * wavelets
   * Fourier bases
   * neural network architecture
* optimization
   * gradient descent, SGD
   * IRLS
   * Newton and quasi-Newton (BFGS etc.)
   * (E-M)
   * map-reduce
* loss functions
* assessment and diagnostics
   * cross-validation (blocked, etc.)
   * bootstrap
* sparsity
* latent structures (continuous vs discrete, dimensionality, linear vs nonlinear ...)

## Topics

1 Overview: software, bias-variance tradeoff
2 Regression methods (OLS/lasso/ridge)
3 Regression with non-Gaussian outcomes (logistic regression etc.)
4 Basis expansions and regularization
5 Model assessment and selection
6 Tree-based methods (CART/MARS/etc.)
7 Boosting and random forests
8 Gaussian processes
9 Optimization and scalability
10 Latent structures (factor/mixture/graphical models)
11 Simulation-based inference
12 Wrap-up
