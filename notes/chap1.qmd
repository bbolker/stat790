---
title: "Introduction<br>(week 1, part 3)"
reference-location: margin
citation-location: margin
bibliography: ../stat790.bib
date: today
date-format: "D MMM YYYY"
---

\newcommand{\y}{\mathbf y}
\newcommand{\X}{\mathbf X}
\newcommand{\bbeta}{\boldsymbol \beta}

<!-- typically renders in docs/ dir  -->

## books

* ESL and ADA cover very similar material
* both compare linear regression and nearest-neighbour methods as opposite ends of a complexity spectrum
* ESL has more on dimensionality


## Fisher's irises

* Canadian content (irises of the Gasp√© peninsula) [@fisherUse1936]
* Fisher was a eugenist (!) [@bodmeroutstanding2021]
* multiple versions of the data set, with errors ... [@bezdekWill1999]
* alternative: [Palmer penguins dataset](https://allisonhorst.github.io/palmerpenguins/)

## linear models

* can write out as $\hat Y = \hat \beta_0 + \sum X_j \hat \beta_j$
* go almost immediately to $\hat Y = X^{\top} \hat \beta$ or $\left<X, \beta\right>$ or $\mathbf X \beta$
* $\mathbf X$ is the *model matrix* (sometimes "design matrix")
* usually includes an intercept column
* can contain *any* (precomputed) functions of input variables
* input vars (directly measured) $\to$ predictor vars (transformations, basis expansions, etc.)
* 1D examples

## least squares

* choose L2 norm (p-norm = $\left( \sum |x|^p \right)^{1/p}$)
* $\sum_i (Y_i - X_i \beta)^2$
* equivalent to $(\y-\X\beta)^\top (\y-\X\beta)$
* differentiate and solve: $\hat \beta = (\X^\top\X)^{-1} \X^\top \y$
* **hat matrix**: 
$$
\begin{split}
\hat x  = H \y & =  \X \hat \beta \\
 & = \X (\X^\top\X)^{-1} \X^\top \y \\
 H &  = \X (\X^\top\X)^{-1} \X^\top
\end{split}
$$
* regression as a *linear filter*
* cf. explicit expression in ADA 1.52

## regression: nuts and bolts

* never do naive linear algebra!
   * `fortunes::fortune("SLOOOOW")`
* R: QR decomp with Householder rotations
   * see: [lm.c](https://github.com/wch/r-source/blob/79298c499218846d14500255efd622b5021c10ec/src/library/stats/src/lm.c), [dqlrs](https://github.com/wch/r-source/blob/79298c499218846d14500255efd622b5021c10ec/src/appl/dqrls.f), [dqrdc2](https://github.com/wch/r-source/blob/79298c499218846d14500255efd622b5021c10ec/src/appl/dqrdc2.f) ([from the beginning](https://github.com/wch/r-source/blob/2c986d4bcd267202c240ebb4300322d788033303/src/appl.f/dqrdc2.f))
* Julia ???
* will dig into computational details a bit next week (?)

## regression as classification

![fig2.1](../pix/boundary1.png)

* slightly weird
* looseness of "classification" vs "regression"
   * should probably use *discriminant analysis* here
   * or logistic regression

## nearest-neighbor

* $\frac{1}{k} \sum_{x_i \in N_k(x)} y_i$
* also a linear smoother: columns of $\X$ are $1/k \times$ *indicator variables*: $(x_i \in N_k(x))$ and the $\beta$ values are $y_i$
* **hardening** predictions: better to leave as a probability?

## consistency

* if we want *consistency* we need (roughly) the number of observations used to make a prediction to grow (fast enough) with the total sample size $N$
* doesn't hold for fixed $k$
* but will work if $k/n \to 0$ as $k\to \infty, n \to \infty$ (ADA)

## from nearest to $k$-NN

* unlike (this version of) linear regression, complexity is adjustable
* from "nearest neighbor" to "$N$-n.n." (i.e. the mean)
* tuning parameter ($k$) is **discrete** (e.g. awkward for optimization)

**poll**: what are some ways we can modify linear regression to have adjustable complexity?

## from NN to kernel smoothers (ADA, ESL 2.8.2)

* kernel **density estimation** may be familiar
* generalize "nearest neighbor" kernel
* from ADA:
$$
\hat \mu(x) = \sum_i \left( \frac{K(x_i, x)}{\sum_j K(x_j, x)} \right) y_i
$$
* or $K(d(x_i,x)/h)$ where $h$ is the **bandwidth**
* also not consistent unless we let $h \to 0$ and $n \to \infty$ (at the "right" rate)

## dimensionality

* **curse of dimensionality**
* more points are near the edge of a set
* more points are needed to "fill in"/characterize a density
* the mode of a distribution is no longer "typical" in some sense[@gelmantypical2020]
* heuristic: surface to volume ratio of a $p$-ball is $p$^[surprisingly slow!]
* results from ESL: distance from origin to nearest point (of $N$) in dimension $p$
$$
d(p,N) = \left(1- \frac{1}{2}^{1/N} \right)^{1/p}
$$
* Bayesians: the mode is not "typical" @gelmantypical2020

## bias-variance expansion/trade-off

* general expansion of $E[(y-\hat \mu(x))^2]$, expanded as 
$$
E[(\underbrace{(y-\mu(x))}_{\textrm{diff betw y, true RF}}+ \underbrace{(\mu(x)-\hat \mu(x)}_{\textrm{diff betw true RF, chosen approx}})^2
$$
* take expectations, drop 0 terms $\to$ variance + bias$^2$
* allow for variation across training sets, get $\sigma^2 + \mathrm{var} + \mathrm{bias}^2$ (ESL 2.47)
* ADA example: true function (sine) is *worse than a constant function* for noisy data (Fig 1.3). cf. @waltersEffects1981
* bias $\approx$ **within-sample** error; easy to minimize

## the picture

![](../pix/biasvar.png)

* want to find the 'sweet spot' with low computational effort, with minimal assumptions, and without snooping

## effective degrees of freedom

* may be able to compute a complexity measure, for simple cases
* ... such as linear weights
* **trace of the hat matrix**; ADA 1.66-1.68
* related to ESL 2.28

## references
