---
title: "Kernel-based methods"
reference-location: margin
citation-location: margin
bibliography: ../stat790.bib
date: today
date-format: "D MMM YYYY"
---

\newcommand{\A}{\mathbf A}
\newcommand{\B}{\mathbf B}
\newcommand{\D}{\mathbf D}
\newcommand{\GGamma}{\boldsymbol \Gamma}
\newcommand{\G}{\mathbf G}
\newcommand{\HH}{\mathbf H}
\newcommand{\I}{\mathbf I}
\newcommand{\K}{\mathbf K}
\newcommand{\N}{\mathbf N}
\newcommand{\Q}{\mathbf Q}
\newcommand{\R}{\mathbf R}
\newcommand{\bS}{\mathbf S}
\newcommand{\U}{\mathbf U}
\newcommand{\V}{\mathbf V}
\newcommand{\W}{\mathbf W}
\newcommand{\X}{\mathbf X}
\newcommand{\Y}{\mathbf Y}
\newcommand{\Z}{\mathbf Z}
\newcommand{\bbeta}{\boldsymbol \beta}
\newcommand{\bzero}{\boldsymbol 0}
\newcommand{\eeta}{\boldsymbol \eta}
\newcommand{\OOmega}{\boldsymbol \Omega}
\newcommand{\hatmat}{\mathbf H}
\newcommand{\grad}{\mathbf g}
\newcommand{\p}{\mathbf p}
\newcommand{\uu}{\mathbf u}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\z}{\mathbf z}
\newcommand{\kron}{\bigotimes}

<!-- typically renders in docs/ dir  -->

```{r utils, echo = FALSE}
## use help("image-methods", "Matrix")
## lattice graphics: ?lattice:xyplot for details on scales
ifun <- function(x, title = "", ck = FALSE, raster = TRUE) {
    image(Matrix(x),
          sub = "", xlab = "", ylab = "",
          colorkey = ck,
          aspect = "fill",
          scales = list(x = list(draw = FALSE),
                        y = list(draw = FALSE)),
          main = title,
          useRaster = raster
          )
}
```

## Kernel-based methods

* depend only on some *distance function* induced between pairs of points 
   * potentially a high (even infinite-dimensional!) space
   * mapping original values to high dim (e.g. high-order interactions)
* kernel function $k$
   * classification: $\hat y_i(\x') = \textrm{sign} \sum w_i y_i k(\x_i, \x')$
   * regression: (the same but without the \textrm{sign()}!)
* for further efficiency can also use **low-rank approximations** of $k()$

## Kernel smoothers

* kernel density estimation
* Nadaraya-Watson kernel regession

## Separating hyperplanes

* ESL section 4.5
* Regress $\y \in \{-1, 1\}$ on $\x$: solve for $\X \bbeta = 0$  
(write as $\beta_0 + \beta^\top \x = 0$, i.e. separate intercept)
* (equivalent to linear discriminant analysis)
* Rosenblatt's algorithm  
   * $(\X \bbeta)/||\bbeta||$ is the signed distance to the separating plane
   * minimize $-\sum_{i \in {\cal M}} y_i (\X \beta)$ (sum of misclassified distances)
	 * gradient wrt $\beta$ = $-\sum (y_i x_i)$
   * stochastic gradient descent (pointwise): adjust $\bbeta$ by $\rho \y_i X_i$ at each step
* elegant but not practical (non-unique, slow, non-convergent if not separable)
* $\to$ penalized version in a larger basis space
* $\textrm{argmin}(\bbeta)$ of $\frac{1}{2} ||\bbeta||^2$ subject to $y_i (\X \beta) \ge 1$
* "standard" convex optimization problem

![](../pix/sep_hyperplane.png)

   
## support vector machines for the non-separable case

* ESL chapter 12
* $y_i (X_i \bbeta \ge M(1-\xi_i)$
* linear loss function on misclassification distances + L2 penalty
* or $\textrm{min} \frac{1}{2} ||\beta||^2 + C \sum \xi_i$
* $C$ is the hyperparameter
* quadratic programming problem

[](../pix/nonsep_hyper.png)

## SVMs and kernels (ESL 12.3)

* alternative formulation

$$
\begin{split}
f(x_i) & = X_i^\top \beta + \beta_0 \\
 & = \sum \alpha_j y_j \left< h(x_i), h(x_j) \right> + \beta_0
\end{split}
$$
where $\alpha_i$ is a different parameterization

* $\left<h(.), h(.) \right>$ is a **kernel function**
* linear SVM finds a separating hyperplane based on distances
* polynomial distance: $(1 + \left<x_i, x_j \right>)^d$
* polynomial $d$ for $n$ inputs (plus intercept) gives rise to a $C(n+2,d)$-dimensional space
* **radial basis function** $\exp\left(-\gamma ||x_i-x_j||^2\right)$
  * infinite-dimensional (think of Taylor expansion)
  * **length scale** $1/\gamma$
  
## SVMs for regression

* fits a loss function $\max(0, |r|-\epsilon)$

## kernels

* "kernel trick" works very generally, but only for L2 penalty
* ESL 12.3.7: cost of optimizing via kernel is ${\cal O}(N^2)$ 
not ${\cal O}(MN^2)$ (where $N$ is number of training points, $M$ is dimension of the feature space)

## kernel PCA

* @scholkopfKernel1997
* we can do PCA by SVD (works if $p > n$): complexity is ${\cal O}(\min(np^2, n^2p)$ https://mathoverflow.net/a/221216 (`stats:::prcomp.default`)
* or by computing covariance and then computing eigenvectors (only works for $n<p$): ${\cal O} n^3$
* kPCA: map $\Phi: \R^n \to F$
* find $K = \left\langle \Phi(x_i) \Phi(x_j) \right\rangle$
   * never worse than $n^3$, no matter how big the **feature space** is (even infinite)
   * better than linear PCA if $p>n$

## kpca example

```{r kpca, message = FALSE}
library(palmerpenguins)
library(tidyverse)
library(kernlab)
pX <- (penguins
    |> select(where(is.numeric))
    |> select(-year)
    |> as.matrix()
    |> scale()
    |> na.omit()
)
## want species to match with NA-adjusted matrix!
ss <- penguins |> drop_na(where(is.numeric)) |> pull(species)
cc <- adjustcolor(palette()[c(1,2,4)], alpha.f = 0.7)
pfun <- function(k) {
    plot(rotated(k),col= cc[ss], pch = 16,
         xlab = "PC1", ylab = "PC2")
}
k0 <- kpca(pX, kernel = "vanilladot", kpar = list())
pfun(k0)
k1 <- kpca(pX, kernel = "polydot", kpar = list(degree = 2))
pfun(k1)
k2 <- kpca(pX, kernel = "rbfdot", kpar = list(sigma = 0.5))
pfun(k2)
k3 <- kpca(pX, kernel = "rbfdot", kpar = list(sigma = 1))
pfun(k3)
```

## expanded PCA by hand

```{r}
pfun2 <- function(p) {
    plot(p$scores[,1], p$scores[,2], col= cc[ss], pch = 16,
         xlab = "PC1", ylab = "PC2")
}
p0 <- princomp( ~ ., data = as.data.frame(pX))
polyform <- (sprintf("poly(%s, degree = 2)", paste(colnames(pX), collapse = ","))
    |> reformulate()
)
X <- model.matrix(polyform, data = as.data.frame(pX))
p1 <- princomp( polyform, data = as.data.frame(pX))
par(mfrow = c(1, 2)); pfun2(p0); pfun(k0)
par(mfrow = c(1, 2)); pfun2(p1); pfun(k1)
```

[how should one choose a kernel?](https://stats.stackexchange.com/questions/131142/how-to-choose-a-kernel-for-kernel-pca)
   - if kPCA is a step in a pipeline, make the kernel type and parameters (e.g. scale for RBF) part of the tuning process
   
## Gaussian processes

* @rasmussenGaussian2005; @krasserGaussian; @krasserSparse
* motivated by Bayesian context, or from classical **geostatistics** (kriging)
* interpolation vs. approximation

* "Under the assumption of Gaussian observation noise the computations needed to make predictions are tractable and are dominated by the inversion of a n × n matrix."

* zero-mean Gaussian prior: $\w \sim {\cal N}(\bzero, \Sigma_p)$

* $\Sigma_p$? **positive definite** function of distance
   * $\x^\top \Sigma \x > 0$ for all $\x \neq 0$
   * all eigenvalues of $\Sigma$ are positive
* only certain **autocovariance functions** $f(r)$ satisfy this condition for all possible $\x$: RBF, Matérn, ...

## GP prior

$$
\left(
\begin{array}{c}
f \\ f_*
\end{array}
\right)
\sim
{\cal N}
\left(
\bzero,
\left(
\begin{array}{cc}
\K & \K_* \\
\K_*^\top & \K_{**} \\
\end{array}
\right)
\right)
$$

## conditional posterior distribution

$$
\begin{split}
\mu_* & = \K_*^\top \K^{-1} f \\
\Sigma_* & = \K_{**} - \K_*^\top \K^{-1} \K
\end{split}
$$

**kriging equations**

## observation variance/measurement error

* **nugget variance**
* allow smoothing rather than interpolation
* cf SVM without complete separation

## hyperparameters

* variance of Gaussian process
* observation variance 'scale' (residual variance)
    * *could* estimate by cross-validation etc.
    * but have MLE, Bayesian estimates immediately available ...
    * level of uncertainty between interpolating points
	* can estimate directly from MVN MLE as $y^\top C y$
* length scale

![](../pix/gauss_proc2.png)

## kernel shape

* **smoothness**
* RBF is infinitely differentiable
* Matérn functions
   * $\nu = 1/2$, $\exp(-|d|)$
   * $\nu = 3/2$, $(1+\sqrt{3}d) \exp(-\sqrt{3}d)$
   * $\nu = 5/2$, $(1+\sqrt{5}d + 5/3 d^2) \exp(-\sqrt{5}d)$
* otherwise Bessel function $K_\nu$ (ugh)

![](../pix/gauss_proc3.png)

![](../pix/matern2.png)

@jonesMatern2021

## stationarity and isotropy

* **isotropy**: exchangeability of parameters/directions
   * **separable** priors (different variance for each direction)
   * could?? model prior correlation as well)
* **stationarity**: kernel depends only on $||x_i-x_j||$
   * non-stationary: location-dependent, center at mean?
   * or model linear covariates + GP

@paciorekNonstationary2003

## GPs and splines

* ; @rasmussenGaussian2005
* `?mgcv::smooth.construct.gp.smooth.spec`
* @kammannGeoadditive2003
    * choose Matérn with $\nu = 3/2$
	* choose scale == max distance between sample points
	* only need to estimate $\sigma^2_f$
	* fit linear mixed model 

$$
\begin{split}
\Z & = K(||x_i-\kappa_k||/\rho) \\
\OOmega & = K(||\kappa_k -\kappa_k'||/\rho) \\
\y & = \X \bbeta + \Z u + \epsilon
\end{split}
$$

minimize $||\y - \X \beta + \Z \u||_2^2 + ||\Omega^{-1} u||_2^2$

## Bayesian model tuning/hyperparameter tuning

* `tune_bayes`
* https://www.tidymodels.org/learn/work/bayes-opt/

## sparsity: nearest-neighbour


## combining kernels

## packages

`plgp`, `spectralGP`, `GauPro`, `tfprobability`, `mlegp`, ...

sparse GPs: `psgp`


