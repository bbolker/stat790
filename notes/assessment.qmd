---
title: "Model assessment"
reference-location: margin
citation-location: margin
bibliography: ../stat790.bib
date: today
date-format: "D MMM YYYY"
---

\newcommand{\A}{\mathbf A}
\newcommand{\B}{\mathbf B}
\newcommand{\D}{\mathbf D}
\newcommand{\GGamma}{\boldsymbol \Gamma}
\newcommand{\G}{\mathbf G}
\newcommand{\HH}{\mathbf H}
\newcommand{\I}{\mathbf I}
\newcommand{\K}{\mathbf K}
\newcommand{\N}{\mathbf N}
\newcommand{\Q}{\mathbf Q}
\newcommand{\R}{\mathbf R}
\newcommand{\bS}{\mathbf S}
\newcommand{\U}{\mathbf U}
\newcommand{\V}{\mathbf V}
\newcommand{\W}{\mathbf W}
\newcommand{\X}{\mathbf X}
\newcommand{\Y}{\mathbf Y}
\newcommand{\Z}{\mathbf Z}
\newcommand{\bbeta}{\boldsymbol \beta}
\newcommand{\bzero}{\boldsymbol 0}
\newcommand{\eeta}{\boldsymbol \eta}
\newcommand{\OOmega}{\boldsymbol \Omega}
\newcommand{\hatmat}{\mathbf H}
\newcommand{\grad}{\mathbf g}
\newcommand{\p}{\mathbf p}
\newcommand{\uu}{\mathbf u}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\z}{\mathbf z}
\newcommand{\kron}{\bigotimes}

<!-- typically renders in docs/ dir  -->

```{r utils}
## use help("image-methods", "Matrix")
## lattice graphics: ?lattice:xyplot for details on scales
ifun <- function(x, title = "", ck = FALSE, raster = TRUE) {
    image(Matrix(x),
          sub = "", xlab = "", ylab = "",
          colorkey = ck,
          aspect = "fill",
          scales = list(x = list(draw = FALSE),
                        y = list(draw = FALSE)),
          main = title,
          useRaster = raster
          )
}
```

## loss functions (regression/quantitative outcome)

* continuous: L2, L1, **Huber loss**:

```{r}
par(las = 1, bty = "l", lwd = 2)
huber <- function(x, d) ifelse(abs(x)<d, x^2/2, d*abs(x)-d/2)
curve(x^2/2, from = -3, to = 3, ylab = "loss")
curve(abs(x), add = TRUE, col = 2)
curve(huber(x, 1), add = TRUE, col = 4)
legend("top", c("L2", "L1", "Huber"), col = c(1, 2, 4), lty = 1)
```

## loss functions (classification)

* 0-1
* **deviance**: $-2 \sum I(G=k) \log \hat p_k$ = -2 log-likelihood
* deviance generalizes to other distributions
* not a loss function, but worth introducing AUC (area under the curve)
    * may be problematic in terms of implied misclassification costs? [@handMeasuring2009]

![](../pix/auc.png)

## a short rant about loss functions

* 0-1 scoring dichotomizes prematurely
* leads to lots of confusing discussion about balancing data sets
* lots of discussion of what to do about imbalanced data sets (SMOTE etc.) [@vandengoorberghharm2022]
* when **should** we balance?
    * when we have to use 0-1 scoring for some technical reason
	* when we have too **much** data (downsampling, i.e., throw away majority class)
* (cf. discussion of variable selection)


![](../pix/calibration.png)

## 

* test error (generalization error): prediction error over a **fixed** independent sample
* **expected** prediction error: test error averaged over test sets
* training error (within-sample): expectation


## selection vs assessment

## train-validation-test

$$E[f(x_0) - x_0^\top \beta^*]^2 + E[x_0^\top \beta^* - E x_0^\top \hat \beta_\alpha]^2$$

* estimation bias = 0 for linear regression etc., positive for ridge etc.

![](../pix/modelspace.png)


* in-sample error: 
  * $C_p$ = err + 2 $d/N \sigma^2_\epsilon$
  
* leakage:
    * non-independence
	* data-dependence of training

* jackknife, bootstrap etc.

## calibration

