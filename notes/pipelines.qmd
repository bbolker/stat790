---
title: "Pipelines"
reference-location: margin
citation-location: margin
bibliography: ../stat790.bib
date: today
date-format: "D MMM YYYY"
---

\newcommand{\A}{\mathbf A}
\newcommand{\B}{\mathbf B}
\newcommand{\D}{\mathbf D}
\newcommand{\GGamma}{\boldsymbol \Gamma}
\newcommand{\G}{\mathbf G}
\newcommand{\HH}{\mathbf H}
\newcommand{\I}{\mathbf I}
\newcommand{\K}{\mathbf K}
\newcommand{\N}{\mathbf N}
\newcommand{\Q}{\mathbf Q}
\newcommand{\R}{\mathbf R}
\newcommand{\bS}{\mathbf S}
\newcommand{\U}{\mathbf U}
\newcommand{\V}{\mathbf V}
\newcommand{\W}{\mathbf W}
\newcommand{\X}{\mathbf X}
\newcommand{\Y}{\mathbf Y}
\newcommand{\Z}{\mathbf Z}
\newcommand{\bbeta}{\boldsymbol \beta}
\newcommand{\bzero}{\boldsymbol 0}
\newcommand{\eeta}{\boldsymbol \eta}
\newcommand{\OOmega}{\boldsymbol \Omega}
\newcommand{\hatmat}{\mathbf H}
\newcommand{\grad}{\mathbf g}
\newcommand{\p}{\mathbf p}
\newcommand{\uu}{\mathbf u}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\z}{\mathbf z}
\newcommand{\kron}{\bigotimes}

<!-- typically renders in docs/ dir  -->

```{r utils, echo = FALSE}
## use help("image-methods", "Matrix")
## lattice graphics: ?lattice:xyplot for details on scales
ifun <- function(x, title = "", ck = FALSE, raster = TRUE) {
    image(Matrix(x),
          sub = "", xlab = "", ylab = "",
          colorkey = ck,
          aspect = "fill",
          scales = list(x = list(draw = FALSE),
                        y = list(draw = FALSE)),
          main = title,
          useRaster = raster
          )
}
```

## workflow systems

- want to *abstract* details of statistical modeling/machine learning
- benefits of abstraction
    - reduce cognitive load
	- shorter code
- costs of abstraction
    - more 'magic'
    - learning another system
	- harder to dig down for details
	- loss of flexibility/harder to modify in ways not foreseen by designers

## R/python

Materials from [Modeling in R and Python](https://rwward.github.io/etf2021-r-py-modeling/)

- `tidymodels`: meta-package for 'tidy' modeling in R
- `scikit-learn`: modeling in Python

## tidymodels

## `parsnip`

- `parsnip` package (CART → `caret` → "carrot"  → `parsnip`)
- unify modeling interfaces (`lm`, `glmnet`, `randomForest`, etc etc etc)
- specify **model** (algorithm), **mode** (classification or regression), **engine** (implementation/package)
- in principle (???) 

## `rsample`

- resampling, cross-validation, bootstrapping, holdout sets ...
- train/test split (`initial_split()`/`training()`/`testing()`)
- cross-validation (`vfold_cv()`), bootstrap (`bootstrap`)
- blocked/grouped methods! `group_vfold_cv`, `group_bootstraps()`

## `recipes`

- feature engineering
- preprocessing (centering/scaling, imputation, dimension reduction, etc.)

## more 

- `workflows`: bundle preprocessing/modeling/post-processing
- `tune`: hyperparameter tuning
- `yardstick`: assessment

## example

```{r load-pkgs, message=FALSE}
library(tidyverse)
library(tidymodels)
library(glmnet)
```

```{r data-split}
historical <- (read_csv("../code/historical_baseball.csv")
    ## should these be done in the 'prep' process?
    |> mutate(across(inducted, ~fct_rev(factor(.))))
    |> filter(ab > 250)
)
data_split <- initial_split(historical, prop = 2/3, strata = inducted)
train_data <- training(data_split)
testing_data <- testing(data_split)
```

## build preprocessing recipe

- Basics here.  
- Could also do PCA selection, collapse rare factor levels (`step_other()`, other filtering ... (see [recipes docs](https://recipes.tidymodels.org/reference/index.html))

```{r recipe}
b_recipe  <- (
    ## (? why does recipe need data?)
    recipe(inducted ~ ., data = train_data)
    ## no '-' operator in formulas
    ## could use e.g. all_numeric_predictors()
    |> step_rm("last_year")
    ## set player_id to be neither predictor or outcome
    |> update_role(player_id, new_role = "ID")
    ## center, scale, remove zero-variance variables
    |> step_center(all_numeric())
    |> step_scale(all_numeric())
    |> step_nzv(all_numeric())
)
print(b_recipe)
```

## 'prep' step

* Set any *data-dependent* filtering steps based on the full training data set
* Avoid data leakage

```{r prep}
b_prepped <- prep(b_recipe)
print(b_prepped)
```

## 'bake' step (and sampling)

* apply prep to new (maybe) data; sample
* can use `strata` to help balance data, and to avoid data leakage

```{r bake}
b_prepped |> bake(train_data) |> rsample::vfold_cv(v=10)
```

## logistic regression

```{r lrc}
lrc_mod <- (
    logistic_reg(mode = "classification",
                 penalty = tune(),
                 mixture = tune())
    |> set_engine(engine = "glmnet")
)
print(lrc_mod)
```

## digression: experimental design

* sample over a multidimensional space?
* grids (easy, inflexible)
* random samples (too clustered)
* **space-filling**
   * Latin hypercube
   * *Sobol sequences*
   
```{r grids, echo = FALSE}
## grid, random, LHS, Sobol
par(mfrow = c(2,2), mar = c(1,1,3,1))
n <- 400
n2 <- sqrt(n)+2
s <- seq(0, 1, length.out = n2)[-c(1,n2)]
pfun <- function(x, y, lab, ...) {
    plot(x, y, axes = FALSE, ann = FALSE, pch = 16, ...)
    box()
    mtext(side = 3, at = 0.5, lab, line=1, cex  = 2)
}
pfun(rep(s, sqrt(n)), rep(s, each = sqrt(n)), "grid")
pfun(runif(n), runif(n), "random")
## lhs
s2 <- seq(0, 1, length.out = n+2)[-c(1,n+2)]
pfun(s2, sample(s2), "Latin hypercube")
ss <- randtoolbox::sobol(n, dim = 2)
pfun(ss[,1], ss[,2],
     "quasirandom (Sobol)",
     col = viridisLite::viridis(n))
```

```{r parallel}
doMC::registerDoMC(cores = 4)
```

```{r tune_grid, cache=TRUE}
tt <- tune_grid(
    grid = 100,
    object = lrc_mod,
    preprocessor = b_prepped,
    resamples = vfold_cv(train_data),
    metrics   = metric_set(mn_log_loss)
    ## , control = control_grid(verbose = TRUE)
)
saveRDS(tt, "tune_grid.rds")
```

```{r metrics}
## readRDS(tt)
cc <- collect_metrics(tt)
gg0 <- ggplot(cc, aes(penalty, mixture)) + geom_point() + scale_x_log10()
sfun <- function(x, log = FALSE, n = 61) {
    if (log) x <- log(x)
    s <- seq(min(x), max(x), length.out = n)
    if (log) exp(s) else s
}
dd <- with(cc,
           expand.grid(penalty = sfun(penalty, TRUE),
                       mixture = sfun(mixture))
           )
if (FALSE) gg0 %+% dd
m1 <- mgcv::gam(mean ~ te(penalty, mixture), data = cc)
dd$pred <- as.numeric(predict(m1, newdata = dd))
gg1 <- ggplot(dd, aes(penalty, mixture)) + 
    scale_fill_viridis_c() +
    geom_tile(aes(fill = pred)) +
    geom_contour(aes(z = pred), colour = "red") +
    scale_x_log10() +
    geom_point(data = cc, colour = "cyan")
print(gg1)
```

## sanity check

```{r glmnet_check, cache=TRUE}
c1 <- cv.glmnet(y = train_data$inducted,
                x = model.matrix(~ . - inducted -player_id, train_data),
                family = binomial(),
                relax = TRUE,
                data = train_data,
                parallel = TRUE)
```

```{r glmnet_plot}
plot(c1)
```

## tangent: testing the `mn_log_loss` rule

```{r mn_log_loss}
data(iris)
svm.model <- e1071::svm(Species ~ ., data = iris, probability = TRUE)
pred <- predict(svm.model, iris, probability = TRUE)
prob <- attr(pred, "probabilities")
spmat <- matrix(0, ncol = length(levels(iris$Species)),
                nrow = length(iris$Species))
spmat[cbind(1:nrow(prob), as.numeric(iris$Species))] <- 1
-1 * mean(sapply(1:nrow(prob),
                 \(i) dmultinom(x = spmat[i,], size = 1, prob[i,], log = TRUE)))
yardstick::mn_log_loss_vec(truth = iris$Species, estimate = prob)
```

## conclusions?

(select final model)

## Python

initial 
```{python split}
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
historical = pd.read_csv("../code/historical_baseball.csv").query("ab>250")

## index var
historical_pidindex = historical.set_index('player_id')
X = historical_pidindex.drop(['inducted', 'last_year'], axis = 1)
y = historical_pidindex.inducted
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size = 1/3)
```

```{python imports}
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold
```

```{python setup}
pipe_scale_lr_lasso = make_pipeline(StandardScaler(),
                                    VarianceThreshold(),
                                    LogisticRegressionCV(Cs = 10,
                                                         penalty = "elasticnet",
                                                         solver = "saga",
                                                         scoring = "neg_log_loss",
                                                         l1_ratios = np.linspace(0, 1, 6),
                                                         cv = 10,
                                                         max_iter = 2000,
                                                         n_jobs = 4))
```

```{python fit, cache = TRUE}
pipe_scale_lr_lasso.fit(X_train, y_train)  # apply scaling on training data
```

```{python output}
coefs = pipe_scale_lr_lasso.named_steps['logisticregressioncv'].coef_
coef_summary = pd.DataFrame(coefs.transpose(), columns = ['coefs'], index = X_train.columns)
print(coef_summary)
pipe_scale_lr_lasso.score(X_test, y_test)
```

**to do**: 

* selected penalty, mixture parameters? (expect v. unstable)
* predictions?
* uncertainty of predictions?
