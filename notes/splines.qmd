---
title: "Splines and basis expansion (week 3?)"
reference-location: margin
citation-location: margin
bibliography: ../stat790.bib
date: today
date-format: "D MMM YYYY"
---

\newcommand{\A}{\mathbf A}
\newcommand{\B}{\mathbf B}
\newcommand{\D}{\mathbf D}
\newcommand{\GGamma}{\boldsymbol \Gamma}
\newcommand{\G}{\mathbf G}
\newcommand{\HH}{\mathbf H}
\newcommand{\I}{\mathbf I}
\newcommand{\K}{\mathbf K}
\newcommand{\N}{\mathbf N}
\newcommand{\Q}{\mathbf Q}
\newcommand{\R}{\mathbf R}
\newcommand{\bS}{\mathbf S}
\newcommand{\U}{\mathbf U}
\newcommand{\V}{\mathbf V}
\newcommand{\W}{\mathbf W}
\newcommand{\X}{\mathbf X}
\newcommand{\Y}{\mathbf Y}
\newcommand{\Z}{\mathbf Z}
\newcommand{\bbeta}{\boldsymbol \beta}
\newcommand{\bzero}{\boldsymbol 0}
\newcommand{\eeta}{\boldsymbol \eta}
\newcommand{\OOmega}{\boldsymbol \Omega}
\newcommand{\hatmat}{\mathbf H}
\newcommand{\grad}{\mathbf g}
\newcommand{\p}{\mathbf p}
\newcommand{\uu}{\mathbf u}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\z}{\mathbf z}
\newcommand{\kron}{\bigotimes}

<!-- typically renders in docs/ dir  -->

```{r utils}
## use help("image-methods", "Matrix")
## lattice graphics: ?lattice:xyplot for details on scales
ifun <- function(x, title = "", ck = FALSE, raster = TRUE) {
    image(Matrix(x),
          sub = "", xlab = "", ylab = "",
          colorkey = ck,
          aspect = "fill",
          scales = list(x = list(draw = FALSE),
                        y = list(draw = FALSE)),
          main = title,
          useRaster = raster
          )
}
```

## linear basis expansion

* transformations of various kinds 
* quadratic expansion
* nonlinear transformations
* indicator variables

Select or regularize from the expanded set.

## polynomial basis

* polynomial basis: $y_i = \sum_{j=0}^n \beta_j x_i^j$

```{r wood_poly}
## replicate figure 3.2 from Wood
library(ggplot2)
x <- seq(0, 1, length = 101)
n <- 4
y <- sapply(0:n, \(j) x^j)
beta <- c(4.31, -10.72, 16.8, 2.22, -10.88)
y <- cbind(y, fx = y %*% beta)
dimnames(y) <- list(x = x, j = c(0:n, "f(x)"))
yy <- as.data.frame(as.table(y))
yy$x <- as.numeric(as.character(yy$x))
ggplot(yy, aes(x, Freq)) + geom_line() + facet_wrap(~j, scale = "free")
```

## piecewise polynomial bases

* constant, linear, continuous
* basis functions
* translate from $x_i$ to columns of $\X$

## splines

* **piecewise** polynomials with continuity/smoothness constraints
* very useful for function approximation
* convert a single numeric predictor into a flexible basis
* efficient
* with multiple predictors, consider **additive models**
* handle interactions (multidim smooth surfaces) *if reasonably low-dimensional*: tensor products etc.

## spline terminology

* **knots**: breakpoints (boundary, interior)
* order-M (ESL): continuous derivatives up to order $M-2$ (cubic, $M=4$)
* typically $M=1$, 2, 4
* number of knots = df (degrees of freedom) -1 -intercept

## truncated power basis

* $X^0 \ldots X^{n}$
* remaining columns are $(x-\xi_\ell)+^{M-1}$ where $\ell$ are the *interior knots*

## truncated power basis

* **Kronecker product**: blockwise multiplication ($\A \kron \B$ multiplies $\B$ by each $a_{ij}$)
* **Khatri-Rao product**: columnwise Kronecker product
   * super-handy for combining indicator variables with 


```{r truncpolyspline}
truncpolyspline <- function(x, df) {
    if (!require("Matrix")) stop("need Matrix package")
    knots <- quantile(x, seq(0, 1, length = df - 1))
    ## should probably use seq() instead of `:`
    ## dim: n x (df-2)
    trunc_fun <- function(k) (x>=k)*(x-k)^3
    S <- sapply(knots[1:(df-2)], trunc_fun)
    S <- as(S, "CsparseMatrix")
    ## dim: n x df
    S <- cbind(x, x^2, S)
    return(S)
}
xvec <- seq(0, 1, length = 101)
tS <- truncpolyspline(xvec, df = 7)
image(tS, aspect = "fill")
matplot(scale(tS), type = "l")
```

```{r truncpoly}
set.seed(101)
beta <- rnorm(8)
plot(xvec, cbind(1, tS) %*% beta)
```

Alternatively: create directly in sparse format.

- `d`: numeric (double-precision), vs `l` (logical), `n` (position)
- `g`: general, vs. `t` (triangular), `s` (symmetric)
- `C`: compressed sparse column form, vs. `R` (row form); `T` (triplet form)

## compressed sparse column form

- `@i`: vector of row-**i**ndices (0-indexed??)
- `@p`: (0-indexed) vector of **p**ointers to starting elements of each column
- `@x`: values

e.g. `M@i[M@p[4]+1]` is the row-index of the first non-zero element in the fourth column;
`M@x[M@p[4]+1]` is the value of the first non-zero element in the fourth column

```{r tps2}
truncpolyspline2 <- function(x, df) {
    knots <- quantile(x, seq(0, 1, length = df - 1))
    nx <- length(x)
    iL <- list()
    pL <- list()
    xL <- list()
    n <- 3
    j <- 0
    pL[[1]] <- 0L
    for (i in 1:(n-1)) {
        j <- j+1
        xL[[j]] <- x^i
        iL[[j]] <- seq(nx)-1L
        pL[[j+1]] <- i*nx
    }
    for  (i in 1:(df-2)) {
        j <- j+1
        ## figure out number of non-zero elements
        ## (could squeeze out a bit more here by counting up)
        nzk <- sum(x < knots[i])
        pos <- (nzk+1):nx
        xL[[j]] <- (x[pos]-knots[i])^3
        iL[[j]] <- pos-1L
        pL[[j+1]] <- pL[[length(pL)]]+(nx-nzk)
    }
    new("dgCMatrix", i = unlist(iL), p = unlist(pL), x = unlist(xL),
        Dim = c(nx, as.integer(df)))
}
tS2 <- truncpolyspline2(xvec, df = 7)
all.equal(unname(tS), tS2) ## TRUE
identical(unname(matrix(tS)), matrix(tS2))  ## TRUE
```

## B-spline basis

* splines of a given order with *minimal support* (i.e., local)
* basis functions defined by recursion (not pretty)
* convenient for regression splines (see below)

## natural cubic splines

* linear constraints beyond boundary knots (so 2d and 3d derivatives are 0 at the boundaries)

```{r spline_knots}
library(splines)
bb <- bs(1:20, df = 5)
attributes(bb)[c("degree", "knots", "Boundary.knots")]
nn <- ns(1:20, df = 7)
attributes(nn)[c("degree", "knots", "Boundary.knots")]
```

```{r spfig1, fig.width = 8}
par(mfrow = c(1,2),las =1, bty ="l")
matplot(ns(1:20, df = 5), type = "l", main = "natural spline")
matplot(bs(1:20, df = 5), type = "l", main = "B-spline")
```

## variance of predictions

* suppose $\V$ = variance of coefficient $\beta$
* then covariance matrix of predictions $\X \beta$ is $\X \V \X^\top$
* Variance of predictions, brute force: $\textrm{Diag}(\X \V \X^\top)$
* Clever: compute diagonal directly 
    * `emulator::quad.diag` (`colSums(crossprod(M, Conj(tx)) * tx)`)

## sparsity patterns

```{r}
library(Matrix)
dd <- data.frame(x=1:200)
Xb <- model.matrix(~splines::bs(x, df = 10), data = dd)
Xn <- model.matrix(~splines::ns(x, df = 10), data = dd)
gridExtra::grid.arrange(
               ncol = 2,
               ifun(Xb, "b-spline"),
               ifun(Xn, "natural spline")
               )
```

## examples: South African heart disease

* use splines in a GLM with no additional effort
* fit splines to all continuous variables
* ESL says "use four natural spline bases" (... elements??)
* i.e. `df = 4` (no intercept)
* stepwise deletion via AIC
* why??
* showing p-values (why???)
* `stepAIC(..., direction = "backward")`

## phoneme example

* combination of feature transformation (time to Fourier domain) and regularization
* smooth first, regress afterwards

## smoothing splines

* as many knots as data points
* plus squared-second-derivative ("wiggliness") penalty

$$
\textrm{RSS} + \lambda \int (f''(t))^2 \, dt
$$
* defined on an infinite-dimensional space
* minimizer is a natural cubic spline with knots at $x_i$

$$
(\y - \N  \theta)^\top (\y - \N \theta) + \lambda \theta^\top \OOmega_N \theta
$$
with $\{\OOmega_N\}_{jk} = \int N_j''(t) N_k''(t) \, dt$
$$
**generalized** ridge regression: penalize by $\lambda \OOmega_N$ rather than $\lambda I$
* same data augmentation methods as before except that now we use $\sqrt{\lambda} C$ where $C$ is a matrix, and the "square root" of $\OOmega_N$

See @woodGeneralized2017, @perperogloureview2019a

## constructing smoothing penalties

* @eilersFlexible1996: use (products of) finite differences as approximation to squared second derivative penalty

```{r showpen}
library(splines)
b <- bs(1:20, df = 20)
m <- diag(-2, nrow = 20)
m[row(m) == col(m) -1 ] <- 1
m[row(m) == col(m) +1 ] <- 1
gridExtra::grid.arrange(
              nrow = 1,
              ifun(b, "b-spline"),
              ifun(m, "differences"),
              ifun(m %*% b, "penalty")
           )
```

Eilers and Marx:

> This system has a banded structure because of the limited overlap of the B-splines. It is seldom worth the trouble to exploit this special structure, as the number of equations is equal to the number of splines, which is generally moderate (10–20).

@woodPsplines2017: it's not that hard (if you're Simon Wood!) to generate the banded matrix $\OOmega$ from the derivatives of the B-spline basis 

```{r mgcv, message = FALSE}
library(mgcv)
set.seed(101)
x <- sort(runif(30))
sm <- smoothCon(s(x, bs = "bs", k = 10), data.frame(x=x))
names(sm[[1]])
gridExtra::grid.arrange(
               nrow = 1,
               ifun(sm[[1]]$X, "X"),
               ifun(sm[[1]]$S[[1]], "smoother"),
               ifun(sm[[1]]$D[[1]], "chol(smoother)")
               )
```

## degrees of freedom and smoother matrix

- The equivalent of the hat matrix is 

$$
\N(\N^\top \N + \lambda \OOmega_N)^{-1} \N^\top
$$

* Also called the **smoother matrix**
* hat matrix is *idempotent* (why?), smoother matrix is *shrinking*
* smoother matrix has lower rank
* effective degrees of freedom = trace of hat matrix (again)
* can write as $(\I + \lambda \K)^{-1}$
* similarly to ridge regression, eigenvectors are shrunk by a factor $1/(1+\lambda d_k)$ where $d_k$ s an eigenvector of $\K$.
 
## reduced-rank splines

* We *can* use as many knots as observations, but do we really need to?
* From `?mgcv::s`:

> ... exact choice of ‘k’ is not generally critical: it should be 
chosen to be large enough that you are reasonably sure of having
enough degrees of freedom to represent the underlying 'truth'
reasonably well, but small enough to maintain reasonable
computational efficiency. Clearly 'large' and 'small' are
dependent on the particular problem being addressed.

> The default basis dimension, ‘k’, is the larger of 10 and ‘m[1]’ (spline order)

## fitting additive models (more than one smooth term): backfitting

@hastieGeneralized1987a

* scatterplot smoother ($S()$: **any** smoothing method, e.g. local linear or kernel estimation)
* **backfitting**
   - take partial residuals: $r_{ij} = y_i - \hat \alpha - \sum_{k \neq j} \hat f_k(X_{ki})$
   - smooth them ($\hat f_j(x_{ji}) = S(r_j|x_{ji})$)
* either do a *cyclic* fit (backfitting), fitting on partial residuals each time
* can do *semiparametric* fitting (some regular linear terms, some smoothed terms)
* **local scoring** (the same as an IRLS step)

$$
\begin{split}
\hat \eta &  = \hat \alpha + \sum \hat f_j \\
\hat \mu & = g^{-1}(\hat \eta) \\
\z & = \hat \eta + (y-\mu)/V(\mu) \\
\w & = \textrm{Diag}(1/V(\mu))
\end{split}
$$

Now do back-fitting (instead of weighted least squares) on $\z$ with weights $\w$

## historical note

* Backfitting was developed in the context of **alternating conditional expectations** [@breimanEstimating1985]; finding optimal transformations for the response and each of the variables in a multivariate regression.
* Univariate ACE is like backfitting but alternating between transformations for $y$ and $x$ rather than among the different predictor variables
* `acepack` package for R

## fitting additive models: alternative

* stuff the whole thing into one giant GLM with appropriate penalization

## generalized cross-validation

@larsenGAM2015, @golubGeneralized1979

* minimize $\textrm{RSS}/(\textrm{Tr}(\I-\bS(\lambda)))^2$
* "a rotation-invariant version of PRESS" ($\sum (e_i/(1-h_ii))^2$)
* replace RSS with approximation of deviance,
$$
|| \sqrt{\W} (\z - \X \bbeta)||^2
$$
for generalized (non-Gaussian) models
* also very close to AIC
* minimize in **outer loop**
   * can find $\partial \beta/\partial(\log \lambda)$ and the derivative of the trace of the hat matrix by algebra (Wood 1st ed. $\S$4.7.1) and use Newton or quasi-Newton to optimize GCV

## REML criterion

* @reissSmoothing2009, @woodFast2011
* less likely to overfit than GCV
* reduced tendency to multiple minima

![](../pix/wood_reml_fig1.png)

## computing the REML criterion

* treat spline smoothing as a *mixed model* problem
* spline (penalized) parameters are $\uu$
* $y|u \sim N(\X\bbeta + \Z \u, \sigma^2 \I)$; $\u \sim N(0, (\sigma^2/\lambda) \W^{-1})$
* where the $\W$ is the penalty matrix
* corresponds to minimizing $||\y - \X\bbeta - \Z \u||^2 + \lambda \u^\top \W \u$
* "fixed effects are viewed as random effects with improper uniform priors and are integrated out" (Wood 2011)
* **Laplace approximation**: may be imprecise for binary or low-mean Poisson data ...


## multidimensional splines

## tensor product

- $g_{jk}(X) = \sum j h_{1j}(X_1) h_{2k}(X_2)$
- if we have a model matrix written out as `expand.grid(xvec, yvec)`
```{r te}
dd <- expand.grid(x = 1:20, y = 1:20)
sm <- smoothCon(te(x, y, k = c(5,5), bs = "bs"), data = dd) 
gridExtra::grid.arrange(
               nrow = 1,
               ifun(sm[[1]]$X, "X"),
               ifun(sm[[1]]$S[[1]], "penalty")
               )
```

* Tensor product $\X$ is the Kronecker product of the two bases, unpacked properly (ugh)

## thin-plate splines

* @woodThin2003
* tensor product smooth
* Impose a wiggliness penalty on the **cross-second-derivatives**
* $\int \partial^2 f/\partial x_1^2 +  \partial^2 f/\partial x_2^2 + \partial^2 f/\partial x_1 x_2$
* results in an **isotropic** smooth (may or may not be desirable)
* full thin-plate spline gets big fast
* **reduced-rank** TPRS: **Lanczos iteration** to find the rank-$k$ truncated eigendecomposition of a symmetric matrix in ${\cal O}(k n^2)$ operations


> The default basis dimension for this class is ‘k=M+k.def’ where ‘M’ is the null space dimension (dimension of unpenalized function space) and ‘k.def’ is 8 for dimension 1, 27 for dimension 2 and 100 for higher dimensions.  This is essentially arbitrary, and should be checked, but as with all penalized regression smoothers, results are statistically insensitive to the exact choi[c]e

## null space

* Space of functions that are **not** exposed to shrinkage
* Typically functions linear in $x$ (i.e., spline shrinks to a linear term as $\lambda \to \infty$.
* e.g. for ridge the null space is the intercept term.
* From `?mgcv::smooth.construct.cs.smooth.spec`:

> The shrinkage version of the smooth [i.e. bs = "cs"], eigen-decomposes the
wiggliness penalty matrix, and sets its 2 zero eigenvalues to
small multiples of the smallest strictly positive eigenvalue. The
penalty is then set to the matrix with eigenvectors corresponding
to those of the original penalty, but eigenvalues set to the
pe[r]turbed versions. This penalty matrix has full rank and shrinks
the curve to zero at high enough smoothing parameters.

## 

```{r}
apropos("smooth.construct") |>
    gsub(pattern = "(smooth.construct.)|(.smooth.spec)", replacement = "")
```

```{r}
xvec <- sort(runif(101))
mfun <- function(bs) {
    sm <- smoothCon(s(x, bs = bs), data.frame(x=xvec))
    matplot(xvec, sm[[1]]$X, type = "l", lty = 1, main = bs, xlab = "", ylab = "")
}
par(mfrow = c(2,3), las = 1, bty = "l")
svec <- c("bs", "cc", "cs", "cr", "ps", "tp")
invisible(sapply(svec, mfun))
```

Consider:

* cost of setting up the basis in the first place
* sparsity (¿ not that important ?)
* mean-squared error for a given $k$

## other languages

* Julia doesn't seem to have much (discussion [here](https://discourse.julialang.org/t/generalized-additive-models-in-julia/10041/5), [beginning of a package](https://github.com/hendersontrent/GAM.jl)

```{julia eval=FALSE}
using Pkg
Pkg.add(url="https://github.com/hendersontrent/GAM.jl.git")
using Random, RDatasets, GAM

mtcars = dataset("datasets", "mtcars");
X = Matrix(mtcars[:, [:AM, :Cyl, :WT, :HP]]);
y = mtcars[:, :MPG];
model = fit_gam(X, y, :gaussian)
```

(fails almost immediately!)

Python: `pyGAM` [here]()

## project possibilities

* grouped lasso with spline penalties for spline selection?
* other combinations of spline/elastic net machinery? [@marraPractical2011]

## wavelets

![](../pix/wavelets.png)
* time and frequency localization
* Haar, symlet
* **fast wavelet transform**: ${\cal O}(n)$ or ${\cal O}(n \log n)$ depending on details/version

@donohoWavelet1995

```{r}
url <- "http://ms.mcmaster.ca/~bolker/measdata/ewcitmeas.dat"
download.file(url, dest = "meas.dat")
dd <- read.table("meas.dat", na.strings = "*", header = TRUE, comment = "")
plot(dd$London, type = "l")
library(wavethresh)
dd$date <- with(dd, as.Date(sprintf("19%d-%d-%d", YY, MM, X.DD)))
## skip (first) NA value: power-of-2 length
w <- wd(dd$London[2:2049], type = "station")
plot(w)
```

```{r friendly_wave, message = FALSE, results = "hide", cache = TRUE}
library(WaveletComp)
library(viridisLite)
dd$log_London <- log10(dd$London)
dd$log_Bristol <- log10(dd$Bristol+1)
dd$log_Bristol[1290]  <- 0  ## replace NA
any(is.na(dd$log_Bristol))
ww <- analyze.wavelet(dd[-1,], "log_London",
                loess.span = 0,
                dt = 1/52,
                dj = 1/100,
                lowerPeriod = 0.1,
                upperPeriod = 10)
ww_b <- analyze.wavelet(dd[-1,], "log_Bristol",
                loess.span = 0,
                dt = 1/52,
                dj = 1/100,
                lowerPeriod = 0.1,
                upperPeriod = 10)
```

```{r wpix1}
library(WaveletComp)
library(viridisLite)
wt.image(ww, show.date = TRUE,
         color.palette = "viridis(n.levels, direction = -1)")
```

```{r wpix2}
wt.image(ww_b, show.date = TRUE,
         color.palette = "viridis(n.levels, direction = -1)")
```

```{r wpix3}
par(las=1, bty = "l")
reconstruct(ww)
```

(R wavelet packages: `wavethresh`, `wsyn`, `WaveletComp`, `Rwave`, ...)

* compression: drop smallest components
* **or** soft-threshold as for lasso (SURE)
* as penalized component: @wandPenalized2011



