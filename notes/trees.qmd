---
title: "Tree-based methods"
reference-location: margin
citation-location: margin
bibliography: ../stat790.bib
date: today
date-format: "D MMM YYYY"
---

\newcommand{\A}{\mathbf A}
\newcommand{\B}{\mathbf B}
\newcommand{\D}{\mathbf D}
\newcommand{\GGamma}{\boldsymbol \Gamma}
\newcommand{\G}{\mathbf G}
\newcommand{\HH}{\mathbf H}
\newcommand{\I}{\mathbf I}
\newcommand{\K}{\mathbf K}
\newcommand{\N}{\mathbf N}
\newcommand{\Q}{\mathbf Q}
\newcommand{\R}{\mathbf R}
\newcommand{\bS}{\mathbf S}
\newcommand{\U}{\mathbf U}
\newcommand{\V}{\mathbf V}
\newcommand{\W}{\mathbf W}
\newcommand{\X}{\mathbf X}
\newcommand{\Y}{\mathbf Y}
\newcommand{\Z}{\mathbf Z}
\newcommand{\bbeta}{\boldsymbol \beta}
\newcommand{\bzero}{\boldsymbol 0}
\newcommand{\eeta}{\boldsymbol \eta}
\newcommand{\OOmega}{\boldsymbol \Omega}
\newcommand{\hatmat}{\mathbf H}
\newcommand{\grad}{\mathbf g}
\newcommand{\p}{\mathbf p}
\newcommand{\uu}{\mathbf u}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\z}{\mathbf z}
\newcommand{\kron}{\bigotimes}

<!-- typically renders in docs/ dir  -->

```{r utils, echo = FALSE}
## use help("image-methods", "Matrix")
## lattice graphics: ?lattice:xyplot for details on scales
ifun <- function(x, title = "", ck = FALSE, raster = TRUE) {
    image(Matrix(x),
          sub = "", xlab = "", ylab = "",
          colorkey = ck,
          aspect = "fill",
          scales = list(x = list(draw = FALSE),
                        y = list(draw = FALSE)),
          main = title,
          useRaster = raster
          )
}
```

## Tree-based methods

- trees are a basic building block of modelign methods ($\sim$ linear regression)
- **greedy** partitioning of parameter space
- efficient updating rules instead of linear algebra
- better at categorical predictors, interactions, missing data
- bias-variance tradeoff, curse of dimensionality, need for hyperparameter tuning ... **still apply**

## Classification and regression trees

- recursive *binary* splitting
- builds basis of rectangular regions
    - predictions homogeneous within regions
    - could be expressed as indicator variables
	
## CART: machinery

- splitting rule
   - regression: improve SSQ, deviance, ...
   - improve misclassification error, Gini coefficient, deviance
   - Gini coefficient: $\sum_k \hat p_{mk} (1-\hat p_{mk})$ (weighted average ($1-p$) loss)
   - deviance: $\sum \hat p_{mk} (-\log \hat p_{mk})$ (weighted average $-\log$ loss)

## tree-splitting rule complexity

- only ${\cal O}(Np)$!
- (more specifically $\sum (\# \textrm{unique}~x_i)$)
- splits only happen at data point values

## complexity pruning

- $1/N_m \sum_{x \in R_m} (y_i - \bar y_m)^2 + \alpha |T|$
- boils down to (total loss) + $\alpha$ \cdot size
- weakest-link pruning (greedy again): collapse least-useful splits first

## categorical predictors

- to avoid combinatorial splitting problems, order categories by 
   - frequency falling in outcome 1 (binary output)
   - mean response value
   - optimal split for Gini/deviance/cross-entropy/L2 loss
   - multicategory harder
- favors categorical vars with many categories ("such variables should be avoided" ... ???)

## loss matrix

- allow weighting of misclassification
- e.g. cost of false positive/negative, **or** value of sensitivity/specificity

## missing predictor variables

- 'missing' category
- use **surrogate variables** (algorithm?? effects of other splitting variables are already computed?)
- is imputation better?

## linear combination splits

- can do generalized discriminant analysis at each split
- weights, split point for $\sum a_j X_j \le s$
- seems better [@lohTreeStructured1988] but Breiman and Friedman disagree [@breimanTreeStructured1988]
- highly empirical!

## spam example

- 4600 messages, 57 predictors (48 word percentages; punctuation percentages; sequences of capitals)
- earlier: misclassification 7.6% from logistic regression, 5.5% from GAM
- CART: 9.3%
- weighted tree does slightly better at high specificity, but still $\ll$ GAM ...

![](../pix/spam_class.png)

## MARS

* like decision trees but piecewise linear (+ interactions) rather than constant
* **hinges** (or "reflected pairs"): 'mini-bases' (pairs of truncated linear spline terms) (total $2 Np$)
* stepwise/stagewise fitting
* also include possible *interactions*
    * products of model terms with unused basis elements
	* at each step $k$ we have $2Np \cdot (2k-1)$ choices to evaluate
	* each can be evaluated in ${\cal O}(1)$
	* principle of **marginality**: only add higher-order terms if lower-order term is already present
* reasonably local basis functions (not as good as B-splines)
* hyperparameters: max interaction depth
* then prune greedily using GCV (denominator: $(1-M/N)^2$, $M = r + cK$ where $r$=number of bases, $K$ = number of knots

## MARS on spam example

* still slightly worse than GAM ...
* GCV chooses model with 60 basis functions

## MARS vs CART

* stepwise vs piecewise linear basis
* presence of higher-order interactions

## MARS with categorical predictors

* "all possible binary partitions" - really? 
* (use ordering trick from CART?)

## computational costs

* additive models via backfitting: $p N \log N$ (initial sort) $+ mpN$ (backfitting) 
   * cf. $N p^2$ [for least-squares](https://math.stackexchange.com/questions/84495/computational-complexity-of-least-square-regression-operation)
* trees: $p N \log N$ to sort, $p N \log N$ for splits ($\log N$ cycles)
* MARS: $N m^2 + p m N$ to add a basis function to a model with $m$ terms $\to$ $N M^3 + p M^2 N$ (monitor stopping?)

## missing data (with MARS/CART)

* **danger will robinson**
* MCAR, MAR, MNAR ...
* categorical: code as "missing"
* discard incomplete observations
* impute beforehand
* impute/handle as part of learning algorithm
   * impute based on mean/median
   * impute conditional on other observations (MICE)
   * trees: **surrogate splits** (easy to look for next-best split)

## random forests

* bootstrap sample data
* grow a tree with a subset of $m$ variables **at each split**
* average prediction from ensemble (mean prediction, or majority vote)
* variance of average of correlated variables = $\sigma^2 (\rho + (1-\rho)/B)$
   * $\rho \approx 0.05$ for bootstrapped **trees**
* subsetting variables reduces correlations between trees

## tuning parameters

* suggested $m = \sqrt{p}$ for classification, min node size 1 (stopping point) 
   * $p/3$, min node size 5 for regression
* min node size/max depth: "our experience is that using full-grown trees seldom costs much, and results in one less tuning parameter"
* number of trees just needs to be "large enough" (curve flattens quickly)
    * monitor progression for iterative algorithms?
* super-easy to parallelize	
* explore tuning parameters: https://github.com/tidymodels/TMwR/issues/356
* out-of-bag samples

## comparisons

* with many trees ($B \to \infty$), RF estimation variance shrinks to $\rho(x) \sigma^2(x)$
* correlation increases with $m$

![](../pix/RFerr.png)

* RF similar to ridge: shrinks strongly correlated variables toward each other

## loss functions

* robustness
* how to pick???

![](../pix/class_loss.png)

## boosting

* "deep" methods
* still an additive model
* **stagewise** rather than simultaneous
* stagewise $\approx$ stepwise, but not recomputing previous coefficients/models

## boosting: AdaBoost

* using {-1, 1} scores
* fit a classifier with current weights $w_i$
* compute **average error** (== avg weights of correct predictions)
* $\alpha_m$ = log-odds of average correctness
* weights of *incorrectly predicted* samples multiplied by odds of avg correctness
* prediction is $\textrm{sign}(\sum \alpha_m G_m(x))$
* can use probability mapped to [-1, 1] instead of classification
* corresponds to **exponential loss** $\exp(-y f)$; deviance is $\log(1+\exp(-2yf))$

## boosting (generally)

* algorithm (stagewise):
   * fit a 'weak learner' to *pseudo-residuals*
   * update model based on the *sum* of the previous model plus the current weak learner
* pseudo-residuals: $-\partial L/\partial \hat y$ 
   * = $2 (y - \hat y)$ for MSE
   * related to *generalized scoring* for GLMs etc.
   * == **gradient** of loss function
* weakest tree: "stump" (== "fork" ?)
* robust criteria don't give rise to fast algorithms

## gradient boosting

* works for any differentiable loss function
* find gradient, line search

<!-- @pressNumerical2007:

![](../pix/steepest_descent.png)

-->

## gradient tree boosting algorithm

(ESL, @bujokasGradient2022)

* fit a decision tree (learner) to pseudo-residuals (= modeling the *gradient of the loss*)
* find the step size $\gamma$ to apply to the new learner:

<!-- https://tex.stackexchange.com/questions/5223/command-for-argmin-or-argmax --> 

$$
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\begin{split}
F_m(x) & = F_{m-1}(x) + \gamma_m h_m(x) \\
\gamma_m & = \argmin_{\gamma} \sum L(y_i,F_{m-1}(x) + \gamma h_m(x))
\end{split}
$$
where $h_m(x)$ is the new prediction for $x$

* for MSE $\gamma_m = \frac{1/N} \sum (h_m(x) (\hat y - y))/\sum (h_m^2)$ 

* for regression trees, $h_m$ is **constant** in each region
* Friedman 'TreeBoost' optimizes $\gamma$ for each region
* $F_m = F_{m-1} + \sum_j \gamma_{jm} 1_{Rjm}(x)$
* $\gamma_{jm}$ reduces to the mean $(\hat y-y)$ for MSE

## hyperparameters

* tree size
    * 2 = 'stump'
	* stumps == **additive** models
	* $J$ determines maximum interaction depth
	* ESL say $4 \le J \le 8$ is good, rarely $>10$, $\approx 6$ usually OK
* early stopping ($M$) ("how many iterations without an improvement in the objective function occur before training should be halted") [@precheltEarly2012]
* learning rate/**shrinkage**
* stochastic boosting

## extreme gradient boosting

* use a "Newton" step (**elementwise** second-order approximation)
   * compute gradient and curvature of $L$ wrt $\hat y$
   * irrelevant/same as gradient descent for MSE, Huber loss, L1 loss ...
* [@sigristGradient2018; @choDoes2018]
* Implementation-dependent stuff? Sparsity, out-of-memory implementation, etc.


<!--
## xgboost rules 

* uses penalty term $\gamma T + \frac{\lambda}{2} ||w||^2$
* loss for leaf $j$ = 
$$
\left (\sum_{i \in I_j} g_i\right) w_j + \frac{1}{2}
$$
-->

## stochastic boosting

* subsample data at each stage (e.g. $\eta = 0.5$)

## stagewise modeling

![](../pix/stagewise.png)


## BART

- @chipmanBART2010
- again a sum of trees
- smart, data-informed ("empirical Bayesian") priors
- fitting procedure is MCMC
- defaults are good, don't need much tuning

## regularizing priors

- informative but non-biasing
- shrink toward 'null value', e.g. ridge/lasso penalties

## BART priors

- tree depth, splitting point, overall residual variance
- componentwise/independent priors on each model
- tree depth: $\alpha (1 + d)^{-\beta}$ 
    - $\alpha = 0.95$, $\beta = 2$: modal $d=2$, most $d \le 5$
- splitting priors: uniform over split points (observed values) and parameters	
- split point/mean: scaled Normal prior
    - overall mean prediction is sum of means across $m$ trees
    - $m\mu_\mu \pm k\sqrt{m}\sigma_\mu = y_{\textrm{min}}$
	- e.g. $k=2$ for $\approx$ 95% range in (ymin, ymax)
- residual variance: inverse-chi-squared with scale matching $\sigma$ 
    - $\hat \sigma$ is marginal std dev *or* linear regression std dev
	- choose df (3-10) so that upper $q$ quantile is at $\hat \sigma$
- overfitting is unlikely, so choose $m$ "big enough" ($m=200$)

## MCMC rules

- **Gibbs sampling**
- draw $\sigma$ from inverse gamma (conjugate prior)
- $T_j$: grow a terminal node (0.25), prune (0.25), change a nonterminal rule (0.4), swap a rule between parent and child (0.1)
- resample $M_j$ values from a normal (and recompute residuals)
- initialize with $m$ stumps

## MCMC stuff

* burn-in (typically 200 steps)
* sampling
* single chain (typically 1000 steps)


