---
title: "Non-Gaussian responses (week 3?)"
reference-location: margin
citation-location: margin
bibliography: ../stat790.bib
date: today
date-format: "D MMM YYYY"
---

\newcommand{\B}{\mathbf B}
\newcommand{\D}{\mathbf D}
\newcommand{\GGamma}{\boldsymbol \Gamma}
\newcommand{\G}{\mathbf G}
\newcommand{\HH}{\mathbf H}
\newcommand{\I}{\mathbf I}
\newcommand{\Q}{\mathbf Q}
\newcommand{\R}{\mathbf R}
\newcommand{\U}{\mathbf U}
\newcommand{\V}{\mathbf V}
\newcommand{\W}{\mathbf W}
\newcommand{\X}{\mathbf X}
\newcommand{\Y}{\mathbf Y}
\newcommand{\Z}{\mathbf Z}
\newcommand{\bbeta}{\boldsymbol \beta}
\newcommand{\bzero}{\boldsymbol 0}
\newcommand{\eeta}{\boldsymbol \eta}
\newcommand{\hatmat}{\mathbf H}
\newcommand{\grad}{\mathbf g}
\newcommand{\p}{\mathbf p}
\newcommand{\uu}{\mathbf u}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\z}{\mathbf z}
\newcommand{\bkl}{\tilde \beta_k(\lambda)}
\newcommand{\bjl}{\tilde \beta_j(\lambda)}

<!-- typically renders in docs/ dir  -->

## Non-Gaussian responses

- Why worry about it?
- Isn't least-squares good enough?
- **poll** ([polleverywhere](https://www.polleverywhere.com/login))

## Some answers

- heteroscedasticity (Gauss-Markov only applies to homog. variance)
- still unbiased but no longer minimum variance
- maybe we shouldn't (e.g. **linear probability model** in econometrics)
    - adjust for heteroscedasticity with **robust/sandwich estimators** etc. (White): 
$$
\hat \V = (\X^\top \X)^{-1} (\X^\top \G \X) (\X^\top \X)^{-1}
$$
where $\G = \textrm{Diag} (\hat \varepsilon_i ^2 )$
(contrast with $s^2 (\X^\top \X)^{-1}$)
* if we have 
* if we have **nonlinear** models, MLEs are no longer unbiased

## Why not linear?

- actual nonlinear patterns (but can handle these by transformation/basis expansion)
- unrealistic predictions (e.g. probabilities outside of $[0,1]$
- varying effects (e.g. effect of a 1-unit change in $x$ on probability must differ depending on baseline probability)
- Why not transform? **poll** ([polleverywhere](https://www.polleverywhere.com/login))

## Logistic regression (ESL $\S$ 4.4)

* Worst-case scenario (farthest from Gaussian)
* ESL starts with a *multinomial* model:

$$
\log \left( \frac{\textrm{Pr}(G=i|X=x)}{\textrm{Pr}(G=K|X=x)}\right) = \beta_{i0} + \beta_i^\top x, \quad i \in 1 \ldots K-1
$$
(and so $\textrm{Pr}(G=K|X=x) = 1/\left(1 + \sum_{i=1}^{K-1} \exp(\beta_{i0} + \beta_i^\top x)\right)$)

* independent of baseline/reparameterization
* log-likelihood $\sum \log p_i(x_i; \theta)$ where $\theta$ is the complete set of parameters

## Log-likelihood

* for two categories, log-likelihood simplifies to
$$
\begin{split}
 & \sum\left( y_i \beta^\top x_i - \log \left( 1+ e^{\beta^\top x_i} \right) \right) \\
=  &  \sum\left( y_i \eta_i - \log \left( 1+ \exp(\eta_i) \right) \right)
\end{split}
$$
* **weight matrix** $\W = \textrm{Diag}(p(1-p))$
    * more generally, $\textrm{Diag}(1/\textrm{Var}(\mu))$
* score equation: 
   * $\sum_i=1^N x_i(y_i - p(x_i; \beta))$
   * Newton update is $\beta^* - \HH^{-1} \grad$
   * gradient: $\X^\top (\y - \p)$
   * generally $\X^\top (\y - \mu) = \X^\top (\y - g^{-1}(\eeta))$
* Hessian: $- \X^\top \W \X$
* solution is $(\X^\top \W \X)^{-1} \X^\top \W \z$
* where $\z = \X \beta_0 + \W^{-1}(\y - \p)$ is the adjusted response

## Newton step

* **iteratively reweighted least squares**
* solve 
$$
\X^\top \W \X \beta^* = \X^\top \W \z
$$


- [4C03 notes](https://bbolker.github.io/stat4c03/notes/glm_deriv.pdf)
- [4C03 notes 2](https://bbolker.github.io/stat4c03/notes/glm_comp.pdf)

## Newton vs IRLS

* Newton vs *Fisher scoring* (expected value of the Hessian); equivalent for the *canonical link*  
(e.g. logistic for binary data, log for Poisson data
* link mostly important for interpretation
* can be disregarded (?) if we are going to handle nonlinearity by basis expansion
* convergence? [@mountHow2012]

## Families

* Gaussian, Poisson, binomial (binary)
* May need to compute *scale*/*dispersion* parameter
    * for exponential families, calculate as $\sqrt{D/(n-p)}$ where $D$ is the *deviance* (-2 log likelihood, equal to SSQ for Gaussian)
	* not exactly the MLE but good enough
* **over-dispersion**: quasi-likelihood 
* more complex familes (negative binomial etc.) have an additional, non-collapsible parameters, need to estimate by MLE (or **profiling**)

## Regularized versions

* lasso, ridge, or elasticnet
* score equations: $\x^|top (\y - \p) = \lambda \cdot \textrm{sign}(\beta_j)$ for **active** variables (non-zero coeffs)

## revisiting ridge by data augmentation

* we want to minimize $||\y - \X \beta||_2^2 + \lambda ||\beta||_2^2$
* the solution to the original regression equations was $\hat \beta = (\X^\top \X)^{-1} \X^\top \y$
* Set
$$
\B = \left(
    \begin{array}{c}
     \X \\
    \sqrt{\lambda} \I
    \end{array}
\right)
$$
* ridge regression should still be solvable by data expansion, i.e. in the IRLS loop use
$$
\B = \left(
    \begin{array}{c}
    \X^\top \W \X \\
    \sqrt{\lambda} \I
    \end{array}
\right)
$$
and 
$$
\y^* = (\y \quad \bzero)
$$
* so that $\B^\top \B = \X^\top \X + \lambda I$ and the $\X^\top \y$ term is unchanged

## ridge + IRLS

* recall that we need to iteratively solve
$$
\X^\top \W \X \beta^* = \X^\top \W \z
$$
* if we want to solve the **weighted** least-squares problem from IRLS, we would normally take
the QR decomposition of $\X' = \X \sqrt{\W}$ (so that $\X^\top \X = \X^\top \W \X$)
* enhance this by adding $\sqrt{\lambda} I$ to $X$ and zeros to $\z$ (no longer $\y$) as before
* ¿ try out [enhanced GLM](code/myglm.R) ?

## proximal gradient descent/Newton

* solving the optimization problem for non-differentiable penalties
* previous solution (cyclic coordinate descent)
* simpler strategies (cyclic coordinate descent) may not work as well
* **proximal** gradient descent or **proximal** IRLS: 
* like the pathwise coordinate descent solution from lasso:
* **solution**:
$$
\bjl \leftarrow S\left( \sum_{i=1}^N x_{ij}(y_i - \tilde y_i^{(j)}), \lambda \right)
$$
* where $S(t, \lambda) = \textrm{sign}(t)(|t| - \lambda)$
* except that we can no longer jump straight to the correct solution.

## proximal operator

* separate objective function into **smooth** part (likelihood/RSS/etc., **plus** ridge penalty) and **non-smooth** part (typically an L1 regularization term)
* **proximal operator**: 
$$
\textrm{argmin}_u \left(\underbrace{h(u)}_{nonconvex part} + \frac{1}{2} || u - x ||_2^2 \right)
$$
* for $h = \lambda ||\beta||_1$ (lasso penalty), we get the soft-threshold operator
$$
\begin{cases}
\beta_i-\lambda & \textrm{if} \: \beta_i < -\lambda \\
0 & \textrm{if} \: -\lambda < \beta_i < \lambda \\
\beta_i+\lambda & \textrm{if} \: \beta_i < -\lambda \\
\end{cases}
$$
(from [Ryan Tibshirani's notes on optimization](https://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf))

## proximal Newton (IRLS)

* take a Newton/IRLS step
* apply the prox operator to soft-threshold
* not going to get into the details! thresholding is more complex than the gradient descent rule [@leeProximal2014]
* need to solve
$$
\textrm{argmin}_u \left(\underbrace{\rlap{$h(u)$}\phantom{\textrm{nonconvex part}}}_{\textrm{nonconvex part}} + \frac{1}{2} (u-x)^\top H (u-x) \right)
$$
i.e. replace $||u-x||_2^2$ with a corresponding quadratic form
* ¿ not sure if the solution corresponds easily to soft-thresholding again?
* also need to be careful about backtracking if necessary
* i.e. taking a Newton step $\H^{-1}\grad$ is better than an uninformed gradient step $t \grad$ (where $t$ is the **learning rate**)
but might overshoot
* i.e., **don't try this at home**
 
([Ryan Tibshirani again](https://www.stat.cmu.edu/~ryantibs/convexopt-F15/lectures/17-prox-newton.pdf))

## more computational details

* from [glmnet family docs](https://glmnet.stanford.edu/articles/glmnetFamily.html)
* using the *name* of the family ("poisson" etc.) uses hard-coded internal algorithms
   * faster (but scaling isn't too bad??)
   * less flexible (alternative families [links, variance functions])
   * slightly less robust (doesn't do backtracking)

## sparse model matrices (side note)

* expanding factors (categorical variables) may make $p$ gigantic
* each factor $f_i$ with $n_i$ levels will be expanded via treatment contrasts, so
$\X$ will have (at least) $\sum_i (n_i-1)$ columns
* `glmnet::makeX`, `Matrix::sparse.model.matrix()`
