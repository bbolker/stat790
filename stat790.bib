@article{batesCrossvalidation,
  title = {Cross-Validation: What Does It Estimate and How Well Does It Do It?},
  author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
  pages = {36},
  abstract = {Cross-validation is a widely-used technique to estimate prediction error, but its behavior is complex and not fully understood. Ideally, one would like to think that cross-validation estimates the prediction error for the model at hand, fit to the training data. We prove that this is not the case for the linear model fit by ordinary least squares; rather it estimates the average prediction error of models fit on other unseen training sets drawn from the same population. We further show that this phenomenon occurs for most popular estimates of prediction error, including data splitting, bootstrapping, and Mallow’s Cp. Next, the standard confidence intervals for prediction error derived from cross-validation may have coverage far below the desired level. Because each data point is used for both training and testing, there are correlations among the measured accuracies for each fold, and so the usual estimate of variance is too small. We introduce a nested cross-validation scheme to estimate this variance more accurately, and show empirically that this modification leads to intervals with approximately correct coverage in many examples where traditional cross-validation intervals fail. Lastly, our analysis also shows that when producing confidence intervals for prediction accuracy with simple data splitting, one should not re-fit the model on the combined data, since this invalidates the confidence intervals.},
  langid = {english},
  file = {/home/bolker/Documents/zotero_new/storage/XRCEZVAJ/Bates et al. - Cross-validation what does it estimate and how we.pdf}
}


@book{woodGeneralized2017,
	series = {{CRC} {Texts} in {Statistical} {Science}},
	title = {Generalized {Additive} {Models}: {An} {Introduction} with {R}},
	url = {https://www.amazon.com/Generalized-Additive-Models-Introduction-Statistical-ebook/dp/B071Z9L5D5/ref=sr_1_1?ie=UTF8&qid=1511887995&sr=8-1&keywords=wood+additive+models},
	urldate = {2017-11-28},
	publisher = {Chapman \& Hall},
	author = {Wood, Simon N.},
	year = {2017},
	file = {Generalized Additive Models\: An Introduction with R, Second Edition (Chapman & Hall/CRC Texts in Statistical Science) 2, Simon N. Wood - Amazon.com:/home/bolker/Documents/zotero_new/storage/Q79P94BB/ref=sr_1_1.html:text/html},
}

@article{elithworking2008,
  title = {A Working Guide to Boosted Regression Trees},
  author = {Elith, J. and Leathwick, J. R. and Hastie, T.},
  year = {2008},
  journal = {Journal of Animal Ecology},
  volume = {77},
  number = {4},
  pages = {802--813},
  issn = {1365-2656},
  doi = {10.1111/j.1365-2656.2008.01390.x},
  abstract = {1 Ecologists use statistical models for both explanation and prediction, and need techniques that are flexible enough to express typical features of their data, such as nonlinearities and interactions. 2 This study provides a working guide to boosted regression trees (BRT), an ensemble method for fitting statistical models that differs fundamentally from conventional techniques that aim to fit a single parsimonious model. Boosted regression trees combine the strengths of two algorithms: regression trees (models that relate a response to their predictors by recursive binary splits) and boosting (an adaptive method for combining many simple models to give improved predictive performance). The final BRT model can be understood as an additive regression model in which individual terms are simple trees, fitted in a forward, stagewise fashion. 3 Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data. They have no need for prior data transformation or elimination of outliers, can fit complex nonlinear relationships, and automatically handle interaction effects between predictors. Fitting multiple trees in BRT overcomes the biggest drawback of single tree models: their relatively poor predictive performance. Although BRT models are complex, they can be summarized in ways that give powerful ecological insight, and their predictive performance is superior to most traditional modelling methods. 4 The unique features of BRT raise a number of practical issues in model fitting. We demonstrate the practicalities and advantages of using BRT through a distributional analysis of the short-finned eel (Anguilla australis Richardson), a native freshwater fish of New Zealand. We use a data set of over 13 000 sites to illustrate effects of several settings, and then fit and interpret a model using a subset of the data. We provide code and a tutorial to enable the wider use of BRT by ecologists.},
  langid = {english},
  keywords = {data mining,machine learning,model averaging,random forests,species distributions},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2656.2008.01390.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2656.2008.01390.x},
  file = {/home/bolker/Documents/zotero_new/storage/AJQP9HL5/Elith et al. - 2008 - A working guide to boosted regression trees.pdf;/home/bolker/Documents/zotero_new/storage/83EYN5SA/j.1365-2656.2008.01390.html}
}

@article{friedmanPathwise2007,
  title = {Pathwise Coordinate Optimization},
  author = {Friedman, Jerome and Hastie, Trevor and Höfling, Holger and Tibshirani, Robert},
  year = {2007},
  month = dec,
  journal = {Annals of Applied Statistics},
  volume = {1},
  number = {2},
  pages = {302--332},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/07-AOAS131},
  abstract = {We consider “one-at-a-time” coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the L1-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the “fused lasso,” however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems.},
  langid = {english},
  mrnumber = {MR2415737},
  zmnumber = {1378.90064},
  keywords = {convex optimization,Coordinate descent,lasso},
  url ={https://projecteuclid.org/euclid.aoas/1196438020},
  file = {/home/bolker/Documents/zotero_new/storage/A6RT6MVR/Friedman et al. - 2007 - Pathwise coordinate optimization.pdf;/home/bolker/Documents/zotero_new/storage/6PL3GMCT/1196438020.html}
}

@article{friedmanRegularization2010,
  title = {Regularization {{Paths}} for {{Generalized Linear Models}} via {{Coordinate Descent}}},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2010},
  journal = {Journal of statistical software},
  volume = {33},
  number = {1},
  pages = {1--22},
  issn = {1548-7660},
  abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ1 (the lasso), ℓ2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
  pmcid = {PMC2929880},
  pmid = {20808728},
  url ={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2929880/},
  file = {/home/bolker/Documents/zotero_new/storage/3P9HUKZH/Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf}
}

@book{jamesintroduction2013,
  title = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  volume = {112},
  publisher = {{Springer}},
  file = {/home/bolker/Documents/zotero_new/storage/KY22P8UZ/James et al. - 2013 - An introduction to statistical learning.pdf;/home/bolker/Documents/zotero_new/storage/AQBFJVSN/10.html}
}

@article{jansonEffective2015,
  title = {Effective {{Degrees}} of {{Freedom}}: {{A Flawed Metaphor}}},
  shorttitle = {Effective {{Degrees}} of {{Freedom}}},
  author = {Janson, Lucas and Fithian, William and Hastie, Trevor},
  year = {2015},
  month = jun,
  journal = {Biometrika},
  volume = {102},
  number = {2},
  eprint = {1312.7851},
  eprinttype = {arxiv},
  pages = {479--485},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asv019},
  abstract = {To most applied statisticians, a fitting procedure's degrees of freedom is synonymous with its model complexity, or its capacity for overfitting to data. In particular, it is often used to parameterize the bias-variance tradeoff in model selection. We argue that, contrary to folk intuition, model complexity and degrees of freedom are not synonymous and may correspond very poorly. We exhibit and theoretically explore various examples of fitting procedures for which degrees of freedom is not monotonic in the model complexity parameter, and can exceed the total dimension of the response space. Even in very simple settings, the degrees of freedom can exceed the dimension of the ambient space by an arbitrarily large amount. We show the degrees of freedom for any non-convex projection method can be unbounded.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,Statistics - Other Statistics},
  url ={http://arxiv.org/abs/1312.7851},
  file = {/home/bolker/Documents/zotero_new/storage/4R9W6EK4/Janson et al. - 2015 - Effective Degrees of Freedom A Flawed Metaphor.pdf;/home/bolker/Documents/zotero_new/storage/IDJCQPKA/1312.html}
}

@article{simonRegularization2011,
  title = {Regularization {{Paths}} for {{Cox}}’s {{Proportional Hazards Model}} via {{Coordinate Descent}}},
  author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2011},
  month = mar,
  journal = {Journal of statistical software},
  volume = {39},
  number = {5},
  pages = {1--13},
  issn = {1548-7660},
  doi = {10.18637/jss.v039.i05},
  abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of ℓ1 and ℓ2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
  pmcid = {PMC4824408},
  pmid = {27065756},
  url ={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4824408/},
  file = {/home/bolker/Documents/zotero_new/storage/BN5NERSF/Simon et al. - 2011 - Regularization Paths for Cox’s Proportional Hazard.pdf}
}

@article{zoudegrees2007,
  title = {On the “Degrees of Freedom” of the Lasso},
  author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
  year = {2007},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {35},
  number = {5},
  pages = {2173--2192},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053607000000127},
  abstract = {We study the effective degrees of freedom of the lasso in the framework of Stein’s unbiased risk estimation (SURE). We show that the number of nonzero coefficients is an unbiased estimate for the degrees of freedom of the lasso—a conclusion that requires no special assumption on the predictors. In addition, the unbiased estimator is shown to be asymptotically consistent. With these results on hand, various model selection criteria—Cp, AIC and BIC—are available, which, along with the LARS algorithm, provide a principled and efficient approach to obtaining the optimal lasso fit with the computational effort of a single ordinary least-squares fit.},
  langid = {english},
  mrnumber = {MR2363967},
  zmnumber = {1126.62061},
  keywords = {Degrees of freedom,LARS algorithm,lasso,model selection,SURE,unbiased estimate},
  url ={https://projecteuclid.org/euclid.aos/1194461726},
  file = {/home/bolker/Documents/zotero_new/storage/2BGWHRP2/Zou et al. - 2007 - On the “degrees of freedom” of the lasso.pdf;/home/bolker/Documents/zotero_new/storage/P9AKEXYL/1194461726.html}
}

@book{hastieelements2009,
  title = {The Elements of Statistical Learning Data Mining, Inference, and Prediction},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H},
  year = {2009},
  publisher = {{Springer}},
  address = {{New York}},
  abstract = {"During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics."--Jacket.},
  isbn = {978-0-387-84858-7 0-387-84858-4 978-0-387-84857-0 0-387-84857-6},
  langid = {english},
  url ={http://public.eblib.com/EBLPublic/PublicView.do?ptiID=437866}
}

@article{wittenpenalized2009,
  title = {A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis},
  author = {Witten, Daniela M. and Tibshirani, Robert and Hastie, Trevor},
  year = {2009},
  month = jan,
  journal = {Biostatistics},
  pages = {kxp008},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxp008},
  abstract = {We present a penalized matrix decomposition (PMD), a new framework for computing a rank-K approximation for a matrix. We approximate the matrix X as , where dk, uk, and vk minimize the squared Frobenius norm of X, subject to penalties on uk and vk. This results in a regularized version of the singular value decomposition. Of particular interest is the use of L1-penalties on uk and vk, which yields a decomposition of X using sparse vectors. We show that when the PMD is applied using an L1-penalty on vk but not on uk, a method for sparse principal components results. In fact, this yields an efficient algorithm for the “SCoTLASS” proposal (Jolliffe and others 2003) for obtaining sparse principal components. This method is demonstrated on a publicly available gene expression data set. We also establish connections between the SCoTLASS method for sparse principal component analysis and the method of Zou and others (2006). In addition, we show that when the PMD is applied to a cross-products matrix, it results in a method for penalized canonical correlation analysis (CCA). We apply this penalized CCA method to simulated data and to a genomic data set consisting of gene expression and DNA copy number measurements on the same set of samples.},
  langid = {english},
  pmid = {19377034},
  keywords = {Canonical correlation analysis,DNA copy number,Integrative genomic analysis,L1,Matrix decomposition,Principal component analysis,Sparse principal component analysis,SVD},
  url ={http://biostatistics.oxfordjournals.org/content/early/2009/04/17/biostatistics.kxp008},
  file = {/home/bolker/Documents/zotero_new/storage/8DFHTXWR/Witten et al. - 2009 - A penalized matrix decomposition, with application.pdf;/home/bolker/Documents/zotero_new/storage/9CHTHTAV/biostatistics.kxp008.html}
}

@article{breimanHeuristics1996,
  title = {Heuristics of Instability and Stabilization in Model Selection},
  author = {Breiman, Leo},
  year = {1996},
  journal = {The Annals of Statistics},
  volume = {24},
  number = {6},
  pages = {2350--2383},
  issn = {0090-5364},
  doi = {10.1214/aos/1032181158},
  mrnumber = {1425957},
  url ={https://mathscinet.ams.org/mathscinet-getitem?mr=1425957},
  file = {/home/bolker/Documents/zotero_new/storage/JG3S24MA/Breiman - 1996 - Heuristics of instability and stabilization in mod.pdf;/home/bolker/Documents/zotero_new/storage/B9QY8VE2/mathscinet-getitem.html}
}

@article{breimanStatistical2001,
  title = {Statistical {{Modeling}}: {{The Two Cultures}}},
  shorttitle = {Statistical {{Modeling}}},
  author = {Breiman, Leo},
  year = {2001},
  month = aug,
  journal = {Statistical Science},
  volume = {16},
  number = {3},
  pages = {199--215},
  issn = {08834237},
  annotation = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  url ={http://www.jstor.org/stable/2676681},
  file = {/home/bolker/Documents/zotero_new/storage/46GU69P4/Breiman - 2001 - Statistical Modeling The Two Cultures.pdf;/home/bolker/Documents/zotero_new/storage/4I2ISTJD/2676681.html}
}

@article{breimanSubmodel1992,
  title = {Submodel {{Selection}} and {{Evaluation}} in {{Regression}}. {{The X-Random Case}}},
  author = {Breiman, Leo and Spector, Philip},
  year = {1992},
  journal = {International Statistical Review / Revue Internationale de Statistique},
  volume = {60},
  number = {3},
  pages = {291--319},
  publisher = {{[Wiley, International Statistical Institute (ISI)]}},
  issn = {0306-7734},
  doi = {10.2307/1403680},
  abstract = {Often, in a regression situation with many variables, a sequence of submodels is generated containing fewer variables by using such methods as stepwise addition or deletion of variables, or 'best subsets'. The question is which of this sequence of submodels is 'best', and how can submodel performance be evaluated. This was explored in Breiman (1988) for a fixed X-design. This is a sequel exploring the case of random X-designs. Analytical results are difficult, if not impossible. This study involved an extensive simulation. The basis of the study is the theoretical definition of prediction error (PE) as the expected squared error produced by applying a prediction equation to the distributional universe of (y, x) values. This definition is used throughout to compare various submodels. There can be startling differences between the x-fixed and x-random situations and different PE estimates are appropriate. Non-resampling estimates such as CP, adjusted R2, etc. turn out to be highly biased methods for submodel selection. The two best methods are cross-validation and bootstrap. One surprise is that 5 fold cross-validation (leave out 20\% of the data) is better at submodel selection and evaluation than leave-one-out cross-validation. There are a number of other surprises. /// Dans l'analyse de problèmes de régression à plusieurs variables (indépendantes), on produit souvent une série de sous-modèles constitués d'un sous-ensemble des variables par des méthodes telles que l'addition par étape, le retrait par étape et la méthode du meilleur sous-ensemble. Le problème est de déterminer lequel de ces sous-modèles est le meilleur et d'évaluer sa performance. Ce problème fut exploré dans Breiman (1988) pour le cas d'une matrice X fixe. Dans ce qui suit on considère le cas où la matrice X est aléatoire. La détermination de résultats analytiques est difficile, sinon impossible. Notre étude a utilisé des simulations de grande envergure. Elle se base sur la définition théorique de l'erreur de prédiction (EP) comme étant l'espérance du carré de l'erreur produite en applicant une équation de prédiction à l'univers distributional des valeurs (y, x). La définition est utilisée dans toute l'étude à fin de comparer divers sous-modèles. Il y a une différence étonnante entre le cas où la matrice X est fixée et celui où elle est aléatoire. Différents estimateurs de la EP sont à propos. Les estimateurs n'utilisant pas de ré-échantillonage, tels que le Cp et le R2 ajusté, produisent des méthodes de sélection ayant grand biais. Les deux meilleures méthodes sont la validation croisée et l'autoamorçage. Une surprise est que la validation croisée quintuple est meilleure que la validation croisée tous sauf un. Il y a plusieurs autres résultats surprenants.},
  url ={https://www.jstor.org/stable/1403680}
}

@article{mccormickgiven2021,
  title = {The "given Data" Paradigm Undermines Both Cultures},
  author = {McCormick, Tyler},
  year = {2021},
  month = may,
  journal = {arXiv:2105.12478 [cs, stat]},
  eprint = {2105.12478},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Breiman organizes "Statistical modeling: The two cultures" around a simple visual. Data, to the far right, are compelled into a "black box" with an arrow and then catapulted left by a second arrow, having been transformed into an output. Breiman then posits two interpretations of this visual as encapsulating a distinction between two cultures in statistics. The divide, he argues is about what happens in the "black box." In this comment, I argue for a broader perspective on statistics and, in doing so, elevate questions from "before" and "after" the box as fruitful areas for statistical innovation and practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  url ={http://arxiv.org/abs/2105.12478},
  file = {/home/bolker/Documents/zotero_new/storage/FH534ALZ/McCormick - 2021 - The given data paradigm undermines both cultures.pdf;/home/bolker/Documents/zotero_new/storage/DDEXI6D5/2105.html}
}

@article{meinshausenQuantile,
  title = {Quantile {{Regression Forests}}},
  author = {Meinshausen, Nicolai},
  pages = {17},
  abstract = {Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.},
  langid = {english},
  file = {/home/bolker/Documents/zotero_new/storage/CEFFUZ6I/Meinshausen - Quantile Regression Forests.pdf}
}

@article{millerBreiman2021,
  title = {Breiman's Two Cultures: {{You}} Don't Have to Choose Sides},
  shorttitle = {Breiman's Two Cultures},
  author = {Miller, Andrew C. and Foti, Nicholas J. and Fox, Emily B.},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.12219 [cs, stat]},
  eprint = {2104.12219},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Breiman's classic paper casts data analysis as a choice between two cultures: data modelers and algorithmic modelers. Stated broadly, data modelers use simple, interpretable models with well-understood theoretical properties to analyze data. Algorithmic modelers prioritize predictive accuracy and use more flexible function approximations to analyze data. This dichotomy overlooks a third set of models \$-\$ mechanistic models derived from scientific theories (e.g., ODE/SDE simulators). Mechanistic models encode application-specific scientific knowledge about the data. And while these categories represent extreme points in model space, modern computational and algorithmic tools enable us to interpolate between these points, producing flexible, interpretable, and scientifically-informed hybrids that can enjoy accurate and robust predictions, and resolve issues with data analysis that Breiman describes, such as the Rashomon effect and Occam's dilemma. Challenges still remain in finding an appropriate point in model space, with many choices on how to compose model components and the degree to which each component informs inferences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  url ={http://arxiv.org/abs/2104.12219},
  file = {/home/bolker/Documents/zotero_new/storage/9J4KDYIH/Miller et al. - 2021 - Breiman's two cultures You don't have to choose s.pdf;/home/bolker/Documents/zotero_new/storage/QHRQMC62/2104.html}
}

@article{vanhouwelingenShrinkage2001,
  title = {Shrinkage and {{Penalized Likelihood}} as {{Methods}} to {{Improve Predictive Accuracy}}},
  author = {{van Houwelingen}, J. C},
  year = {2001},
  journal = {Statistica Neerlandica},
  volume = {55},
  number = {1},
  pages = {17--34},
  issn = {0039-0402},
  doi = {10.1111/1467-9574.00154},
  abstract = {A review is given of shrinkage and penalization as tools to improve predictive accuracy of regression models. The James-Stein estimator is taken as starting point. Procedures covered are Pre-test Estimation, the Ridge Regression of Hoerl and Kennard, the Shrinkage Estimators of Copas and Van Houwelingen and Le Cessie, the LASSO of Tibshirani and the Garotte of Breiman. An attempt is made to place all these procedures in a unifying framework of semi-Bayesian methodology. Applications are briefly mentioned, but not amply discussed.},
  keywords = {Garotte,LASSO,Pre-test Estimation,Ridge Regression},
  url ={http://journals.scholarsportal.info/details/00390402/v55i0001/17_saplamtipa.xml},
  file = {/home/bolker/Documents/zotero_new/storage/DAFUHND7/van Houwelingen - 2001 - Shrinkage and Penalized Likelihood as Methods to I.pdf}
}


@article{gelmanReflections2021,
  title = {Reflections on {{Breiman}}'s {{Two Cultures}} of {{Statistical Modeling}}},
  author = {Gelman, Andrew},
  year = {2021},
  journal = {Observational Studies},
  volume = {7},
  number = {1},
  pages = {95--98},
  publisher = {{University of Pennsylvania Press}},
  issn = {2767-3324},
  doi = {10.1353/obs.2021.0025},
  abstract = {In his article on Two Cultures of Statistical Modeling, Leo Breiman argued for an algorithmic approach to statistics, as exemplified by his pathbreaking research on large regularized models that fit data and have good predictive properties but without attempting to capture true underlying structure. I think Breiman was right about the benefits of open-ended predictive methods for complex modern problems. I also discuss some points of disagreement, notably Breiman's dismissal of Bayesian methods, which I think reflected a misunderstanding on his part, in that he did not recognized that Bayesian inference can be viewed as regularized prediction and does not rely on an assumption that the fitted model is true. In retrospect, we can learn both from Breiman's deep foresight and from his occasional oversights.},
  keywords = {algorithms,Bayesian inference,prediction,statistical modeling},
  url ={https://muse.jhu.edu/article/799750},
  file = {/home/bolker/Documents/zotero_new/storage/NCI8H5A2/Gelman - 2021 - Reflections on Breiman's Two Cultures of Statistic.pdf}
}

@article{raperLeo2020,
  title = {Leo {{Breiman}}'s "{{Two Cultures}}"},
  author = {Raper, Simon},
  year = {2020},
  journal = {Significance},
  volume = {17},
  number = {1},
  pages = {34--37},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2020.01357.x},
  abstract = {Simon Raper continues his exploration of key moments in the history of statistics with a close look at Leo Breiman's 2001 critique of the data modelling and algorithmic modelling cultures. Almost 20 years on, Breiman's words are perhaps more relevant than ever},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2020.01357.x},
  url ={https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2020.01357.x},
  file = {/home/bolker/Documents/zotero_new/storage/LWUBZBRK/Raper - 2020 - Leo Breiman's “Two Cultures”.pdf;/home/bolker/Documents/zotero_new/storage/YT5CS5RB/j.1740-9713.2020.01357.html}
}

@misc{ratzCan2021,
  title = {Can {{QR Decomposition Be Actually Faster}}? {{Schwarz-Rutishauser Algorithm}}},
  shorttitle = {Can {{QR Decomposition Be Actually Faster}}?},
  author = {Ratz, Arthur V.},
  year = {2021},
  month = apr,
  journal = {Medium},
  abstract = {QR Factorization. Schwarz-Rutishauser Algorithm (Explained). Performance Optimization. 100\% Working Code Samples In Python 3.9.x And NumPy},
  howpublished = {https://towardsdatascience.com/can-qr-decomposition-be-actually-faster-schwarz-rutishauser-algorithm-a32c0cde8b9b},
  langid = {english},
  file = {/home/bolker/Zotero/storage/SI6AJ2M3/can-qr-decomposition-be-actually-faster-schwarz-rutishauser-algorithm-a32c0cde8b9b.html}
}

@article{elithWorking2008a,
  title = {A Working Guide to Boosted Regression Trees},
  author = {Elith, J. and Leathwick, J. R. and Hastie, T.},
  year = {2008},
  month = jul,
  journal = {Journal of Animal Ecology},
  volume = {77},
  number = {4},
  pages = {802--813},
  issn = {1365-2656},
  doi = {10.1111/j.1365-2656.2008.01390.x},
  abstract = {1 Ecologists use statistical models for both explanation and prediction, and need techniques that are flexible enough to express typical features of their data, such as nonlinearities and interactions. 2 This study provides a working guide to boosted regression trees (BRT), an ensemble method for fitting statistical models that differs fundamentally from conventional techniques that aim to fit a single parsimonious model. Boosted regression trees combine the strengths of two algorithms: regression trees (models that relate a response to their predictors by recursive binary splits) and boosting (an adaptive method for combining many simple models to give improved predictive performance). The final BRT model can be understood as an additive regression model in which individual terms are simple trees, fitted in a forward, stagewise fashion. 3 Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data. They have no need for prior data transformation or elimination of outliers, can fit complex nonlinear relationships, and automatically handle interaction effects between predictors. Fitting multiple trees in BRT overcomes the biggest drawback of single tree models: their relatively poor predictive performance. Although BRT models are complex, they can be summarized in ways that give powerful ecological insight, and their predictive performance is superior to most traditional modelling methods. 4 The unique features of BRT raise a number of practical issues in model fitting. We demonstrate the practicalities and advantages of using BRT through a distributional analysis of the short-finned eel (Anguilla australis Richardson), a native freshwater fish of New Zealand. We use a data set of over 13 000 sites to illustrate effects of several settings, and then fit and interpret a model using a subset of the data. We provide code and a tutorial to enable the wider use of BRT by ecologists.},
  copyright = {\textcopyright{} 2008 The Authors. Journal compilation \textcopyright{} 2008 British Ecological Society},
  langid = {english},
  keywords = {data mining,machine learning,model averaging,random forests,species distributions},
  file = {/home/bolker/Zotero/storage/G7UUPE4K/Elith et al. - 2008 - A working guide to boosted regression trees.pdf;/home/bolker/Zotero/storage/NNH3GMC4/j.1365-2656.2008.01390.html}
}

@article{bienLasso2013,
  title = {A Lasso for Hierarchical Interactions},
  author = {Bien, Jacob and Taylor, Jonathan and Tibshirani, Robert},
  year = {2013},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {41},
  number = {3},
  pages = {1111--1141},
  issn = {0090-5364},
  doi = {10.1214/13-AOS1096},
  abstract = {We add a set of convex constraints to the lasso to produce sparse interaction models that honor the hierarchy restriction that an interaction only be included in a model if one or both variables are marginally important. We give a precise characterization of the effect of this hierarchy constraint, prove that hierarchy holds with probability one and derive an unbiased estimate for the degrees of freedom of our estimator. A bound on this estimate reveals the amount of fitting ``saved'' by the hierarchy constraint.},
  langid = {english}
}

@article{friedmanPathwise2007,
  title = {Pathwise Coordinate Optimization},
  author = {Friedman, Jerome and Hastie, Trevor and H{\"o}fling, Holger and Tibshirani, Robert},
  year = {2007},
  month = dec,
  journal = {Annals of Applied Statistics},
  volume = {1},
  number = {2},
  pages = {302--332},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/07-AOAS131},
  abstract = {We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the L1-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the ``fused lasso,'' however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems.},
  langid = {english},
  mrnumber = {MR2415737},
  zmnumber = {1378.90064},
  keywords = {convex optimization,Coordinate descent,lasso},
  file = {/home/bolker/Zotero/storage/A6RT6MVR/Friedman et al. - 2007 - Pathwise coordinate optimization.pdf;/home/bolker/Zotero/storage/6PL3GMCT/1196438020.html}
}

@article{wittenPenalized2009,
  title = {A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis},
  author = {Witten, Daniela M. and Tibshirani, Robert and Hastie, Trevor},
  year = {2009},
  month = jan,
  journal = {Biostatistics},
  pages = {kxp008},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxp008},
  abstract = {We present a penalized matrix decomposition (PMD), a new framework for computing a rank-K approximation for a matrix. We approximate the matrix X as , where dk, uk, and vk minimize the squared Frobenius norm of X, subject to penalties on uk and vk. This results in a regularized version of the singular value decomposition. Of particular interest is the use of L1-penalties on uk and vk, which yields a decomposition of X using sparse vectors. We show that when the PMD is applied using an L1-penalty on vk but not on uk, a method for sparse principal components results. In fact, this yields an efficient algorithm for the ``SCoTLASS'' proposal (Jolliffe and others 2003) for obtaining sparse principal components. This method is demonstrated on a publicly available gene expression data set. We also establish connections between the SCoTLASS method for sparse principal component analysis and the method of Zou and others (2006). In addition, we show that when the PMD is applied to a cross-products matrix, it results in a method for penalized canonical correlation analysis (CCA). We apply this penalized CCA method to simulated data and to a genomic data set consisting of gene expression and DNA copy number measurements on the same set of samples.},
  langid = {english},
  pmid = {19377034},
  keywords = {Canonical correlation analysis,DNA copy number,Integrative genomic analysis,L1,Matrix decomposition,Principal component analysis,Sparse principal component analysis,SVD},
  file = {/home/bolker/Zotero/storage/8DFHTXWR/Witten et al. - 2009 - A penalized matrix decomposition, with application.pdf;/home/bolker/Zotero/storage/9CHTHTAV/biostatistics.kxp008.html}
}


@article{hastieRidge2020,
  title = {Ridge {{Regularization}}: {{An Essential Concept}} in {{Data Science}}},
  shorttitle = {Ridge {{Regularization}}},
  author = {Hastie, Trevor},
  year = {2020},
  month = oct,
  journal = {Technometrics},
  volume = {62},
  number = {4},
  pages = {426--433},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2020.1791959},
  abstract = {Ridge or more formally l2 regularization shows up in many areas of statistics and machine learning. It is one of those essential devices that any good data scientist needs to master for their craft. In this brief ridge fest, I have collected together some of the magic and beauty of ridge that my colleagues and I have encountered over the past 40 years in applied statistics.},
  keywords = {Data science,Retrospective,Ridge regression},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2020.1791959},
  file = {/home/bolker/Zotero/storage/732VNZZ9/Hastie - 2020 - Ridge Regularization An Essential Concept in Data.pdf}
}


@book{nazarathyStatistics2021,
  title = {Statistics with {Julia}: Fundamentals for Data Science, Machine Learning and Artificial Intelligence},
  shorttitle = {Statistics with {{Julia}}},
  author = {Nazarathy, Yoni and Klok, Hayden},
  year = {2021},
  series = {Springer {{Series}} in the {{Data Sciences}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-70901-3},
  isbn = {978-3-030-70900-6 978-3-030-70901-3},
  langid = {english}
}

@misc{navarroScience2019,
  title = {Science and Statistics},
  author = {Navarro, Danielle},
  year = {2019},
  month = mar,
  address = {{Aarhus University}},
  abstract = {http://interactingminds.au.dk/events/single-events/artikel/2-day-workshop-open-science-and-reproducibility/},
  langid = {english},
  file = {/home/bolker/Zotero/storage/UX69QA8G/scienceandstatistics.html},
  url = {https://slides.com/djnavarro/scienceandstatistics}
}

@misc{adarValue2015,
  title = {On the Value of Command-Line ``Bullshittery''},
  author = {Adar, Eytan},
  year = {2015},
  month = oct,
  journal = {Medium},
  abstract = {Go read this first for context. Done? Good.},
  url = {https://medium.com/@eytanadar/on-the-value-of-command-line-bullshittery-94dc19ec8c61},
  langid = {english},
  file = {/home/bolker/Zotero/storage/VFLDYT2H/on-the-value-of-command-line-bullshittery-94dc19ec8c61.html}
}

@book{shaliziAdvanced2022,
  title = {Advanced Data Analysis from an Elementary Point of View},
  author = {Shalizi, Cosma Rohilla},
  year = {2022},
  url = {https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/}
}

@misc{lindelovCommon2019,
  title = {Common Statistical Tests Are Linear Models (or: How to Teach Stats)},
  author = {Lindeløv, Jonas Kristoffer},
  year = {2019},
  month = jun,
  url = {https://lindeloev.github.io/tests-as-linear/},
  file = {/home/bolker/Documents/zotero_new/storage/YEWVY7VE/tests-as-linear.html}
}

@article{shaferTutorial2008,
  title = {A Tutorial on Conformal Prediction},
  author = {Shafer, Glenn and Vovk, Vladimir},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  pages = {371--421},
  abstract = {Conformal prediction uses past experience to determine precise levels of confidence in new predictions. Given an error probability ε, together with a method that makes a prediction yˆ of a label y, it produces a set of labels, typically containing yˆ, that also contains y with probability 1 − ε. Conformal prediction can be applied to any method for producing yˆ: a nearest-neighbor method, a support-vector machine, ridge regression, etc.},
  langid = {english},
  url = {https://jmlr.csail.mit.edu/papers/volume9/shafer08a/shafer08a.pdf},
  file = {/home/bolker/Documents/zotero_new/storage/V4UKY2P8/Shafer and Vovk - A Tutorial on Conformal Prediction.pdf}
}

@article{dezeureHighDimensional2015a,
  title = {High-Dimensional Inference: Confidence Intervals, p-values and {R} software hdi},
  shorttitle = {High-{{Dimensional Inference}}},
  author = {Dezeure, Ruben and Bühlmann, Peter and Meier, Lukas and Meinshausen, Nicolai},
  year = {2015},
  month = nov,
  journal = {Statistical Science},
  volume = {30},
  number = {4},
  pages = {533--558},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/15-STS527},
  abstract = {We present a (selective) review of recent frequentist high-dimensional inference methods for constructing ppp-values and confidence intervals in linear and generalized linear models. We include a broad, comparative empirical study which complements the viewpoint from statistical methodology and theory. Furthermore, we introduce and illustrate the R-package hdi which easily allows the use of different methods and supports reproducibility.},
  langid = {english},
  mrnumber = {MR3432840},
  zmnumber = {06946201},
  keywords = {$p$-value,Clustering,confidence interval,generalized linear model,high-dimensional statistical inference,linear model,multiple testing,R-software},
  note = {https://projecteuclid.org/euclid.ss/1449670857},
  file = {/home/bolker/Documents/zotero_new/storage/X6QR9YRM/Dezeure et al. - 2015 - High-Dimensional Inference Confidence Intervals, .pdf;/home/bolker/Documents/zotero_new/storage/4Q8TUABS/1449670857.html}
}

@article{andersonirises1935,
  title = {The Irises of the {{Gaspe}} Peninsula},
  author = {Anderson, Edgar},
  year = {1935},
  journal = {Bull. Am. Iris Soc.},
  volume = {59},
  pages = {2--5},
  note = {https://www.biodiversitylibrary.org/item/270486\#page/344/mode/1up},
  file = {/home/bolker/Documents/zotero_new/storage/ZD6Q5SPC/1571980073972926080.html}
}

@article{andersonSpecies1936,
  title = {The {{Species Problem}} in {{Iris}}},
  author = {Anderson, Edgar},
  year = {1936},
  journal = {Annals of the Missouri Botanical Garden},
  volume = {23},
  number = {3},
  pages = {457--509},
  publisher = {{Missouri Botanical Garden Press}},
  issn = {0026-6493},
  doi = {10.2307/2394164},
  note = {https://www.jstor.org/stable/2394164},
  file = {/home/bolker/Documents/zotero_new/storage/IXF3AU76/Anderson - 1936 - The Species Problem in Iris.pdf}
}

@article{bezdekWill1999,
  title = {Will the Real Iris Data Please Stand Up?},
  author = {Bezdek, J.C. and Keller, J.M. and Krishnapuram, R. and Kuncheva, L.I. and Pal, N.R.},
  year = {1999},
  month = jun,
  journal = {IEEE Transactions on Fuzzy Systems},
  volume = {7},
  number = {3},
  pages = {368--369},
  issn = {1941-0034},
  doi = {10.1109/91.771092},
  abstract = {This correspondence points out several published errors in replicates of the well-known iris data, which was collected by Anderson (1935) but first published by Fisher (1936).},
  keywords = {Books,Computer science,Databases,Face detection,Iris,Machine learning,Mathematics,Pattern recognition,Testing,Writing},
  file = {/home/bolker/Documents/zotero_new/storage/ZLN3I9XM/771092.html}
}

@article{bodmeroutstanding2021,
  title = {The Outstanding Scientist, {R}.{A}. {Fisher}: His Views on Eugenics and Race},
  shorttitle = {The Outstanding Scientist, {{R}}.{{A}}. {{Fisher}}},
  author = {Bodmer, Walter and Bailey, R. A. and Charlesworth, Brian and {Eyre-Walker}, Adam and Farewell, Vernon and Mead, Andrew and Senn, Stephen},
  year = {2021},
  month = apr,
  journal = {Heredity},
  volume = {126},
  number = {4},
  pages = {565--576},
  publisher = {{Nature Publishing Group}},
  issn = {1365-2540},
  doi = {10.1038/s41437-020-00394-6},
  copyright = {2021 The Author(s), under exclusive licence to The Genetics Society},
  langid = {english},
  keywords = {Evolution,Genetics},
  note = {https://www.nature.com/articles/s41437-020-00394-6},
  file = {/home/bolker/Documents/zotero_new/storage/NMQEVV64/Bodmer et al. - 2021 - The outstanding scientist, R.A. Fisher his views .pdf}
}

@article{fisherUse1936,
  title = {The {{Use}} of {{Multiple Measurements}} in {{Taxonomic Problems}}},
  author = {Fisher, R. A.},
  year = {1936},
  journal = {Annals of Eugenics},
  volume = {7},
  number = {2},
  pages = {179--188},
  issn = {2050-1439},
  doi = {10.1111/j.1469-1809.1936.tb02137.x},
  abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
  note = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
  file = {/home/bolker/Documents/zotero_new/storage/ZILQV7TS/Fisher - 1936 - The Use of Multiple Measurements in Taxonomic Prob.pdf;/home/bolker/Documents/zotero_new/storage/CJ6BXN78/j.1469-1809.1936.tb02137.html}
}

@misc{gelmantypical2020,
  title = {The Typical Set and Its Relevance to {{Bayesian}} Computation},
  author = {Gelman, Andrew},
  year = {2020},
  month = aug,
  journal = {Statistical Modeling, Causal Inference, and Social Science},
  howpublished = {https://statmodeling.stat.columbia.edu/2020/08/02/the-typical-set-and-its-relevance-to-bayesian-computation/},
  file = {/home/bolker/Documents/zotero_new/storage/UGQTPXZI/the-typical-set-and-its-relevance-to-bayesian-computation.html}
}

@article{waltersEffects1981,
  title = {Effects of {{Measurement Errors}} on the {{Assessment}} of {{Stock}}–{{Recruitment Relationships}}},
  author = {Walters, Carl J. and Ludwig, Donald},
  year = {1981},
  month = jun,
  journal = {Canadian Journal of Fisheries and Aquatic Sciences},
  volume = {38},
  number = {6},
  pages = {704--710},
  issn = {0706-652X},
  doi = {10.1139/f81-093},
  abstract = {Errors in measuring spawning stocks can have a profound effect on the appearance of stock–recruitment relationships. Large errors make recruitments appear to be independent of spawning stocks. This effect promotes overexploitation rather than simply making the relationship noisier and harder to measure. Efforts to explain variation in recruitment through factors other than spawning stock may be deceptive as well. Much fisheries theory and practice are based on the assumption that recruitment is largely independent of spawning stock; that assumption is not credible except in cases where spawning stocks are known to have been measured accurately.Key words: stock, recruitment, statistics, overexploitation, Les relations stock–recrutement peuvent être profondément influencées par des erreurs de mesures des stocks reproducteurs. De grandes erreurs font que le recrutement semble être indépendant des stocks reproducteurs. Cet effet encourage la surexploitation plutôt que de simplement rendre la relation plus criante et plus difficile à mesurer. Il peut être tout aussi trompeur de faire appel à des facteurs autres que le stock reproducteur pour expliquer la variation du recrutement. La théorie et la pratique des pêches reposent surtout sur l'hypothèse que le recrutement est pour une bonne part indépendant du stock reproducteur; cette hypothèse manque de crédibilité, à moins qu'on ne soit certain que les stocks ont été mesurés avec précision.},
  note = {http://www.nrcresearchpress.com/doi/abs/10.1139/f81-093},
  file = {/home/bolker/Documents/zotero_new/storage/WBG2T75Q/Walters_Ludwig_1981_Effects of Measurement Errors on the Assessment of Stock–Recruitment.pdf;/home/bolker/Documents/zotero_new/storage/T9Q5TZP3/f81-093.html}
}

@misc{bryan_project-oriented_2017,
	title = {Project-oriented workflow},
	url = {https://www.tidyverse.org/blog/2017/12/workflow-vs-script/},
	abstract = {Advice on workflows for developing R scripts. How to think about whether an action belongs in the script or elsewhere.},
	language = {en-us},
	urldate = {2021-01-14},
	journal = {Tidyverse},
	author = {Bryan, Jenny},
	month = dec,
	year = {2017}
}




@inproceedings{jovicreview2015,
	title = {A review of feature selection methods with applications},
	doi = {10.1109/MIPRO.2015.7160458},
	abstract = {Feature selection (FS) methods can be used in data pre-processing to achieve efficient data reduction. This is useful for finding accurate data models. Since exhaustive search for optimal feature subset is infeasible in most cases, many search strategies have been proposed in literature. The usual applications of FS are in classification, clustering, and regression tasks. This review considers most of the commonly used FS techniques. Particular emphasis is on the application aspects. In addition to standard filter, wrapper, and embedded methods, we also provide insight into FS for recent hybrid approaches and other advanced topics.},
	booktitle = {38th {International} {Convention} on {Information} and {Communication} {Technology}, {Electronics} and {Microelectronics} ({MIPRO})},
		author = {Jović, A. and Brkić, K. and Bogunović, N.},
	month = may,
	year = {2015},
	keywords = {Accuracy, Classification algorithms, Clustering algorithms, Filtering algorithms, Information filters, Search problems},
	pages = {1200--1205}
}


@article{pengFeature2005,
	title = {Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy},
	volume = {27},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2005.159},
	abstract = {Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Peng, Hanchuan and Long, Fuhui and Ding, C.},
	month = aug,
	year = {2005},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Algorithm design and analysis, Cancer, classification., Costs, Diversity reception, Index Terms- Feature selection, maximal dependency, maximal relevance, minimal redundancy, mutual information, Mutual information, Pattern classification, Performance analysis, Redundancy, Support vector machine classification, Support vector machines},
	pages = {1226--1238},
	file = {IEEE Xplore Abstract Record:/home/bolker/Documents/zotero_new/storage/9J9HEHAK/1453511.html:text/html},
}


@article{murtaughPerformance2009,
	title = {Performance of several variable-selection methods applied to real ecological data},
	volume = {12},
	issn = {1461023X, 14610248},
	url = {http://doi.wiley.com/10.1111/j.1461-0248.2009.01361.x},
	doi = {10.1111/j.1461-0248.2009.01361.x},
	language = {en},
	number = {10},
	urldate = {2015-07-11},
	journal = {Ecology Letters},
	author = {Murtaugh, Paul A.},
	month = oct,
	year = {2009},
	pages = {1061--1068},
	file = {Murtaugh 2008.pdf:/home/bolker/Documents/zotero_new/storage/FE4V8ES4/Murtaugh 2008.pdf:application/pdf},
}

@article{dahlgrenAlternative2010,
	title = {Alternative regression methods are not considered in {Murtaugh} (2009) or by ecologists in general},
	volume = {13},
	issn = {1461-0248},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1461-0248.2010.01460.x/abstract},
	doi = {10.1111/j.1461-0248.2010.01460.x},
	abstract = {Ecology Letters (2010) 13: E7–E9 
Abstract
Murtaugh (2009) recently illustrated that all subsets variable selection is very similar to stepwise regression. This, however, does not necessarily mean both methods are useful. On the contrary, the same problems with overfitting should apply. Ecologists should, if model building is indeed necessary, consider more reliable regression methods now available.},
	language = {en},
	number = {5},
	urldate = {2016-04-14},
	journal = {Ecology Letters},
	author = {Dahlgren, Johan P.},
	month = may,
	year = {2010},
	keywords = {Shrinkage, Variable selection, AIC, BIC, lasso, all subsets, ridge regression, stepwise multiple regression},
	pages = {E7--E9},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/ZKQHERDI/abstract.html:text/html},
}


@article{blanchetForward2008,
	title = {Forward {Selection} of {Explanatory} {Variables}},
	volume = {89},
	issn = {0012-9658},
	url = {http://www.esajournals.org/doi/abs/10.1890/07-0986.1},
	doi = {10.1890/07-0986.1},
	abstract = {This paper proposes a new way of using forward selection of explanatory variables in regression or canonical redundancy analysis. The classical forward selection method presents two problems: a highly inflated Type I error and an overestimation of the amount of explained variance. Correcting these problems will greatly improve the performance of this very useful method in ecological modeling. To prevent the first problem, we propose a two-step procedure. First, a global test using all explanatory variables is carried out. If, and only if, the global test is significant, one can proceed with forward selection. To prevent overestimation of the explained variance, the forward selection has to be carried out with two stopping criteria: (1) the usual alpha significance level and (2) the adjusted coefficient of multiple determination () calculated using all explanatory variables. When forward selection identifies a variable that brings one or the other criterion over the fixed threshold, that variable is rejected, and the procedure is stopped. This improved method is validated by simulations involving univariate and multivariate response data. An ecological example is presented using data from the Bryce Canyon National Park, Utah, USA.},
	number = {9},
	urldate = {2013-01-03},
	journal = {Ecology},
	author = {Blanchet, F. Guillaume and Legendre, Pierre and Borcard, Daniel},
	month = sep,
	year = {2008},
	keywords = {type I error, simulation study, forward selection, Moran's eigenvector maps (MEM), non-orthogonal explanatory variables, orthogonal explanatory variables, principal coordinates of neighbor matrices (PCNM)},
	pages = {2623--2632},
	file = {ESA Snapshot:/home/bolker/Documents/zotero_new/storage/TEWI8Z5I/showCitFormats.html:text/html},
}



@article{dysonWise2005,
	title = {Wise {Man}},
	issn = {0028-7504},
	url = {https://www.nybooks.com/articles/2005/10/20/wise-man/},
	abstract = {Great scientists come in two varieties, which Isaiah Berlin, quoting the seventh-century-BC poet Archilochus, called foxes and hedgehogs. Foxes know many},
	language = {en},
	urldate = {2023-01-14},
	journal = {New York Review of Books},
	author = {Dyson, Freeman},
	month = oct,
	year = {2005},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/CWBZNU4N/wise-man.html:text/html},
}


@article{stoneAsymptotic1977a,
	title = {An {Asymptotic} {Equivalence} of {Choice} of {Model} by {Cross}-{Validation} and {Akaike}'s {Criterion}},
	volume = {39},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984877},
	abstract = {A logarithmic assessment of the performance of a predicting density is found to lead to asymptotic equivalence of choice of model by cross-validation and Akaike's criterion, when maximum likelihood estimation is used within each model.},
	number = {1},
	urldate = {2023-01-14},
	journal = {J. Royal Stat. Soc. B},
	author = {Stone, M.},
	year = {1977},
	pages = {44--47}
}


@inproceedings{venablesExegeses1998,
	address = {Washington, DC},
	series = {1998 {International} {S}-{PLUS} {User} {Conference}},
	title = {Exegeses on {Linear} {Models}},
	url = {http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf},
	author = {Venables, W. N},
	year = {1998},
}

@misc{atlasQR2013,
	title = {{QR} factorization for ridge regression},
	url = {https://math.stackexchange.com/questions/299481/qr-factorization-for-ridge-regression},
	urldate = {2023-01-15},
	journal = {Mathematics Stack Exchange},
	author = {Atlas},
	month = feb,
	year = {2013},
}


@article{friedmanRegularization2010,
	title = {Regularization {Paths} for {Generalized} {Linear} {Models} via {Coordinate} {Descent}},
	volume = {33},
	issn = {1548-7660},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2929880/},
	abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ1 (the lasso), ℓ2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
	number = {1},
	urldate = {2020-06-25},
	journal = {Journal of statistical software},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
	year = {2010},
	pmid = {20808728},
	pmcid = {PMC2929880},
	pages = {1--22},
	file = {PubMed Central Full Text PDF:/home/bolker/Documents/zotero_new/storage/3P9HUKZH/Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf:application/pdf},
}


@article{hastieRidge2020,
	title = {Ridge {Regularization}: {An} {Essential} {Concept} in {Data} {Science}},
	volume = {62},
	issn = {0040-1706},
	shorttitle = {Ridge {Regularization}},
	url = {https://doi.org/10.1080/00401706.2020.1791959},
	doi = {10.1080/00401706.2020.1791959},
	abstract = {Ridge or more formally l2 regularization shows up in many areas of statistics and machine learning. It is one of those essential devices that any good data scientist needs to master for their craft. In this brief ridge fest, I have collected together some of the magic and beauty of ridge that my colleagues and I have encountered over the past 40 years in applied statistics.},
	number = {4},
	urldate = {2022-11-03},
	journal = {Technometrics},
	author = {Hastie, Trevor},
	month = oct,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2020.1791959},
	keywords = {Data science, Ridge regression, Retrospective},
	pages = {426--433},
	file = {Accepted Version:/home/bolker/Documents/zotero_new/storage/732VNZZ9/Hastie - 2020 - Ridge Regularization An Essential Concept in Data.pdf:application/pdf},
}

@article{giraud-carrierJustification2005,
  title = {Toward a Justification of Meta-Learning: {{Is}} the No Free Lunch Theorem a Show-Stopper?},
  shorttitle = {Toward a Justification of Meta-Learning},
  author = {{Giraud-Carrier}, Christophe and Provost, Foster},
  year = {2005},
  month = jan,
  journal = {Proceedings of the ICML-2005 Workshop on Meta-learning},
  abstract = {We present a preliminary analysis of the fun-damental viability of meta-learning, revisit-ing the No Free Lunch (NFL) theorem. The analysis shows that given some simple and very basic assumptions, the NFL theorem is of little relevance to research in Machine Learning. We augment the basic NFL frame-work to illustrate that the notion of an Ulti-mate Learning Algorithm is well defined. We show that, although cross-validation still is not a viable way to construct general-purpose learning algorithms, meta-learning offers a natural alternative. We still have to pay for our lunch, but the cost is reasonable: the nec-essary fundamental assumptions are ones we all make anyway.},
  file = {/home/bolker/Zotero/storage/GJH45PRV/Giraud-Carrier and Provost - 2005 - Toward a justification of meta-learning Is the no.pdf}
}

@article{wolpertNo1997,
  title = {No Free Lunch Theorems for Optimization},
  author = {Wolpert, D.H. and Macready, W.G.},
  year = {1997},
  month = apr,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {67--82},
  issn = {1941-0026},
  doi = {10.1109/4235.585893},
  abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of "no free lunch" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori "head-to-head" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
  keywords = {Algorithm design and analysis,Bayesian methods,Evolutionary computation,Information theory,Iron,Minimax techniques,Performance analysis,Probability distribution,Simulated annealing},
  file = {/home/bolker/Zotero/storage/NM8P77V4/Wolpert and Macready - 1997 - No free lunch theorems for optimization.pdf}
}

@misc{bremen79Neural2020,
  title = {Neural {{Networks}} ({{Maybe}}) {{Evolved}} to {{Make Adam The Best Optimizer}}},
  author = {{bremen79}},
  year = {2020},
  month = dec,
  journal = {Parameter-free Learning and Optimization Algorithms},
  abstract = {Disclaimer: This post will be a little different than my usual ones. In fact, I won't prove anything and I will just briefly explain some of my conjectures around optimization in deep neural \ldots},
  langid = {english},
  file = {/home/bolker/Zotero/storage/5NGYSYMT/neural-network-maybe-evolved-to-make-adam-the-best-optimizer.html}
}




@article{zhaoDefense2021,
	title = {In Defense of the Indefensible: A Very Naïve Approach to High-Dimensional Inference},
	volume = {36},
	issn = {0883-4237, 2168-8745},
	shorttitle = {In {Defense} of the {Indefensible}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-36/issue-4/In-Defense-of-the-Indefensible--A-Very-Na%c3%afve-Approach/10.1214/20-STS815.full},
	doi = {10.1214/20-STS815},
	abstract = {A great deal of interest has recently focused on conducting inference on the parameters in a high-dimensional linear model. In this paper, we consider a simple and very naïve two-step procedure for this task, in which we (i) fit a lasso model in order to obtain a subset of the variables, and (ii) fit a least squares model on the lasso-selected set. Conventional statistical wisdom tells us that we cannot make use of the standard statistical inference tools for the resulting least squares model (such as confidence intervals and p-values), since we peeked at the data twice: once in running the lasso, and again in fitting the least squares model. However, in this paper, we show that under a certain set of assumptions, with high probability, the set of variables selected by the lasso is identical to the one selected by the noiseless lasso and is hence deterministic. Consequently, the naïve two-step approach can yield asymptotically valid inference. We utilize this finding to develop the naïve confidence interval, which can be used to draw inference on the regression coefficients of the model selected by the lasso, as well as the naïve score test, which can be used to test the hypotheses regarding the full-model regression coefficients.},
	number = {4},
	urldate = {2023-01-23},
	journal = {Statistical Science},
	author = {Zhao, Sen and Witten, Daniela and Shojaie, Ali},
	month = nov,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Confidence interval, Lasso, p-value, Post-selection inference, Significance testing},
	pages = {562--577},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/GGVB6L9Q/Zhao et al. - 2021 - In Defense of the Indefensible A Very Naïve Appro.pdf:application/pdf},
}


@article{yuanModel2006,
	title = {Model selection and estimation in regression with grouped variables},
	volume = {68},
	issn = {1369-7412, 1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00532.x},
	doi = {10.1111/j.1467-9868.2005.00532.x},
	abstract = {We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efﬁcient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
	language = {en},
	number = {1},
	urldate = {2023-01-22},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Yuan, Ming and Lin, Yi},
	month = feb,
	year = {2006},
	pages = {49--67},
	file = {Yuan and Lin - 2006 - Model selection and estimation in regression with .pdf:/home/bolker/Documents/zotero_new/storage/WPB8ARPT/Yuan and Lin - 2006 - Model selection and estimation in regression with .pdf:application/pdf},
}

@misc{mountHow2012,
  title = {How Robust Is Logistic Regression?},
  author = {Mount, John},
  year = {2012},
  month = aug,
  journal = {Win Vector LLC},
  abstract = {Logistic Regression is a popular and effective technique for modeling categorical outcomes as a function of both continuous and categorical variables. The question is: how robust is it? Or: how rob\ldots},
  howpublished = {https://win-vector.com/2012/08/23/how-robust-is-logistic-regression/},
  langid = {american},
  file = {/home/bolker/Zotero/storage/DYBG6VD8/how-robust-is-logistic-regression.html}
}

@article{leeProximal2014,
	title = {Proximal {Newton}-{Type} {Methods} for {Minimizing} {Composite} {Functions}},
	volume = {24},
	issn = {1052-6234, 1095-7189},
	url = {http://epubs.siam.org/doi/10.1137/130921428},
	doi = {10.1137/130921428},
	abstract = {We generalize Newton-type methods for minimizing smooth functions to handle a sum of two convex functions: a smooth function and a nonsmooth function with a simple proximal mapping. We show that the resulting proximal Newton-type methods inherit the desirable convergence behavior of Newton-type methods for minimizing smooth functions, even when search directions are computed inexactly. Many popular methods tailored to problems arising in bioinformatics, signal processing, and statistical learning are special cases of proximal Newton-type methods, and our analysis yields new convergence results for some of these methods.},
	language = {en},
	number = {3},
	urldate = {2023-02-01},
	journal = {SIAM Journal on Optimization},
	author = {Lee, Jason D. and Sun, Yuekai and Saunders, Michael A.},
	month = jan,
	year = {2014},
	pages = {1420--1443},
	file = {Lee et al. - 2014 - Proximal Newton-Type Methods for Minimizing Compos.pdf:/home/bolker/Documents/zotero_new/storage/AI5AKFX5/Lee et al. - 2014 - Proximal Newton-Type Methods for Minimizing Compos.pdf:application/pdf},
}


@article{perperogloureview2019a,
	title = {A review of spline function procedures in {R}},
	volume = {19},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-019-0666-3},
	doi = {10.1186/s12874-019-0666-3},
	abstract = {With progress on both the theoretical and the computational fronts the use of spline modelling has become an established tool in statistical regression analysis. An important issue in spline modelling is the availability of user friendly, well documented software packages. Following the idea of the STRengthening Analytical Thinking for Observational Studies initiative to provide users with guidance documents on the application of statistical methods in observational research, the aim of this article is to provide an overview of the most widely used spline-based techniques and their implementation in R.},
	number = {1},
	urldate = {2023-02-01},
	journal = {BMC Medical Research Methodology},
	author = {Perperoglou, Aris and Sauerbrei, Willi and Abrahamowicz, Michal and Schmid, Matthias},
	month = mar,
	year = {2019},
	keywords = {Functional form of continuous covariates, Multivariable modelling},
	pages = {46},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/6RVM58RB/Perperoglou et al. - 2019 - A review of spline function procedures in R.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/YHYJLI4J/s12874-019-0666-3.html:text/html},
}

@article{golubGeneralized1979,
  title = {Generalized {{Cross-Validation}} as a {{Method}} for {{Choosing}} a {{Good Ridge Parameter}}},
  author = {Golub, Gene H. and Heath, Michael and Wahba, Grace},
  year = {1979},
  month = may,
  journal = {Technometrics},
  volume = {21},
  number = {2},
  pages = {215--223},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1979.10489751},
  abstract = {Consider the ridge estimate ({$\lambda$}) for {$\beta$} in the model unknown, ({$\lambda$}) = (X T X + n{$\lambda$}I)-1 X T y. We study the method of generalized cross-validation (GCV) for choosing a good value for {$\lambda$} from the data. The estimate is the minimizer of V({$\lambda$}) given by where A({$\lambda$}) = X(X T X + n{$\lambda$}I)-1 X T . This estimate is a rotation-invariant version of Allen's PRESS, or ordinary cross-validation. This estimate behaves like a risk improvement estimator, but does not require an estimate of {$\sigma$}2, so can be used when n - p is small, or even if p {$\geq$} 2 n in certain cases. The GCV method can also be used in subset selection and singular value truncation methods for regression, and even to choose from among mixtures of these methods.},
  keywords = {Cross-validation,Ridge parameter,Ridge regression},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1979.10489751},
  file = {/home/bolker/Zotero/storage/4FG7CYQL/Golub et al. - 1979 - Generalized Cross-Validation as a Method for Choos.pdf}
}

@misc{larsenGAM2015,
  title = {{{GAM}}: {{The Predictive Modeling Silver Bullet}} | {{Stitch Fix Technology}} \textendash{} {{Multithreaded}}},
  shorttitle = {{{GAM}}},
  author = {Larsen, Kim},
  year = {2015},
  month = jul,
  journal = {MultiThreaded (StitchFix)},
  abstract = {Imagine that you step into a room of data scientists; the dress code is casual and the scent of strong coffee is hanging in the air. You ask the data scienti...},
  url = {https://multithreaded.stitchfix.com/blog/2015/07/30/gam/},
  file = {/home/bolker/Zotero/storage/5XYN6F7U/gam.html}
}


@article{el-bachirFast,
	title = {Fast {Automatic} {Smoothing} for {Generalized} {Additive} {Models}},
	abstract = {Generalized additive models (GAMs) are regression models wherein parameters of probability distributions depend on input variables through a sum of smooth functions, whose degrees of smoothness are selected by L2 regularization. Such models have become the de-facto standard nonlinear regression models when interpretability and ﬂexibility are required, but reliable and fast methods for automatic smoothing in large data sets are still lacking. We develop a general methodology for automatically learning the optimal degree of L2 regularization for GAMs using an empirical Bayes approach. The smooth functions are penalized by hyper-parameters that are learned simultaneously by maximization of a marginal likelihood using an approximate expectation-maximization algorithm. The latter involves a double Laplace approximation at the E-step, and leads to an eﬃcient M-step. Empirical analysis shows that the resulting algorithm is numerically stable, faster than the best existing methods and achieves state-of-the-art accuracy. For illustration, we apply it to an important and challenging problem in the analysis of extremal data.},
	language = {en},
	author = {El-Bachir, Yousra and Davison, Anthony C},
	file = {El-Bachir and Davison - Fast Automatic Smoothing for Generalized Additive .pdf:/home/bolker/Documents/zotero_new/storage/EDQVMPTY/El-Bachir and Davison - Fast Automatic Smoothing for Generalized Additive .pdf:application/pdf},
}

@article{marraPractical2011,
	title = {Practical variable selection for generalized additive models},
	volume = {55},
	issn = {0167-9473},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947311000491},
	doi = {10.1016/j.csda.2011.02.004},
	abstract = {The problem of variable selection within the class of generalized additive models, when there are many covariates to choose from but the number of predictors is still somewhat smaller than the number of observations, is considered. Two very simple but effective shrinkage methods and an extension of the nonnegative garrote estimator are introduced. The proposals avoid having to use nonparametric testing methods for which there is no general reliable distributional theory. Moreover, component selection is carried out in one single step as opposed to many selection procedures which involve an exhaustive search of all possible models. The empirical performance of the proposed methods is compared to that of some available techniques via an extensive simulation study. The results show under which conditions one method can be preferred over another, hence providing applied researchers with some practical guidelines. The procedures are also illustrated analysing data on plasma beta-carotene levels from a cross-sectional study conducted in the United States.},
	language = {en},
	number = {7},
	urldate = {2023-02-07},
	journal = {Computational Statistics \& Data Analysis},
	author = {Marra, Giampiero and Wood, Simon N.},
	month = jul,
	year = {2011},
	keywords = {Generalized additive model, Nonnegative garrote estimator, Penalized thin plate regression spline, Practical variable selection, Shrinkage smoother},
	pages = {2372--2387},
	file = {ScienceDirect Full Text PDF:/home/bolker/Documents/zotero_new/storage/XAAWNJJ6/Marra and Wood - 2011 - Practical variable selection for generalized addit.pdf:application/pdf;ScienceDirect Snapshot:/home/bolker/Documents/zotero_new/storage/FS7AT22Y/S0167947311000491.html:text/html},
}

@article{woodFast2011,
	title = {Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models},
	volume = {73},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2010.00749.x},
	doi = {10.1111/j.1467-9868.2010.00749.x},
	abstract = {Summary. Recent work by Reiss and Ogden provides a theoretical basis for sometimes preferring restricted maximum likelihood (REML) to generalized cross-validation (GCV) for smoothing parameter selection in semiparametric regression. However, existing REML or marginal likelihood (ML) based methods for semiparametric generalized linear models (GLMs) use iterative REML or ML estimation of the smoothing parameters of working linear approximations to the GLM. Such indirect schemes need not converge and fail to do so in a non-negligible proportion of practical analyses. By contrast, very reliable prediction error criteria smoothing parameter selection methods are available, based on direct optimization of GCV, or related criteria, for the GLM itself. Since such methods directly optimize properly defined functions of the smoothing parameters, they have much more reliable convergence properties. The paper develops the first such method for REML or ML estimation of smoothing parameters. A Laplace approximation is used to obtain an approximate REML or ML for any GLM, which is suitable for efficient direct optimization. This REML or ML criterion requires that Newton–Raphson iteration, rather than Fisher scoring, be used for GLM fitting, and a computationally stable approach to this is proposed. The REML or ML criterion itself is optimized by a Newton method, with the derivatives required obtained by a mixture of implicit differentiation and direct methods. The method will cope with numerical rank deficiency in the fitted model and in fact provides a slight improvement in numerical robustness on the earlier method of Wood for prediction error criteria based smoothness selection. Simulation results suggest that the new REML and ML methods offer some improvement in mean-square error performance relative to GCV or Akaike's information criterion in most cases, without the small number of severe undersmoothing failures to which Akaike's information criterion and GCV are prone. This is achieved at the same computational cost as GCV or Akaike's information criterion. The new approach also eliminates the convergence failures of previous REML- or ML-based approaches for penalized GLMs and usually has lower computational cost than these alternatives. Example applications are presented in adaptive smoothing, scalar on function regression and generalized additive model selection.},
	language = {en},
	number = {1},
	urldate = {2023-02-07},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Wood, Simon N.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2010.00749.x},
	keywords = {Adaptive smoothing, Generalized additive mixed model, Generalized additive model, Generalized cross-validation, Marginal likelihood, Model selection, Penalized generalized linear model, Penalized regression splines, Restricted maximum likelihood, Scalar on function regression, Stable computation},
	pages = {3--36},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/STZE99H9/Wood - 2011 - Fast stable restricted maximum likelihood and marg.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/YBY9GSA9/j.1467-9868.2010.00749.html:text/html},
}

@article{reissSmoothing2009,
	title = {Smoothing parameter selection for a class of semiparametric linear models},
	volume = {71},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2008.00695.x},
	doi = {10.1111/j.1467-9868.2008.00695.x},
	abstract = {Summary. Spline-based approaches to non-parametric and semiparametric regression, as well as to regression of scalar outcomes on functional predictors, entail choosing a parameter controlling the extent to which roughness of the fitted function is penalized. We demonstrate that the equations determining two popular methods for smoothing parameter selection, generalized cross-validation and restricted maximum likelihood, share a similar form that allows us to prove several results which are common to both, and to derive a condition under which they yield identical values. These ideas are illustrated by application of functional principal component regression, a method for regressing scalars on functions, to two chemometric data sets.},
	language = {en},
	number = {2},
	urldate = {2023-02-07},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Reiss, Philip T. and Ogden, R. Todd},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2008.00695.x},
	keywords = {B-splines, Functional linear model, Functional principal component regression, Generalized cross-validation, Linear mixed model, Roughness penalty},
	pages = {505--523},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/IL7LZIWR/Reiss and Todd Ogden - 2009 - Smoothing parameter selection for a class of semip.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/AP2LYYKQ/j.1467-9868.2008.00695.html:text/html},
}

@article{hastieGeneralized1987a,
	title = {Generalized {Additive} {Models}: {Some} {Applications}},
	volume = {82},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Generalized {Additive} {Models}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1987.10478440},
	doi = {10.1080/01621459.1987.10478440},
	language = {en},
	number = {398},
	urldate = {2023-02-07},
	journal = {Journal of the American Statistical Association},
	author = {Hastie, Trevor and Tibshirani, Robert},
	month = jun,
	year = {1987},
	pages = {371--386},
	file = {Hastie and Tibshirani - 1987 - Generalized Additive Models Some Applications.pdf:/home/bolker/Documents/zotero_new/storage/R64VQXE4/Hastie and Tibshirani - 1987 - Generalized Additive Models Some Applications.pdf:application/pdf},
}


@article{breimanEstimating1985,
	title = {Estimating {Optimal} {Transformations} for {Multiple} {Regression} and {Correlation}},
	volume = {80},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1985.10478157},
	doi = {10.1080/01621459.1985.10478157},
	abstract = {In regression analysis the response variable Y and the predictor variables X 1 …, Xp are often replaced by functions θ(Y) and Ø1(X 1), …, Ø p (Xp ). We discuss a procedure for estimating those functions θ and Ø1, …, Ø p that minimize e 2 = E[θ(Y) — Σ Ø j (Xj )]2/var[θ(Y)], given only a sample (yk , xk1 , …, xkp ), 1 ⩽ k ⩽ N and making minimal assumptions concerning the data distribution or the form of the solution functions. For the bivariate case, p = 1, θ and Ø satisfy ρ = p(θ, Ø) = maxθ,Øρ[θ(Y), Ø(X)], where ρ is the product moment correlation coefficient and ρ is the maximal correlation between X and Y. Our procedure thus also provides a method for estimating the maximal correlation between two variables.},
	number = {391},
	urldate = {2023-02-07},
	journal = {Journal of the American Statistical Association},
	author = {Breiman, Leo and Friedman, Jerome H.},
	month = sep,
	year = {1985},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1985.10478157},
	keywords = {ACE, Smoothing},
	pages = {580--598},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/UEE5F44K/Breiman and Friedman - 1985 - Estimating Optimal Transformations for Multiple Re.pdf:application/pdf},
}


@article{eilersFlexible1996,
	title = {Flexible smoothing with {B}-splines and penalties},
	volume = {11},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-11/issue-2/Flexible-smoothing-with-B-splines-and-penalties/10.1214/ss/1038425655.full},
	doi = {10.1214/ss/1038425655},
	abstract = {B-splines are attractive for nonparametric modelling, but choosing the optimal number and positions of knots is a complex task. Equidistant knots can be used, but their small and discrete number allows only limited control over smoothness and fit. We propose to use a relatively large number of knots and a difference penalty on coefficients of adjacent B-splines. We show connections to the familiar spline penalty on the integral of the squared second derivative. A short overview of B-splines, of their construction and of penalized likelihood is presented. We discuss properties of penalized B-splines and propose various criteria for the choice of an optimal penalty parameter. Nonparametric logistic regression, density estimation and scatterplot smoothing are used as examples. Some details of the computations are presented.},
	number = {2},
	urldate = {2023-02-12},
	journal = {Statistical Science},
	author = {Eilers, Paul H. C. and Marx, Brian D.},
	month = may,
	year = {1996},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Density estimation, generalized linear models, nonparametric models, smoothing, splines},
	pages = {89--121},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/YHR93PJA/Eilers and Marx - 1996 - Flexible smoothing with B-splines and penalties.pdf:application/pdf},
}


@article{woodPsplines2017,
	title = {P-splines with derivative based penalties and tensor product smoothing of unevenly distributed data},
	volume = {27},
	issn = {0960-3174, 1573-1375},
	url = {http://arxiv.org/abs/1605.02446},
	doi = {10.1007/s11222-016-9666-x},
	abstract = {The P-splines of Eilers and Marx (1996) combine a B-spline basis with a discrete quadratic penalty on the basis coefﬁcients, to produce a reduced rank spline like smoother. P-splines have three properties that make them very popular as reduced rank smoothers: i) the basis and the penalty are sparse, enabling efﬁcient computation, especially for Bayesian stochastic simulation; ii) it is possible to ﬂexibly ‘mix-and-match’ the order of B-spline basis and penalty, rather than the order of penalty controlling the order of the basis as in spline smoothing; iii) it is very easy to set up the Bspline basis functions and penalties. The discrete penalties are somewhat less interpretable in terms of function shape than the traditional derivative based spline penalties, but tend towards penalties proportional to traditional spline penalties in the limit of large basis size. However part of the point of P-splines is not to use a large basis size. In addition the spline basis functions arise from solving functional optimization problems involving derivative based penalties, so moving to discrete penalties for smoothing may not always be desirable. The purpose of this note is to point out that the three properties of basis-penalty sparsity, mix-and-match penalization and ease of setup are readily obtainable with B-splines subject to derivative based penalization. The penalty setup typically requires a few lines of code, rather than the two lines typically required for P-splines, but this one off disadvantage seems to be the only one associated with using derivative based penalties. As an example application, it is shown how basis-penalty sparsity enables efﬁcient computation with tensor product smoothers of scattered data.},
	language = {en},
	number = {4},
	urldate = {2023-02-12},
	journal = {Statistics and Computing},
	author = {Wood, Simon N.},
	month = jul,
	year = {2017},
	note = {arXiv:1605.02446 [stat]},
	keywords = {Statistics - Computation},
	pages = {985--989},
	file = {Wood - 2017 - P-splines with derivative based penalties and tens.pdf:/home/bolker/Documents/zotero_new/storage/PNFIMTZ5/Wood - 2017 - P-splines with derivative based penalties and tens.pdf:application/pdf},
}

@article{wandPenalized2011,
	title = {Penalized wavelets: {Embedding} wavelets into semiparametric regression},
	volume = {5},
	issn = {1935-7524},
	shorttitle = {Penalized wavelets},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-5/issue-none/Penalized-wavelets-Embedding-wavelets-into-semiparametric-regression/10.1214/11-EJS652.full},
	doi = {10.1214/11-EJS652},
	abstract = {We introduce the concept of penalized wavelets to facilitate seamless embedding of wavelets into semiparametric regression models. In particular, we show that penalized wavelets are analogous to penalized splines; the latter being the established approach to function estimation in semiparametric regression. They differ only in the type of penalization that is appropriate. This fact is not borne out by the existing wavelet literature, where the regression modelling and ﬁtting issues are overshadowed by computational issues such as efﬁciency gains afforded by the Discrete Wavelet Transform and partially obscured by a tendency to work in the wavelet coefﬁcient space. With penalized wavelet structure in place, we then show that ﬁtting and inference can be achieved via the same general approaches used for penalized splines: penalized least squares, maximum likelihood and best prediction within a frequentist mixed model framework, and Markov chain Monte Carlo and mean ﬁeld variational Bayes within a Bayesian framework. Penalized wavelets are also shown have a close relationship with wide data (“p n”) regression and beneﬁt from ongoing research on that topic.},
	language = {en},
	number = {none},
	urldate = {2022-12-28},
	journal = {Electronic Journal of Statistics},
	author = {Wand, M.P. and Ormerod, J.T.},
	month = jan,
	year = {2011},
	file = {Wand and Ormerod - 2011 - Penalized wavelets Embedding wavelets into semipa.pdf:/home/bolker/Documents/zotero_new/storage/L9Y7CTJ2/Wand and Ormerod - 2011 - Penalized wavelets Embedding wavelets into semipa.pdf:application/pdf},
}

@article{donohoWavelet1995,
	title = {Wavelet {Shrinkage}: {Asymptopia}?},
	volume = {57},
	issn = {2517-6161},
	shorttitle = {Wavelet {Shrinkage}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02032.x},
	doi = {10.1111/j.2517-6161.1995.tb02032.x},
	abstract = {Much recent effort has sought asymptotically minimax methods for recovering infinite dimensional objects—curves, densities, spectral densities, images—from noisy data. A now rich and complex body of work develops nearly or exactly minimax estimators for an array of interesting problems. Unfortunately, the results have rarely moved into practice, for a variety of reasons—among them being similarity to known methods, computational intractability and lack of spatial adaptivity. We discuss a method for curve estimation based on n noisy data: translate the empirical wavelet coefficients towards the origin by an amount √(2 log n)σ/√n. The proposal differs from those in current use, is computationally practical and is spatially adaptive; it thus avoids several of the previous objections. Further, the method is nearly minimax both for a wide variety of loss functions—pointwise error, global error measured in Lp-norms, pointwise and global error in estimation of derivatives—and for a wide range of smoothness classes, including standard Holder and Sobolev classes, and bounded variation. This is a much broader near optimality than anything previously proposed: we draw loose parallels with near optimality in robustness and also with the broad near eigenfunction properties of wavelets themselves. Finally, the theory underlying the method is interesting, as it exploits a correspondence between statistical questions and questions of optimal recovery and information-based complexity.},
	language = {en},
	number = {2},
	urldate = {2022-12-26},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Donoho, David L. and Johnstone, Iain M. and Kerkyacharian, Gérard and Picard, Dominique},
	year = {1995},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1995.tb02032.x},
	keywords = {adaptive estimation, besov spaces, density estimation, minimax estimation, nonparametric regression, optimal recovery, spatial adaptation, wavelet orthonormal bases},
	pages = {301--337},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/7UGB74ZK/Donoho et al. - 1995 - Wavelet Shrinkage Asymptopia.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/6KWC8CMX/j.2517-6161.1995.tb02032.html:text/html},
}

@article{abramovichWavelet1998,
	title = {Wavelet thresholding via a {Bayesian} approach},
	volume = {60},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00151},
	doi = {10.1111/1467-9868.00151},
	abstract = {We discuss a Bayesian formalism which gives rise to a type of wavelet threshold estimation in nonparametric regression. A prior distribution is imposed on the wavelet coefficients of the unknown response function, designed to capture the sparseness of wavelet expansion that is common to most applications. For the prior specified, the posterior median yields a thresholding procedure. Our prior model for the underlying function can be adjusted to give functions falling in any specific Besov space. We establish a relationship between the hyperparameters of the prior model and the parameters of those Besov spaces within which realizations from the prior will fall. Such a relationship gives insight into the meaning of the Besov space parameters. Moreover, the relationship established makes it possible in principle to incorporate prior knowledge about the function's regularity properties into the prior model for its wavelet coefficients. However, prior knowledge about a function's regularity properties might be difficult to elicit; with this in mind, we propose a standard choice of prior hyperparameters that works well in our examples. Several simulated examples are used to illustrate our method, and comparisons are made with other thresholding methods. We also present an application to a data set that was collected in an anaesthesiological study.},
	language = {en},
	number = {4},
	urldate = {2022-12-26},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Abramovich, F. and Sapatinas, T. and Silverman, B. W.},
	year = {1998},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00151},
	keywords = {Nonparametric regression, Adaptive estimation, Anaesthetics, Bayes model, Besov spaces, Thresholding, Wavelet transform},
	pages = {725--749},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/NL5QH5UZ/1467-9868.html:text/html},
}

@article{barberReal2004,
	title = {Real nonparametric regression using complex wavelets},
	volume = {66},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2004.B5604.x},
	doi = {10.1111/j.1467-9868.2004.B5604.x},
	abstract = {Summary. Wavelet shrinkage is an effective nonparametric regression technique, especially when the underlying curve has irregular features such as spikes or discontinuities. The basic idea is simple: take the discrete wavelet transform of data consisting of a signal corrupted by noise; shrink or remove the wavelet coefficients to remove the noise; then invert the discrete wavelet transform to form an estimate of the true underlying curve. Various researchers have proposed increasingly sophisticated methods of doing this by using real-valued wavelets. Complex-valued wavelets exist but are rarely used. We propose two new complex-valued wavelet shrinkage techniques: one based on multiwavelet style shrinkage and the other using Bayesian methods. Extensive simulations show that our methods almost always give significantly more accurate estimates than methods based on real-valued wavelets. Further, our multiwavelet style shrinkage method is both simpler and dramatically faster than its competitors. To understand the excellent performance of this method we present a new risk bound on its hard thresholded coefficients.},
	language = {en},
	number = {4},
	urldate = {2022-12-26},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Barber, Stuart and Nason, Guy P.},
	year = {2004},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2004.B5604.x},
	keywords = {Complex normal distribution, Complex-valued wavelets, Curve estimation, Empirical Bayes method, Multiwavelets, Wavelet shrinkage},
	pages = {927--939},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/IZ9AIQ6J/j.1467-9868.2004.B5604.html:text/html},
}


@article{woodThin2003,
	title = {Thin plate regression splines},
	volume = {65},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00374},
	doi = {10.1111/1467-9868.00374},
	abstract = {Summary. I discuss the production of low rank smoothers for d ≥ 1 dimensional data, which can be fitted by regression or penalized regression methods. The smoothers are constructed by a simple transformation and truncation of the basis that arises from the solution of the thin plate spline smoothing problem and are optimal in the sense that the truncation is designed to result in the minimum possible perturbation of the thin plate spline smoothing problem given the dimension of the basis used to construct the smoother. By making use of Lanczos iteration the basis change and truncation are computationally efficient. The smoothers allow the use of approximate thin plate spline models with large data sets, avoid the problems that are associated with ‘knot placement’ that usually complicate modelling with regression splines or penalized regression splines, provide a sensible way of modelling interaction terms in generalized additive models, provide low rank approximations to generalized smoothing spline models, appropriate for use with large data sets, provide a means for incorporating smooth functions of more than one variable into non-linear models and improve the computational efficiency of penalized likelihood models incorporating thin plate splines. Given that the approach produces spline-like models with a sparse basis, it also provides a natural way of incorporating unpenalized spline-like terms in linear and generalized linear models, and these can be treated just like any other model terms from the point of view of model selection, inference and diagnostics.},
	language = {en},
	number = {1},
	urldate = {2023-02-13},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Wood, Simon N.},
	year = {2003},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00374},
	keywords = {Generalized additive model, Regression spline, Thin plate spline},
	pages = {95--114},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/VJKXGIPM/Wood - 2003 - Thin plate regression splines.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/MA6MW9DA/1467-9868.html:text/html},
}



@article{barberPredictive2021,
	title = {Predictive inference with the jackknife+},
	volume = {49},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-1/Predictive-inference-with-the-jackknife/10.1214/20-AOS1965.full},
	doi = {10.1214/20-AOS1965},
	abstract = {This paper introduces the jackknife+, which is a novel method for constructing predictive confidence intervals. Whereas the jackknife outputs an interval centered at the predicted response of a test point, with the width of the interval determined by the quantiles of leave-one-out residuals, the jackknife+ also uses the leave-one-out predictions at the test point to account for the variability in the fitted regression function. Assuming exchangeable training samples, we prove that this crucial modification permits rigorous coverage guarantees regardless of the distribution of the data points, for any algorithm that treats the training points symmetrically. Such guarantees are not possible for the original jackknife and we demonstrate examples where the coverage rate may actually vanish. Our theoretical and empirical analysis reveals that the jackknife and the jackknife+ intervals achieve nearly exact coverage and have similar lengths whenever the fitting algorithm obeys some form of stability. Further, we extend the jackknife+ to \$K\$-fold cross validation and similarly establish rigorous coverage properties. Our methods are related to cross-conformal prediction proposed by Vovk (Ann. Math. Artif. Intell. 74 (2015) 9–28) and we discuss connections.},
	number = {1},
	urldate = {2023-02-21},
	journal = {The Annals of Statistics},
	author = {Barber, Rina Foygel and Candès, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
	month = feb,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62F40, 62G08, 62G09, conformal inference, cross-validation, distribution-free, jackknife, leave-one-out, stability},
	pages = {486--507},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/7UD6XJM5/Barber et al. - 2021 - Predictive inference with the jackknife+.pdf:application/pdf},
}

@article{efronLeisurely1983a,
	title = {A {Leisurely} {Look} at the {Bootstrap}, the {Jackknife}, and {Cross}-{Validation}},
	volume = {37},
	issn = {0003-1305},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1983.10483087},
	doi = {10.1080/00031305.1983.10483087},
	abstract = {This is an invited expository article for The American Statistician. It reviews the nonparametric estimation of statistical error, mainly the bias and standard error of an estimator, or the error rate of a prediction rule. The presentation is written at a relaxed mathematical level, omitting most proofs, regularity conditions, and technical details.},
	number = {1},
	urldate = {2023-02-21},
	journal = {The American Statistician},
	author = {Efron, Bradley and Gong, Gail},
	month = feb,
	year = {1983},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1983.10483087},
	keywords = {Bias estimation, Error rate prediction, Nonparametric confidence intervals, Nonparametric standard errors, Variance estimation},
	pages = {36--48},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/ZH9C3XRX/Efron and Gong - 1983 - A Leisurely Look at the Bootstrap, the Jackknife, .pdf:application/pdf},
}


@article{shaferTutorial2008a,
	title = {A {Tutorial} on {Conformal} {Prediction}},
	volume = {9},
	abstract = {Conformal prediction uses past experience to determine precise levels of conﬁdence in new predictions. Given an error probability ε, together with a method that makes a prediction yˆ of a label y, it produces a set of labels, typically containing yˆ, that also contains y with probability 1 − ε. Conformal prediction can be applied to any method for producing yˆ: a nearest-neighbor method, a support-vector machine, ridge regression, etc.},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Shafer, Glenn and Vovk, Vladimir},
	year = {2008},
	pages = {371--421},
	file = {Shafer and Vovk - A Tutorial on Conformal Prediction.pdf:/home/bolker/Documents/zotero_new/storage/RZ6HW86E/Shafer and Vovk - A Tutorial on Conformal Prediction.pdf:application/pdf},
}

@article{wagerConfidence,
	title = {Conﬁdence {Intervals} for {Random} {Forests}: {The} {Jackknife} and the {Inﬁnitesimal} {Jackknife}},
	abstract = {We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2013) that are based on the jackknife and the inﬁnitesimal jackknife (IJ). In practice, bagged predictors are computed using a ﬁnite number B of bootstrap replicates, and working with a large B can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require B = Θ(n1.5) bootstrap replicates to converge, where n is the size of the training set. We propose improved versions that only require B = Θ(n) replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our ﬁndings with multiple experiments and simulation studies.},
	language = {en},
	author = {Wager, Stefan and Hastie, Trevor and Efron, Bradley},
	file = {Wager et al. - Conﬁdence Intervals for Random Forests The Jackkn.pdf:/home/bolker/Documents/zotero_new/storage/AMGKAHAI/Wager et al. - Conﬁdence Intervals for Random Forests The Jackkn.pdf:application/pdf},
}


@article{vandengoorberghharm2022,
	title = {The harm of class imbalance corrections for risk prediction models: illustration and simulation using logistic regression},
	issn = {1527-974X},
	shorttitle = {The harm of class imbalance corrections for risk prediction models},
	url = {https://doi.org/10.1093/jamia/ocac093},
	doi = {10.1093/jamia/ocac093},
	abstract = {Methods to correct class imbalance (imbalance between the frequency of outcome events and nonevents) are receiving increasing interest for developing prediction models. We examined the effect of imbalance correction on the performance of logistic regression models.Prediction models were developed using standard and penalized (ridge) logistic regression under 4 methods to address class imbalance: no correction, random undersampling, random oversampling, and SMOTE. Model performance was evaluated in terms of discrimination, calibration, and classification. Using Monte Carlo simulations, we studied the impact of training set size, number of predictors, and the outcome event fraction. A case study on prediction modeling for ovarian cancer diagnosis is presented.The use of random undersampling, random oversampling, or SMOTE yielded poorly calibrated models: the probability to belong to the minority class was strongly overestimated. These methods did not result in higher areas under the ROC curve when compared with models developed without correction for class imbalance. Although imbalance correction improved the balance between sensitivity and specificity, similar results were obtained by shifting the probability threshold instead.Imbalance correction led to models with strong miscalibration without better ability to distinguish between patients with and without the outcome event. The inaccurate probability estimates reduce the clinical utility of the model, because decisions about treatment are ill-informed.Outcome imbalance is not a problem in itself, imbalance correction may even worsen model performance.},
	urldate = {2022-06-10},
	journal = {Journal of the American Medical Informatics Association},
	author = {{van den Goorbergh}, Ruben and van Smeden, Maarten and Timmerman, Dirk and Van Calster, Ben},
	month = jun,
	year = {2022},
	pages = {ocac093},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/S4H8AD5I/van den Goorbergh et al. - 2022 - The harm of class imbalance corrections for risk p.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/LPRT453B/6605096.html:text/html},
}


@article{handMeasuring2009,
	title = {Measuring classifier performance: a coherent alternative to the area under the {ROC} curve},
	volume = {77},
	issn = {0885-6125, 1573-0565},
	shorttitle = {Measuring classifier performance},
	url = {http://link.springer.com/10.1007/s10994-009-5119-5},
	doi = {10.1007/s10994-009-5119-5},
	abstract = {The area under the ROC curve (AUC) is a very widely used measure of performance for classiﬁcation and diagnostic rules. It has the appealing property of being objective, requiring no subjective input from the user. On the other hand, the AUC has disadvantages, some of which are well known. For example, the AUC can give potentially misleading results if ROC curves cross. However, the AUC also has a much more serious deﬁciency, and one which appears not to have been previously recognised. This is that it is fundamentally incoherent in terms of misclassiﬁcation costs: the AUC uses different misclassiﬁcation cost distributions for different classiﬁers. This means that using the AUC is equivalent to using different metrics to evaluate different classiﬁcation rules. It is equivalent to saying that, using one classiﬁer, misclassifying a class 1 point is p times as serious as misclassifying a class 0 point, but, using another classiﬁer, misclassifying a class 1 point is P times as serious, where p = P . This is nonsensical because the relative severities of different kinds of misclassiﬁcations of individual points is a property of the problem, not the classiﬁers which happen to have been chosen. This property is explored in detail, and a simple valid alternative to the AUC is proposed.},
	language = {en},
	number = {1},
	urldate = {2022-03-30},
	journal = {Machine Learning},
	author = {Hand, David J.},
	month = oct,
	year = {2009},
	pages = {103--123},
	file = {Hand - 2009 - Measuring classifier performance a coherent alter.pdf:/home/bolker/Documents/zotero_new/storage/UV63UCY6/Hand - 2009 - Measuring classifier performance a coherent alter.pdf:application/pdf},
}



@article{yangCan2005,
	title = {Can the strengths of {AIC} and {BIC} be shared? {A} conflict between model identification and regression estimation},
	volume = {92},
	shorttitle = {Can the strengths of {AIC} and {BIC} be shared?},
	url = {http://biomet.oxfordjournals.org/cgi/content/abstract/92/4/937},
	doi = {10.1093/biomet/92.4.937},
	abstract = {A traditional approach to statistical inference is to identify the true or best model first with little or no consideration of the specific goal of inference in the model identification stage. Can the pursuit of the true model also lead to optimal regression estimation? In model selection, it is well known that BIC is consistent in selecting the true model, and AIC is minimax-rate optimal for estimating the regression function. A recent promising direction is adaptive model selection, in which, in contrast to AIC and BIC, the penalty term is data-dependent. Some theoretical and empirical results have been obtained in support of adaptive model selection, but it is still not clear if it can really share the strengths of AIC and BIC. Model combining or averaging has attracted increasing attention as a means to overcome the model selection uncertainty. Can Bayesian model averaging be optimal for estimating the regression function in a minimax sense? We show that the answers to these questions are basically in the negative: for any model selection criterion to be consistent, it must behave suboptimally for estimating the regression function in terms of minimax rate of covergence; and Bayesian model averaging cannot be minimax-rate optimal for regression estimation.},
	number = {4},
	urldate = {2009-02-15},
	journal = {Biometrika},
	author = {Yang, Yuhong},
	month = dec,
	year = {2005},
	pages = {937--950},
	file = {HighWire Full Text PDF:/home/bolker/Documents/zotero_new/storage/JBGMQWCJ/Yang - 2005 - Can the strengths of AIC and BIC be shared A conf.pdf:application/pdf;HighWire Snapshot:/home/bolker/Documents/zotero_new/storage/SX3JIZDN/937.html:text/html},
}



@article{chawlaSMOTE2002,
	title = {{SMOTE}: {Synthetic} {Minority} {Over}-sampling {Technique}},
	volume = {16},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {{SMOTE}},
	url = {https://www.jair.org/index.php/jair/article/view/10302},
	doi = {10.1613/jair.953},
	abstract = {An approach to the construction of classifiers from    imbalanced datasets is described. A dataset is imbalanced if the    classification categories are not approximately equally    represented. Often real-world data sets are predominately composed of    ``normal'' examples with only a small percentage of ``abnormal'' or    ``interesting'' examples. It is also the case that the cost of    misclassifying an abnormal (interesting) example as a normal example    is often much higher than the cost of the reverse    error. Under-sampling of the majority (normal) class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class. This paper shows that a combination of our method of    over-sampling the minority (abnormal) class and under-sampling the    majority (normal) class can achieve better classifier performance (in    ROC space) than only under-sampling the majority class.  This paper    also shows that a combination of our method of over-sampling the    minority class and under-sampling the majority class can achieve    better classifier performance (in ROC space) than varying the loss    ratios in Ripper or class priors in Naive Bayes. Our method of    over-sampling the minority class involves creating synthetic minority    class examples.  Experiments are performed using C4.5, Ripper and a    Naive Bayes classifier. The method is evaluated using the area under    the Receiver Operating Characteristic curve (AUC) and the ROC convex    hull strategy.},
	language = {en},
	urldate = {2023-02-24},
	journal = {Journal of Artificial Intelligence Research},
	author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
	month = jun,
	year = {2002},
	pages = {321--357},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/3E8RDDS7/Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf:application/pdf},
}


@article{clarkeStatistical2014,
	title = {Statistical {Problem} {Classes} and {Their} {Links} to {Information} {Theory}},
	volume = {33},
	issn = {0747-4938},
	url = {https://doi.org/10.1080/07474938.2013.807190},
	doi = {10.1080/07474938.2013.807190},
	abstract = {We begin by recalling the tripartite division of statistical problems into three classes, M-closed, M-complete, and M-open and then reviewing the key ideas of introductory Shannon theory. Focusing on the related but distinct goals of model selection and prediction, we argue that different techniques for these two goals are appropriate for the three different problem classes. For M-closed problems we give relative entropy justification that the Bayes information criterion (BIC) is appropriate for model selection and that the Bayes model average is information optimal for prediction. For M-complete problems, we discuss the principle of maximum entropy and a way to use the rate distortion function to bypass the inaccessibility of the true distribution. For prediction in the M-complete class, there is little work done on information based model averaging so we discuss the Akaike information criterion (AIC) and its properties and variants. For the M-open class, we argue that essentially only predictive criteria are suitable. Thus, as an analog to model selection, we present the key ideas of prediction along a string under a codelength criterion and propose a general form of this criterion. Since little work appears to have been done on information methods for general prediction in the M-open class of problems, we mention the field of information theoretic learning in certain general function spaces.},
	number = {1-4},
	urldate = {2020-10-09},
	journal = {Econometric Reviews},
	author = {Clarke, Bertrand and Clarke, Jennifer and Yu, Chi Wai},
	month = feb,
	year = {2014},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/07474938.2013.807190},
	keywords = {Bayesian, C52, C53, Codelength, Entropy, Information theory, M-closed, M-complete, M-open, Model selection, Mutual information, Prediction, Rate distortion, Relative entropy},
	pages = {337--371},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/ZR4HGYXW/07474938.2013.html:text/html},
}


@article{chenOne2021,
	title = {The {One} {Standard} {Error} {Rule} for {Model} {Selection}: {Does} {It} {Work}?},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2571-905X},
	shorttitle = {The {One} {Standard} {Error} {Rule} for {Model} {Selection}},
	url = {https://www.mdpi.com/2571-905X/4/4/51},
	doi = {10.3390/stats4040051},
	abstract = {Previous research provided a lot of discussion on the selection of regularization parameters when it comes to the application of regularization methods for high-dimensional regression. The popular “One Standard Error Rule” (1se rule) used with cross validation (CV) is to select the most parsimonious model whose prediction error is not much worse than the minimum CV error. This paper examines the validity of the 1se rule from a theoretical angle and also studies its estimation accuracy and performances in applications of regression estimation and variable selection, particularly for Lasso in a regression framework. Our theoretical result shows that when a regression procedure produces the regression estimator converging relatively fast to the true regression function, the standard error estimation formula in the 1se rule is justified asymptotically. The numerical results show the following: 1. the 1se rule in general does not necessarily provide a good estimation for the intended standard deviation of the cross validation error. The estimation bias can be 50–100\% upwards or downwards in various situations; 2. the results tend to support that 1se rule usually outperforms the regular CV in sparse variable selection and alleviates the over-selection tendency of Lasso; 3. in regression estimation or prediction, the 1se rule often performs worse. In addition, comparisons are made over two real data sets: Boston Housing Prices (large sample size n, small/moderate number of variables p) and Bardet–Biedl data (large p, small n). Data guided simulations are done to provide insight on the relative performances of the 1se rule and the regular CV.},
	language = {en},
	number = {4},
	urldate = {2023-02-24},
	journal = {Stats},
	author = {Chen, Yuchen and Yang, Yuhong},
	month = dec,
	year = {2021},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {estimation accuracy, regression estimation, subsampling, tuning parameter selection, variable selection},
	pages = {868--892},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/EK2RTTGI/Chen and Yang - 2021 - The One Standard Error Rule for Model Selection D.pdf:application/pdf},
}


@misc{bussolaAI2020,
	title = {{AI} slipping on tiles: data leakage in digital pathology},
	shorttitle = {{AI} slipping on tiles},
	url = {http://arxiv.org/abs/1909.06539},
	doi = {10.48550/arXiv.1909.06539},
	abstract = {Reproducibility of AI models on biomedical data still stays as a major concern for their acceptance into the clinical practice. Initiatives for reproducibility in the development of predictive biomarkers as the MAQC Consortium already underlined the importance of appropriate Data Analysis Plans (DAPs) to control for different types of bias, including data leakage from the training to the test set. In the context of digital pathology, the leakage typically lurks in weakly designed experiments not accounting for the subjects in their data partitioning schemes. This issue is then exacerbated when fractions or subregions of slides (i.e. "tiles") are considered. Despite this aspect is largely recognized by the community, we argue that it is often overlooked. In this study, we assess the impact of data leakage on the performance of machine learning models trained and validated on multiple histology data collection. We prove that, even with a properly designed DAP (10x5 repeated cross-validation), predictive scores can be inflated up to 41\% when tiles from the same subject are used both in training and validation sets by deep learning models. We replicate the experiments for \$4\$ classification tasks on 3 histopathological datasets, for a total of 374 subjects, 556 slides and more than 27,000 tiles. Also, we discuss the effects of data leakage on transfer learning strategies with models pre-trained on general-purpose datasets or off-task digital pathology collections. Finally, we propose a solution that automates the creation of leakage-free deep learning pipelines for digital pathology based on histolab, a novel Python package for histology data preprocessing. We validate the solution on two public datasets (TCGA and GTEx).},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Bussola, Nicole and Marcolini, Alessia and Maggio, Valerio and Jurman, Giuseppe and Furlanello, Cesare},
	month = nov,
	year = {2020},
	note = {arXiv:1909.06539 [eess, q-bio]},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/4SX8J7WC/Bussola et al. - 2020 - AI slipping on tiles data leakage in digital path.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/XH6QD7DJ/1909.html:text/html},
}


@article{wengerAssessing2012,
	title = {Assessing transferability of ecological models: an underappreciated aspect of statistical validation},
	volume = {3},
	issn = {2041210X},
	shorttitle = {Assessing transferability of ecological models},
	url = {http://doi.wiley.com/10.1111/j.2041-210X.2011.00170.x},
	doi = {10.1111/j.2041-210X.2011.00170.x},
	number = {2},
	urldate = {2013-06-29},
	journal = {Methods in Ecology and Evolution},
	author = {Wenger, Seth J. and Olden, Julian D.},
	month = apr,
	year = {2012},
	pages = {260--267},
	file = {Wenger_Olden_MEE2012.pdf:/home/bolker/Documents/zotero_new/storage/JHTX2G5N/Wenger_Olden_MEE2012.pdf:application/pdf},
}


@article{harrisGenerating2015,
	title = {Generating realistic assemblages with a joint species distribution model},
	volume = {6},
	issn = {2041-210X},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12332/abstract},
	doi = {10.1111/2041-210X.12332},
	abstract = {* Species distribution models (SDMs) represent important analytical and predictive tools for ecologists. Until now, these models have either assumed (i) that species' occurrence probabilities are uncorrelated or (ii) that species respond linearly to preselected environmental variables. These two assumptions currently prevent ecologists from modelling assemblages with realistic co-occurrence and species richness properties.


* This paper introduces a stochastic feedforward neural network, called ‘mistnet’, which makes neither assumption. Thus, unlike most SDMs, mistnet can account for non-independent co-occurrence patterns driven by unobserved environmental heterogeneity. And unlike several recently proposed joint SDMs, the model can also learn nonlinear functions relating species' occurrence probabilities to environmental predictors.


* Mistnet makes more accurate predictions about the North American bird communities found along Breeding Bird Survey transects than several alternative methods tested. In particular, typical assemblages held out of sample for validation were each tens of thousands of times more likely under the mistnet model than under independent combinations of single-species predictions.


* Apart from improved accuracy, mistnet shows two other important benefits for ecological research and management. First: by analysing co-occurrence data, mistnet can identify unmeasured – and perhaps unanticipated – environmental variables that drive species turnover. For example, the model identified a strong grassland/forest gradient, even though only temperature and precipitation were given as model inputs. Second: mistnet is able to take advantage of outside information to guide its predictions towards more realistic assemblages. For example, mistnet automatically adjusts its expectations to include more forest-associated species in response to a stray observation of a forest-dwelling warbler.},
	language = {en},
	number = {4},
	urldate = {2017-05-08},
	journal = {Methods in Ecology and Evolution},
	author = {Harris, David J.},
	month = apr,
	year = {2015},
	keywords = {Birds, species distribution model, breeding bird survey, neural network, probabilistic graphical model, species assemblages},
	pages = {465--473},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/9B2C3UKF/Harris - 2015 - Generating realistic assemblages with a joint spec.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/I9EVGAZ6/abstract.html:text/html},
}


@article{robertsCrossvalidation2017,
	title = {Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure},
	volume = {40},
	issn = {09067590},
	url = {http://doi.wiley.com/10.1111/ecog.02881},
	doi = {10.1111/ecog.02881},
	language = {en},
	number = {8},
	urldate = {2017-09-13},
	journal = {Ecography},
	author = {Roberts, David R. and Bahn, Volker and Ciuti, Simone and Boyce, Mark S. and Elith, Jane and Guillera-Arroita, Gurutzeta and Hauenstein, Severin and Lahoz-Monfort, José J. and Schröder, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Hartig, Florian and Dormann, Carsten F.},
	month = aug,
	year = {2017},
	pages = {913--929},
	file = {ecog2881.pdf:/home/bolker/Documents/zotero_new/storage/REZ7BUNJ/ecog2881.pdf:application/pdf},
}


@article{valaviblockCV2019,
	title = {{blockCV}: {An} r package for generating spatially or environmentally separated folds for k-fold cross-validation of species distribution models},
	volume = {10},
	issn = {2041-210X},
	shorttitle = {{blockCV}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13107},
	doi = {10.1111/2041-210X.13107},
	abstract = {When applied to structured data, conventional random cross-validation techniques can lead to underestimation of prediction error, and may result in inappropriate model selection. We present the r package blockCV, a new toolbox for cross-validation of species distribution modelling. Although it has been developed with species distribution modelling in mind, it can be used for any spatial modelling. The package can generate spatially or environmentally separated folds. It includes tools to measure spatial autocorrelation ranges in candidate covariates, providing the user with insights into the spatial structure in these data. It also offers interactive graphical capabilities for creating spatial blocks and exploring data folds. Package blockCV enables modellers to more easily implement a range of evaluation approaches. It will help the modelling community learn more about the impacts of evaluation approaches on our understanding of predictive performance of species distribution models.},
	language = {en},
	number = {2},
	urldate = {2023-02-24},
	journal = {Methods in Ecology and Evolution},
	author = {Valavi, Roozbeh and Elith, Jane and Lahoz-Monfort, José J. and Guillera-Arroita, Gurutzeta},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13107},
	keywords = {block cross-validation, environmental blocking, model evaluation, spatial autocorrelation, spatial blocking, spatial leave-one-out, species distribution modelling, structured environment},
	pages = {225--232},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/5V8LQLC5/Valavi et al. - 2019 - blockCV An r package for generating spatially or .pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/CAK7DWYG/2041-210X.html:text/html},
}


@article{cyguComparing2023,
	title = {Comparing machine learning approaches to incorporate time-varying covariates in predicting cancer survival time},
	volume = {13},
	copyright = {2023 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-28393-7},
	doi = {10.1038/s41598-023-28393-7},
	abstract = {The Cox proportional hazards model is commonly used in evaluating risk factors in cancer survival data. The model assumes an additive, linear relationship between the risk factors and the log hazard. However, this assumption may be too simplistic. Further, failure to take time-varying covariates into account, if present, may lower prediction accuracy. In this retrospective, population-based, prognostic study of data from patients diagnosed with cancer from 2008 to 2015 in Ontario, Canada, we applied machine learning-based time-to-event prediction methods and compared their predictive performance in two sets of analyses: (1) yearly-cohort-based time-invariant and (2) fully time-varying covariates analysis. Machine learning-based methods—gradient boosting model (gbm), random survival forest (rsf), elastic net (enet), lasso and ridge—were compared to the traditional Cox proportional hazards (coxph) model and the prior study which used the yearly-cohort-based time-invariant analysis. Using Harrell’s C index as our primary measure, we found that using both machine learning techniques and incorporating time-dependent covariates can improve predictive performance. Gradient boosting machine showed the best performance on test data in both time-invariant and time-varying covariates analysis.},
	language = {en},
	number = {1},
	urldate = {2023-02-24},
	journal = {Scientific Reports},
	author = {Cygu, Steve and Seow, Hsien and Dushoff, Jonathan and Bolker, Benjamin M.},
	month = jan,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Breast cancer, Cancer models, Computational science, Gastrointestinal cancer, Lung cancer},
	pages = {1370},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/NZDFT58B/Cygu et al. - 2023 - Comparing machine learning approaches to incorpora.pdf:application/pdf},
}



@article{milaNearest2022,
	title = {Nearest neighbour distance matching {Leave}-{One}-{Out} {Cross}-{Validation} for map validation},
	volume = {13},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13851},
	doi = {10.1111/2041-210X.13851},
	abstract = {Several spatial and non-spatial Cross-Validation (CV) methods have been used to perform map validation when additional sampling for validation purposes is not possible, yet it is unclear in which situations one CV method might be preferred over the other. Three factors have been identified as determinants of the performance of CV methods for map validation: the prediction area (geographical interpolation vs. extrapolation), the sampling pattern and the landscape spatial autocorrelation. In this study, we propose a new CV strategy that takes the geographical prediction space into account, and test how the new method compares with other established CV methods under different configurations of these three factors. We propose a variation of Leave-One-Out (LOO) CV for map validation, called Nearest Neighbour Distance Matching (NNDM) LOO CV, in which the nearest neighbour distance distribution function between the test and training data during the CV process is matched to the nearest neighbour distance distribution function between the target prediction and training points. Using random forest as a machine learning algorithm, we then examine the suitability of NNDM LOO CV as well as the established LOO (non-spatial) and buffered-LOO (bLOO, spatial) CV methods in two simulations with varying prediction areas, landscape autocorrelation and sampling distributions. LOO CV provided good map accuracy estimates in landscapes with short autocorrelation ranges, or when estimating geographical interpolation map accuracy with randomly distributed samples. bLOO CV yielded realistic error estimates when estimating map accuracy in new prediction areas, but generally overestimated geographical interpolation errors. NNDM LOO CV returned reliable estimates in all scenarios we considered. While LOO and bLOO CV provided reliable map accuracy estimates only in certain situations, our newly proposed NNDM LOO CV method returned robust estimates and generalised to LOO and bLOO CV whenever these methods were the most appropriate approach. Our work recognises the necessity of considering the geographical prediction space when designing CV-based methods for map validation.},
	language = {en},
	number = {6},
	urldate = {2023-02-24},
	journal = {Methods in Ecology and Evolution},
	author = {Milà, Carles and Mateu, Jorge and Pebesma, Edzer and Meyer, Hanna},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13851},
	keywords = {Cross-Validation, map accuracy estimation, map validation, spatial point patterns, spatial prediction},
	pages = {1304--1316},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/NQLA46XL/Milà et al. - 2022 - Nearest neighbour distance matching Leave-One-Out .pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/EHE9MIDV/2041-210X.html:text/html},
}


@misc{kuhnNested2017,
	title = {Nested Resampling with rsample},
	url = {http://appliedpredictivemodeling.com/blog/2017/9/2/njdc83d01pzysvvlgik02t5qnaljnd},
	language = {en-US},
	urldate = {2023-02-24},
	journal = {Applied Predictive Modeling},
	author = {Kuhn, Max},
	month = sep,
	year = {2017}
}


@article{wainerNested2021,
	title = {Nested cross-validation when selecting classifiers is overzealous for most practical applications},
	volume = {182},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421006540},
	doi = {10.1016/j.eswa.2021.115222},
	abstract = {When selecting a classification algorithm to be applied to a particular problem, one has to simultaneously select the best algorithm for that dataset and the best set of hyperparameters for the chosen model. The usual approach is to apply a nested cross-validation procedure: hyperparameter selection is performed in the inner cross-validation, while the outer cross-validation computes an unbiased estimate of the expected accuracy of the algorithm with cross-validation based hyperparameter tuning. The alternative approach, which we shall call “flat cross-validation”, uses a single cross-validation step both to select the optimal hyperparameter values and to provide an estimate of the expected accuracy of the algorithm that, while biased, may nevertheless still be used to select the best learning algorithm. We tested both procedures using 12 different algorithms on 115 real-life binary datasets and conclude that using the less computationally costly flat cross-validation procedure will generally result in the selection of an algorithm that is, for all practical purposes, of similar quality to that selected via nested cross-validation, provided the learning algorithms have relatively few hyperparameters to be optimised.},
	language = {en},
	urldate = {2023-02-24},
	journal = {Expert Systems with Applications},
	author = {Wainer, Jacques and Cawley, Gavin},
	month = nov,
	year = {2021},
	keywords = {Classification, cross-validation, Hyperparameters, Model selection, Nested cross-validation},
	pages = {115222},
	file = {ScienceDirect Full Text PDF:/home/bolker/Documents/zotero_new/storage/R523ASSB/Wainer and Cawley - 2021 - Nested cross-validation when selecting classifiers.pdf:application/pdf},
}



@inproceedings{guoCalibration2017a,
	title = {On {Calibration} of {Modern} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v70/guo17a.html},
	abstract = {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
	language = {en},
	urldate = {2023-02-24},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1321--1330},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/4VNITLZ2/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf:application/pdf;Supplementary PDF:/home/bolker/Documents/zotero_new/storage/G57IRQN7/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf:application/pdf},
}

@inproceedings{mindererRevisiting2021,
	title = {Revisiting the {Calibration} of {Modern} {Neural} {Networks}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/8420d359404024567b5aefda1231af24-Abstract.html},
	abstract = {Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.},
	urldate = {2023-02-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Minderer, Matthias and Djolonga, Josip and Romijnders, Rob and Hubis, Frances and Zhai, Xiaohua and Houlsby, Neil and Tran, Dustin and Lucic, Mario},
	year = {2021},
	pages = {15682--15694},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/QVDDTAU2/Minderer et al. - 2021 - Revisiting the Calibration of Modern Neural Networ.pdf:application/pdf},
}



@misc{taquetMAPIE2021,
	title = {With {MAPIE}, uncertainties are back in machine learning !},
	url = {https://towardsdatascience.com/with-mapie-uncertainties-are-back-in-machine-learning-882d5c17fdc3},
	abstract = {Presenting MAPIE, a scikit-learn-compatible package that allows you to easily estimate uncertainties associated with your favourite ML…},
	language = {en},
	urldate = {2023-02-24},
	journal = {Medium},
	author = {Taquet, Vianney},
	month = sep,
	year = {2021},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/HPC32RTJ/with-mapie-uncertainties-are-back-in-machine-learning-882d5c17fdc3.html:text/html},
}

@article{breimanTreeStructured1988,
  title = {Tree-{{Structured Classification Via Generalized Discriminant Analysis}}: {{Comment}}},
  shorttitle = {Tree-{{Structured Classification Via Generalized Discriminant Analysis}}},
  author = {Breiman, Leo and Friedman, Jerome H.},
  year = {1988},
  journal = {Journal of the American Statistical Association},
  volume = {83},
  number = {403},
  eprint = {2289296},
  eprinttype = {jstor},
  pages = {725--727},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2289296},
  file = {/home/bolker/Zotero/storage/36MKBX5R/Breiman and Friedman - 1988 - Tree-Structured Classification Via Generalized Dis.pdf}
}

@article{lohTreeStructured1988,
  title = {Tree-{{Structured Classification}} via {{Generalized Discriminant Analysis}}},
  author = {Loh, Wei-Yin and Vanichsetakul, Nunta},
  year = {1988},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {83},
  number = {403},
  pages = {715--725},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1988.10478652},
  abstract = {The problem of constructing classification rules that can be represented as decision trees is considered. Each object to be classified has an associated x vector containing possibly incomplete covariate information. Tree construction is based on the information provided in a ``learning sample'' of objects with known class identities. The x vectors in the learning sample may have missing values as well. Procedures are proposed for each of the components of classifier construction, such as split selection, tree-size determination, treatment of missing values, and ranking of variables. The main idea is recursive application of linear discriminant analysis, with the variables at each stage being appropriately chosen according to the data and the type of splits desired. Standard statistical techniques used as basic building blocks include analysis of variance, linear and canonical discriminant analysis, and principal component analysis. A new method of tree-structured classification is obtained by assembling the pieces. This method can accommodate prior probabilities as well as unequal misclassification costs and can yield trees with univariate, linear combination, or linear combination with polar coordinate splits. The method is compared with the CART method of Breiman, Friedman, Olshen, and Stone (1984). Some of the operational differences are that the new method (a) can have multiple splits per node, (b) is nonrandomized, (c) uses a direct stopping rule, (d) handles missing values by estimation, (e) allows both ordered and unordered variables in the same linear combination split, (f) is not invariant of monotone transformations of the individual variables, and (g) is computationally faster. Simulation experiments suggest that the two methods have comparable classification accuracy. The Boston housing data are analyzed in a classification context for illustration.},
  keywords = {Cross-validation,Decision tree,Discriminant analysis,Machine learning,Misclassification,Missing values,Principal components,Recursive partitioning}
}


@article{friedmanSparse2008,
	title = {Sparse inverse covariance estimation with the graphical lasso},
	volume = {9},
	issn = {1465-4644},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3019769/},
	doi = {10.1093/biostatistics/kxm045},
	abstract = {We consider the problem of estimating sparse graphs by a lasso penalty applied to the
inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop
a simple algorithm—the graphical lasso—that is remarkably
fast: It solves a 1000-node problem (∼500000 parameters) in at most a minute and is
30–4000 times faster than competing methods. It also provides a conceptual link
between the exact problem and the approximation suggested by Meinshausen and Bühlmann
(2006). We illustrate the method on some cell-signaling data from proteomics.},
	number = {3},
	urldate = {2023-03-12},
	journal = {Biostatistics (Oxford, England)},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = jul,
	year = {2008},
	pmid = {18079126},
	pmcid = {PMC3019769},
	pages = {432--441},
	file = {PubMed Central Full Text PDF:/home/bolker/Documents/zotero_new/storage/2GFIKRBD/Friedman et al. - 2008 - Sparse inverse covariance estimation with the grap.pdf:application/pdf},
}



@misc{jurekScalable2022,
	title = {Scalable {Spatio}-{Temporal} {Smoothing} via {Hierarchical} {Sparse} {Cholesky} {Decomposition}},
	url = {http://arxiv.org/abs/2207.09384},
	doi = {10.48550/arXiv.2207.09384},
	abstract = {We propose an approximation to the forward-filter-backward-sampler (FFBS) algorithm for large-scale spatio-temporal smoothing. FFBS is commonly used in Bayesian statistics when working with linear Gaussian state-space models, but it requires inverting covariance matrices which have the size of the latent state vector. The computational burden associated with this operation effectively prohibits its applications in high-dimensional settings. We propose a scalable spatio-temporal FFBS approach based on the hierarchical Vecchia approximation of Gaussian processes, which has been previously successfully used in spatial statistics. On simulated and real data, our approach outperformed a low-rank FFBS approximation.},
	urldate = {2022-07-21},
	publisher = {arXiv},
	author = {Jurek, Marcin and Katzfuss, Matthias},
	month = jul,
	year = {2022},
	note = {arXiv:2207.09384 [stat]},
	keywords = {Statistics - Methodology, Statistics - Computation},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/UEFZBUDC/Jurek and Katzfuss - 2022 - Scalable Spatio-Temporal Smoothing via Hierarchica.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/P4S4LBYP/2207.html:text/html},
}

@misc{krasserSparse,
	title = {Sparse {Gaussian} processes},
	url = {http://krasserm.github.io/2020/12/12/gaussian-processes-sparse/},
	urldate = {2022-07-28},
	author = {Krasser, Martin},
	file = {Sparse Gaussian processes - Martin Krasser's Blog:/home/bolker/Documents/zotero_new/storage/FWMM8XJD/gaussian-processes-sparse.html:text/html},
}

@article{rueApproximate2009,
	title = {Approximate {Bayesian} inference for latent {Gaussian} models by using integrated nested {Laplace} approximations},
	volume = {71},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2008.00700.x},
	doi = {10.1111/j.1467-9868.2008.00700.x},
	abstract = {Summary. Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
	language = {en},
	number = {2},
	urldate = {2022-09-15},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Rue, Håvard and Martino, Sara and Chopin, Nicolas},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2008.00700.x},
	keywords = {Parallel computing, Generalized additive mixed models, Laplace approximation, Gaussian Markov random fields, Approximate Bayesian inference, Sparse matrices, Structured additive regression models},
	pages = {319--392},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/5IZUZEC3/Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/62ZU8SHQ/j.1467-9868.2008.00700.html:text/html},
}

@article{jakkalaDeep2021,
	title = {Deep {Gaussian} {Processes}: {A} {Survey}},
	shorttitle = {Deep {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2106.12135},
	abstract = {Gaussian processes are one of the dominant approaches in Bayesian learning. Although the approach has been applied to numerous problems with great success, it has a few fundamental limitations. Multiple methods in literature have addressed these limitations. However, there has not been a comprehensive survey of the topics as of yet. Most existing surveys focus on only one particular variant of Gaussian processes and their derivatives. This survey details the core motivations for using Gaussian processes, their mathematical formulations, limitations, and research themes that have flourished over the years to address said limitations. Furthermore, one particular research area is Deep Gaussian Processes (DGPs), it has improved substantially in the past decade. The significant publications that advanced the forefront of this research area are outlined in their survey. Finally, a brief discussion on open problems and research directions for future work is presented at the end.},
	urldate = {2021-10-26},
	journal = {arXiv:2106.12135 [cs, stat]},
	author = {Jakkala, Kalvik},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.12135},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 23 pages, 5 figures},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/GNMCCDYN/Jakkala - 2021 - Deep Gaussian Processes A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/J7UG6KP7/2106.html:text/html},
}

@article{agrawalKernel2019,
	title = {The {Kernel} {Interaction} {Trick}: {Fast} {Bayesian} {Discovery} of {Pairwise} {Interactions} in {High} {Dimensions}},
	shorttitle = {The {Kernel} {Interaction} {Trick}},
	url = {http://arxiv.org/abs/1905.06501},
	abstract = {Discovering interaction effects on a response of interest is a fundamental problem faced in biology, medicine, economics, and many other scientific disciplines. In theory, Bayesian methods for discovering pairwise interactions enjoy many benefits such as coherent uncertainty quantification, the ability to incorporate background knowledge, and desirable shrinkage properties. In practice, however, Bayesian methods are often computationally intractable for even moderate-dimensional problems. Our key insight is that many hierarchical models of practical interest admit a particular Gaussian process (GP) representation; the GP allows us to capture the posterior with a vector of O(p) kernel hyper-parameters rather than O(p{\textasciicircum}2) interactions and main effects. With the implicit representation, we can run Markov chain Monte Carlo (MCMC) over model hyper-parameters in time and memory linear in p per iteration. We focus on sparsity-inducing models and show on datasets with a variety of covariate behaviors that our method: (1) reduces runtime by orders of magnitude over naive applications of MCMC, (2) provides lower Type I and Type II error relative to state-of-the-art LASSO-based approaches, and (3) offers improved computational scaling in high dimensions relative to existing Bayesian and LASSO-based approaches.},
	urldate = {2021-05-19},
	journal = {arXiv:1905.06501 [cs, stat]},
	author = {Agrawal, Raj and Huggins, Jonathan H. and Trippe, Brian and Broderick, Tamara},
	month = dec,
	year = {2019},
	note = {arXiv: 1905.06501},
	keywords = {Statistics - Methodology, Statistics - Computation, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICML 2019. 20 pages, 4 figures, 3 tables},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/ZJU2FVT6/Agrawal et al. - 2019 - The Kernel Interaction Trick Fast Bayesian Discov.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/99R4G9A7/1905.html:text/html},
}

@article{sansoBayesian2008,
	title = {Bayesian {Spatio}-{Temporal} {Models} {Based} on {Discrete} {Convolutions}},
	volume = {36},
	copyright = {Copyright © 2008 Statistical Society of Canada},
	issn = {0319-5724},
	url = {http://www.jstor.org/stable/20445307},
	abstract = {The authors consider a class of models for spatio-temporal processes based on convolving independent processes with a discrete kernel that is represented by a lower triangular matrix. They study two families of models. In the first one, spatial Gaussian processes with isotropic correlations are convoluted with a kernel that provides temporal dependencies. In the second family, AR(p) processes are convoluted with a kernel providing spatial interactions. The covariance structures associated with these two families are quite rich. Their covariance functions that are stationary and separable in space and time as well as time dependent nonseparable and nonisotropic ones. /// Les auteurs s'intéressent à une classe de modèles pour les processus spatio-temporels basés sur la convolution de processus indépendants avec un noyau discret représenté par une matrice triangulaire inférieure. Ils étudient deux familles de modèles. Dans la première, des processus spatiaux gaussiens à corrélations isotropes sont convolués avec un noyau induisant des dépendances temporelles. Dans la seconde, des processus AR(p) sont convolués avec un noyau induisant des interactions spatiales. Les structures de covariance associées à ces deux familles sont très riches. Leurs fonctions de covariance peuvent être stationnaires et séparables dans l'espace et dans le temps ou encore dépendantes du temps, non séparables et non isotropes.},
	number = {2},
	urldate = {2012-09-26},
	journal = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
	author = {Sansó, Bruno and Schmidt, Alexandra M. and Nobre, Aline A.},
	month = jun,
	year = {2008},
	note = {ArticleType: research-article / Full publication date: Jun., 2008 / Copyright © 2008 Statistical Society of Canada},
	pages = {239--258},
}

@article{bodinLatent2017,
	title = {Latent {Gaussian} {Process} {Regression}},
	url = {http://arxiv.org/abs/1707.05534},
	abstract = {We introduce Latent Gaussian Process Regression which is a latent variable extension allowing modelling of non-stationary processes using stationary GP priors. The approach is built on extending the input space of a regression problem with a latent variable that is used to modulate the covariance function over the input space. We show how our approach can be used to model non-stationary processes but also how multi-modal or non-functional processes can be described where the input signal cannot fully disambiguate the output. We exemplify the approach on a set of synthetic data and provide results on real data from geostatistics.},
	urldate = {2017-07-19},
	journal = {arXiv:1707.05534 [cs, stat]},
	author = {Bodin, Erik and Campbell, Neill D. F. and Ek, Carl Henrik},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.05534},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1707.05534 PDF:/home/bolker/Documents/zotero_new/storage/Z7V7NZBE/Bodin et al. - 2017 - Latent Gaussian Process Regression.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/K8ZH4ES6/1707.html:text/html},
}

@article{garrido-merchanDealing2017,
	title = {Dealing with {Integer}-valued {Variables} in {Bayesian} {Optimization} with {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1706.03673},
	abstract = {Bayesian optimization (BO) methods are useful for optimizing functions that are expensive to evaluate, lack an analytical expression and whose evaluations can be contaminated by noise. These methods rely on a probabilistic model of the objective function, typically a Gaussian process (GP), upon which an acquisition function is built. This function guides the optimization process and measures the expected utility of performing an evaluation of the objective at a new point. GPs assume continous input variables. When this is not the case, such as when some of the input variables take integer values, one has to introduce extra approximations. A common approach is to round the suggested variable value to the closest integer before doing the evaluation of the objective. We show that this can lead to problems in the optimization process and describe a more principled approach to account for input variables that are integer-valued. We illustrate in both synthetic and a real experiments the utility of our approach, which significantly improves the results of standard BO methods on problems involving integer-valued variables.},
	urldate = {2018-03-16},
	journal = {arXiv:1706.03673 [stat]},
	author = {Garrido-Merchán, Eduardo C. and Hernández-Lobato, Daniel},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03673},
	keywords = {Statistics - Machine Learning},
	annote = {Comment: 7 pages},
	file = {arXiv\:1706.03673 PDF:/home/bolker/Documents/zotero_new/storage/4XR92JL5/Garrido-Merchán and Hernández-Lobato - 2017 - Dealing with Integer-valued Variables in Bayesian .pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/IDHXHY4D/1706.html:text/html},
}

@article{poynorCombining2017a,
	title = {Combining functional data with hierarchical {Gaussian} process models},
	volume = {24},
	issn = {1573-3009},
	url = {https://doi.org/10.1007/s10651-017-0366-2},
	doi = {10.1007/s10651-017-0366-2},
	abstract = {Gaussian process models have been used in applications ranging from machine learning to fisheries management. In the Bayesian framework, the Gaussian process is used as a prior for unknown functions, allowing the data to drive the relationship between inputs and outputs. In our research, we consider a scenario in which response and input data are available from several similar, but not necessarily identical, sources. When little information is known about one or more of the populations it may be advantageous to model all populations together. We present a hierarchical Gaussian process model with a structure that allows distinct features for each source as well as shared underlying characteristics. Key features and properties of the model are discussed and demonstrated in a number of simulation examples. The model is then applied to a data set consisting of three populations of Rotifer Brachionus calyciflorus Pallas. Specifically, we model the log growth rate of the populations using a combination of lagged population sizes. The various lag combinations are formally compared to obtain the best model inputs. We then formally compare the leading hierarchical Gaussian process model with the inferential results obtained under the independent Gaussian process model.},
	language = {en},
	number = {2},
	urldate = {2019-10-20},
	journal = {Environmental and Ecological Statistics},
	author = {Poynor, Valerie and Munch, Stephan},
	month = jun,
	year = {2017},
	keywords = {Bayesian nonparametric functional analysis, Dependent populations, Hierarchical Gaussian process model},
	pages = {175--199},
	file = {Springer Full Text PDF:/home/bolker/Documents/zotero_new/storage/67MKRM8S/Poynor and Munch - 2017 - Combining functional data with hierarchical Gaussi.pdf:application/pdf},
}

@article{gortlerVisual2019,
	title = {A {Visual} {Exploration} of {Gaussian} {Processes}},
	volume = {4},
	issn = {2476-0757},
	url = {https://distill.pub/2019/visual-exploration-gaussian-processes},
	doi = {10.23915/distill.00017},
	abstract = {How to turn a collection of small building blocks into a versatile tool for solving regression problems.},
	language = {en},
	number = {4},
	urldate = {2020-07-18},
	journal = {Distill},
	author = {Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
	month = apr,
	year = {2019},
	pages = {e17},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/ISDCZQXY/visual-exploration-gaussian-processes.html:text/html},
}

@article{buckingham-jefferyGaussian2018,
	title = {Gaussian process approximations for fast inference from infectious disease data},
	volume = {301},
	issn = {0025-5564},
	url = {https://www.sciencedirect.com/science/article/pii/S0025556417303644},
	doi = {10.1016/j.mbs.2018.02.003},
	abstract = {We present a flexible framework for deriving and quantifying the accuracy of Gaussian process approximations to non-linear stochastic individual-based models of epidemics. We develop this for the SIR and SEIR models, and we show how it can be used to perform quick maximum likelihood inference for the underlying parameters given population estimates of the number of infecteds or cases at given time points. We also show how the unobserved processes can be inferred at the same time as the underlying parameters.},
	language = {en},
	urldate = {2021-03-16},
	journal = {Mathematical Biosciences},
	author = {Buckingham-Jeffery, Elizabeth and Isham, Valerie and House, Thomas},
	month = jul,
	year = {2018},
	keywords = {MLE, SEIR, SIR, Stochastic Taylor expansion},
	pages = {111--120},
	file = {ScienceDirect Full Text PDF:/home/bolker/Documents/zotero_new/storage/KBE77GSN/Buckingham-Jeffery et al. - 2018 - Gaussian process approximations for fast inference.pdf:application/pdf;ScienceDirect Snapshot:/home/bolker/Documents/zotero_new/storage/K78VY9UT/S0025556417303644.html:text/html},
}

@misc{vehtariGaussian2021,
	title = {Gaussian process demonstration with {Stan}},
	url = {https://avehtari.github.io/casestudies/Motorcycle/motorcycle_gpcourse.html},
	urldate = {2021-02-10},
	author = {Vehtari, Aki},
	year = {2021},
	file = {Gaussian process demonstration with Stan:/home/bolker/Documents/zotero_new/storage/MSJ3NWXW/motorcycle_gpcourse.html:text/html},
}

@inproceedings{riihimakiGaussian2010,
	title = {Gaussian processes with monotonicity information},
	url = {https://proceedings.mlr.press/v9/riihimaki10a.html},
	abstract = {A method for using monotonicity information in multivariate Gaussian process regression and classification is proposed. Monotonicity information is introduced with virtual derivative observations, and the resulting posterior is approximated with expectation propagation. Behaviour of the method is illustrated with artificial regression examples, and the method is used in a real world health care classification problem to include monotonicity information with respect to one of the covariates.},
	language = {en},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Riihimäki, Jaakko and Vehtari, Aki},
	month = mar,
	year = {2010},
	note = {ISSN: 1938-7228},
	pages = {645--652},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/EJ866BWV/Riihimäki and Vehtari - 2010 - Gaussian processes with monotonicity information.pdf:application/pdf},
}

@misc{wengerPreconditioning2022,
	title = {Preconditioning for {Scalable} {Gaussian} {Process} {Hyperparameter} {Optimization}},
	url = {http://arxiv.org/abs/2107.00243},
	doi = {10.48550/arXiv.2107.00243},
	abstract = {Gaussian process hyperparameter optimization requires linear solves with, and log-determinants of, large kernel matrices. Iterative numerical techniques are becoming popular to scale to larger datasets, relying on the conjugate gradient method (CG) for the linear solves and stochastic trace estimation for the log-determinant. This work introduces new algorithmic and theoretical insights for preconditioning these computations. While preconditioning is well understood in the context of CG, we demonstrate that it can also accelerate convergence and reduce variance of the estimates for the log-determinant and its derivative. We prove general probabilistic error bounds for the preconditioned computation of the log-determinant, log-marginal likelihood and its derivatives. Additionally, we derive specific rates for a range of kernel-preconditioner combinations, showing that up to exponential convergence can be achieved. Our theoretical results enable provably efficient optimization of kernel hyperparameters, which we validate empirically on large-scale benchmark problems. There our approach accelerates training by up to an order of magnitude.},
	urldate = {2022-07-19},
	publisher = {arXiv},
	author = {Wenger, Jonathan and Pleiss, Geoff and Hennig, Philipp and Cunningham, John P. and Gardner, Jacob R.},
	month = jun,
	year = {2022},
	note = {arXiv:2107.00243 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	annote = {Comment: International Conference on Machine Learning (ICML)},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/YF3A7ET9/Wenger et al. - 2022 - Preconditioning for Scalable Gaussian Process Hype.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/H7CK5FHX/2107.html:text/html},
}

@article{deoliveiraInformation2022,
	title = {On {Information} {About} {Covariance} {Parameters} in {Gaussian} {Matérn} {Random} {Fields}},
	issn = {1537-2693},
	url = {https://doi.org/10.1007/s13253-022-00510-5},
	doi = {10.1007/s13253-022-00510-5},
	abstract = {The Matérn family of covariance functions is currently the most commonly used for the analysis of geostatistical data due to its ability to describe different smoothness behaviors. Yet, in many applications, the smoothness parameter is set at an arbitrary value. This practice is due partly to computational challenges faced when attempting to estimate all covariance parameters and partly to unqualified claims in the literature stating that geostatistical data have little or no information about the smoothness parameter. This work critically investigates this claim and shows it is not true in general. Specifically, it is shown that the information the data have about the correlation parameters varies substantially depending on the true model and sampling design and, in particular, the information about the smoothness parameter can be large, in some cases larger than the information about the range parameter. In light of these findings, we suggest to reassess the aforementioned practice and instead establish inferences from data-based estimates of both range and smoothness parameters, especially for strongly dependent non-smooth processes observed on irregular sampling designs. A data set of daily rainfall totals is used to motivate the discussion and gauge this common practice.},
	language = {en},
	urldate = {2022-09-02},
	journal = {Journal of Agricultural, Biological and Environmental Statistics},
	author = {De Oliveira, Victor and Han, Zifei},
	month = aug,
	year = {2022},
	keywords = {Fisher information, Geostatistics, Sampling design, Microergodic parameter, Smoothness parameter},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/HBIBFDSS/De Oliveira and Han - 2022 - On Information About Covariance Parameters in Gaus.pdf:application/pdf},
}

@book{rasmussenGaussian2005,
	address = {Cambridge, Mass},
	title = {Gaussian {Processes} for {Machine} {Learning}},
	isbn = {978-0-262-18253-9},
	language = {English},
	publisher = {The MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	month = nov,
	year = {2005},
}

@article{rueApproximate2009,
	title = {Approximate {Bayesian} inference for latent {Gaussian} models by using integrated nested {Laplace} approximations},
	volume = {71},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2008.00700.x},
	doi = {10.1111/j.1467-9868.2008.00700.x},
	abstract = {Summary. Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
	language = {en},
	number = {2},
	urldate = {2022-09-15},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Rue, Håvard and Martino, Sara and Chopin, Nicolas},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2008.00700.x},
	keywords = {Parallel computing, Generalized additive mixed models, Laplace approximation, Gaussian Markov random fields, Approximate Bayesian inference, Sparse matrices, Structured additive regression models},
	pages = {319--392},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/5IZUZEC3/Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/62ZU8SHQ/j.1467-9868.2008.00700.html:text/html},
}

@article{hensmanScalable,
	title = {Scalable {Variational} {Gaussian} {Process} {Classiﬁcation}},
	abstract = {Gaussian process classiﬁcation is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, outperforming the state of the art on benchmark datasets. Importantly, the variational formulation can be exploited to allow classiﬁcation in problems with millions of data points, as we demonstrate in experiments.},
	language = {en},
	author = {Hensman, James and Ghahramani, Zoubin},
	pages = {10},
	file = {Hensman and Ghahramani - Scalable Variational Gaussian Process Classiﬁcatio.pdf:/home/bolker/Documents/zotero_new/storage/TNAQYDSY/Hensman and Ghahramani - Scalable Variational Gaussian Process Classiﬁcatio.pdf:application/pdf},
}

@article{burdenSAR2015,
	title = {The {SAR} {Model} for {Very} {Large} {Datasets}: {A} {Reduced} {Rank} {Approach}},
	volume = {3},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2225-1146},
	shorttitle = {The {SAR} {Model} for {Very} {Large} {Datasets}},
	url = {https://www.mdpi.com/2225-1146/3/2/317},
	doi = {10.3390/econometrics3020317},
	abstract = {The SAR model is widely used in spatial econometrics to model Gaussian processes on a discrete spatial lattice, but for large datasets, fitting it becomes computationally prohibitive, and hence, its usefulness can be limited. A computationally-efficient spatial model is the spatial random effects (SRE) model, and in this article, we calibrate it to the SAR model of interest using a generalisation of the Moran operator that allows for heteroskedasticity and an asymmetric SAR spatial dependence matrix. In general, spatial data have a measurement-error component, which we model, and we use restricted maximum likelihood to estimate the SRE model covariance parameters; its required computational time is only the order of the size of the dataset. Our implementation is demonstrated using mean usual weekly income data from the 2011 Australian Census.},
	language = {en},
	number = {2},
	urldate = {2022-12-13},
	journal = {Econometrics},
	author = {Burden, Sandy and Cressie, Noel and Steel, David G.},
	month = jun,
	year = {2015},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {asymmetric spatial dependence matrix, Australian census, heteroskedasticity, Moran operator, spatial autoregressive model, spatial basis functions, spatial random effects model},
	pages = {317--338},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/6J47JBY9/Burden et al. - 2015 - The SAR Model for Very Large Datasets A Reduced R.pdf:application/pdf},
}


@book{pressNumerical2007,
	edition = {3},
	title = {Numerical {Recipes} 3rd {Edition}: {The} {Art} of {Scientific} {Computing}},
	isbn = {0-521-88068-8},
	shorttitle = {Numerical {Recipes} 3rd {Edition}},
	publisher = {Cambridge University Press},
	author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
	month = sep,
	year = {2007},
}


@incollection{precheltEarly2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Early {Stopping} - But when?},
	url = {http://page.mi.fu-berlin.de/~prechelt/Biblio/stop_tricks1997.pdf},
	abstract = {Validation can be used to detect when over tting starts during supervised training of a neural network; training is then stopped before convergence to avoid the over tting  early stopping". The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeo between training time and generalization: From the given mix of 1296 training runs using di erent 12 problems and 24 di erent network architectures I conclude slower stopping criteria allow for small improvements in generalization here: about 4 on average, but cost much more training time here: about factor 4 longer on average.},
	language = {en},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	author = {Prechelt, Lutz},
	editor = {Montavon, Grégoire and Müller, Klaus-Robert},
	year = {2012},
	pages = {53--67},
	file = {Prechelt - Early Stopping  but when.pdf:/home/bolker/Documents/zotero_new/storage/NX8U9VME/Prechelt - Early Stopping  but when.pdf:application/pdf},
}


@misc{bujokasGradient2022,
  title = {Gradient {{Boosting}} in {{Python}} from {{Scratch}}},
  author = {Bujokas, Eligijus},
  year = {2022},
  month = mar,
  journal = {Medium},
  urldate = {2023-03-14},
  abstract = {Coding and explaining in depth the very popular and competition-winning gradient boosting algorithm using Python},
  url = {https://towardsdatascience.com/gradient-boosting-in-python-from-scratch-788d1cf1ca7},
  langid = {english},
  file = {/home/bolker/Zotero/storage/7EQSLK6D/gradient-boosting-in-python-from-scratch-788d1cf1ca7.html}
}


@misc{choDoes2018,
  title = {Does {{Xgboost}} Do {{Newton}} Boosting?},
  shorttitle = {Does {{Xgboost}} Do {{Newton}} Boosting?},
  author = {Cho, Philip Hyunshu},
  year = {2018},
  journal = {GitHub},
  urldate = {2023-03-15},
  abstract = {In this paper below\&quot;\&quot;, it mentions \&quot;Newton boosting\&quot; as the boosting algorithm used by XGBoost on page 42, which need to calculate hessian matrix. \&quot;Tree Boosting With XGBoo...},
  url = {https://github.com/dmlc/xgboost/issues/3227},
  langid = {english}
}

@misc{sigristGradient2018,
  title = {Gradient and {{Newton Boosting}} for {{Classification}} and {{Regression}}},
  author = {Sigrist, Fabio},
  year = {2018},
  month = aug,
  journal = {arXiv.org},
  doi = {10.48550/arXiv.1808.03064},
  urldate = {2023-03-15},
  abstract = {Boosting algorithms are frequently used in applied data science and in research. To date, the distinction between boosting with either gradient descent or second-order Newton updates is often not made in both applied and methodological research, and it is thus implicitly assumed that the difference is irrelevant. The goal of this article is to clarify this situation. In particular, we present gradient and Newton boosting, as well as a hybrid variant of the two, in a unified framework. We compare these boosting algorithms with trees as base learners using various datasets and loss functions. Our experiments show that Newton boosting outperforms gradient and hybrid gradient-Newton boosting in terms of predictive accuracy on the majority of datasets. We also present evidence that the reason for this is not faster convergence of Newton boosting. In addition, we introduce a novel tuning parameter for tree-based Newton boosting which is interpretable and important for predictive accuracy.},
  url = {https://arxiv.org/abs/1808.03064v7},
  langid = {english},
  file = {/home/bolker/Zotero/storage/UJN7YU5V/Sigrist - 2018 - Gradient and Newton Boosting for Classification an.pdf}
}


@misc{abbottFar2022,
  title = {Far from {{Asymptopia}}},
  author = {Abbott, Michael C. and Machta, Benjamin B.},
  year = {2022},
  month = may,
  number = {arXiv:2205.03343},
  eprint = {arXiv:2205.03343},
  publisher = {{arXiv}},
  urldate = {2023-03-18},
  abstract = {Inference from limited data requires a notion of measure on parameter space, which is most explicit in the Bayesian framework as a prior distribution. Jeffreys prior is the best-known uninformative choice, the invariant volume element from information geometry, but we demonstrate here that this leads to enormous bias in typical high-dimensional models. This is because models found in science typically have an effective dimensionality of accessible behaviours much smaller than the number of microscopic parameters. Any measure which treats all of these parameters equally is far from uniform when projected onto the sub-space of relevant parameters, due to variations in the local co-volume of irrelevant directions. We present results on a principled choice of measure which avoids this issue, and leads to unbiased posteriors, by focusing on relevant parameters. This optimal prior depends on the quantity of data to be gathered, and approaches Jeffreys prior in the asymptotic limit. But for typical models this limit cannot be justified without an impossibly large increase in the quantity of data, exponential in the number of microscopic parameters.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning,Statistics - Other Statistics},
  file = {/home/bolker/Zotero/storage/LGNW4FJP/Abbott and Machta - 2022 - Far from Asymptopia.pdf}
}

@book{burzykowskiIntroduction,
  title = {5 {{Introduction}} to {{Instance-level Exploration}} | {{Explanatory Model Analysis}}},
  author = {Burzykowski, Przemyslaw Biecek {and} Tomasz},
  urldate = {2023-03-18},
  abstract = {This book introduces unified language for exploration, explanation and examination of predictive machine learning models.},
  file = {/home/bolker/Zotero/storage/JZWXWRAP/InstanceLevelExploration.html}
}

@inproceedings{chenXGBoost2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  eprint = {1603.02754},
  primaryclass = {cs},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  urldate = {2023-03-18},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/bolker/Zotero/storage/645XIXBJ/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf;/home/bolker/Zotero/storage/AS99337G/1603.html}
}

@inproceedings{kumarProblems2020,
  title = {Problems with {{Shapley-value-based}} Explanations as Feature Importance Measures},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Kumar, I. Elizabeth and Venkatasubramanian, Suresh and Scheidegger, Carlos and Friedler, Sorelle},
  year = {2020},
  month = nov,
  pages = {5491--5500},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-03-19},
  abstract = {Game-theoretic formulations of feature importance have become popular as a way to "explain" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game's unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values do not provide explanations which suit human-centric goals of explainability.},
  langid = {english},
  file = {/home/bolker/Zotero/storage/S3B3KBID/Kumar et al. - 2020 - Problems with Shapley-value-based explanations as .pdf}
}

@book{silge18,
  title = {18 {{Explaining Models}} and {{Predictions}} | {{Tidy Modeling}} with {{R}}},
  author = {Silge, Max Kuhn {and} Julia},
  urldate = {2023-03-19},
  abstract = {The tidymodels framework is a collection of R packages for modeling and machine learning using tidyverse principles. This book provides a thorough introduction to how to use tidymodels, and an outline of good methodology and statistical practice for phases of the modeling process.},
  file = {/home/bolker/Zotero/storage/X9685UFJ/explain.html}
}

@book{burzykowskiExplanatory2020,
  title = {Explanatory Model Analysis},
  author = {Burzykowski, Przemyslaw Biecek {and} Tomasz},
  date = 2020,
  urldate = {2023-03-19},
  abstract = {This book introduces unified language for exploration, explanation and examination of predictive machine learning models.},
  file = {/home/bolker/Zotero/storage/AJ85RIWM/ema.drwhy.ai.html}
}
