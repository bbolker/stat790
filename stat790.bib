@article{batesCrossvalidation,
  title = {Cross-Validation: What Does It Estimate and How Well Does It Do It?},
  author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
  pages = {36},
  abstract = {Cross-validation is a widely-used technique to estimate prediction error, but its behavior is complex and not fully understood. Ideally, one would like to think that cross-validation estimates the prediction error for the model at hand, fit to the training data. We prove that this is not the case for the linear model fit by ordinary least squares; rather it estimates the average prediction error of models fit on other unseen training sets drawn from the same population. We further show that this phenomenon occurs for most popular estimates of prediction error, including data splitting, bootstrapping, and Mallow’s Cp. Next, the standard confidence intervals for prediction error derived from cross-validation may have coverage far below the desired level. Because each data point is used for both training and testing, there are correlations among the measured accuracies for each fold, and so the usual estimate of variance is too small. We introduce a nested cross-validation scheme to estimate this variance more accurately, and show empirically that this modification leads to intervals with approximately correct coverage in many examples where traditional cross-validation intervals fail. Lastly, our analysis also shows that when producing confidence intervals for prediction accuracy with simple data splitting, one should not re-fit the model on the combined data, since this invalidates the confidence intervals.},
  langid = {english},
  file = {/home/bolker/Documents/zotero_new/storage/XRCEZVAJ/Bates et al. - Cross-validation what does it estimate and how we.pdf}
}

@article{elithworking2008,
  title = {A Working Guide to Boosted Regression Trees},
  author = {Elith, J. and Leathwick, J. R. and Hastie, T.},
  year = {2008},
  journal = {Journal of Animal Ecology},
  volume = {77},
  number = {4},
  pages = {802--813},
  issn = {1365-2656},
  doi = {10.1111/j.1365-2656.2008.01390.x},
  abstract = {1 Ecologists use statistical models for both explanation and prediction, and need techniques that are flexible enough to express typical features of their data, such as nonlinearities and interactions. 2 This study provides a working guide to boosted regression trees (BRT), an ensemble method for fitting statistical models that differs fundamentally from conventional techniques that aim to fit a single parsimonious model. Boosted regression trees combine the strengths of two algorithms: regression trees (models that relate a response to their predictors by recursive binary splits) and boosting (an adaptive method for combining many simple models to give improved predictive performance). The final BRT model can be understood as an additive regression model in which individual terms are simple trees, fitted in a forward, stagewise fashion. 3 Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data. They have no need for prior data transformation or elimination of outliers, can fit complex nonlinear relationships, and automatically handle interaction effects between predictors. Fitting multiple trees in BRT overcomes the biggest drawback of single tree models: their relatively poor predictive performance. Although BRT models are complex, they can be summarized in ways that give powerful ecological insight, and their predictive performance is superior to most traditional modelling methods. 4 The unique features of BRT raise a number of practical issues in model fitting. We demonstrate the practicalities and advantages of using BRT through a distributional analysis of the short-finned eel (Anguilla australis Richardson), a native freshwater fish of New Zealand. We use a data set of over 13 000 sites to illustrate effects of several settings, and then fit and interpret a model using a subset of the data. We provide code and a tutorial to enable the wider use of BRT by ecologists.},
  langid = {english},
  keywords = {data mining,machine learning,model averaging,random forests,species distributions},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2656.2008.01390.x},
  note = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2656.2008.01390.x},
  file = {/home/bolker/Documents/zotero_new/storage/AJQP9HL5/Elith et al. - 2008 - A working guide to boosted regression trees.pdf;/home/bolker/Documents/zotero_new/storage/83EYN5SA/j.1365-2656.2008.01390.html}
}

@article{friedmanPathwise2007,
  title = {Pathwise Coordinate Optimization},
  author = {Friedman, Jerome and Hastie, Trevor and Höfling, Holger and Tibshirani, Robert},
  year = {2007},
  month = dec,
  journal = {Annals of Applied Statistics},
  volume = {1},
  number = {2},
  pages = {302--332},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/07-AOAS131},
  abstract = {We consider “one-at-a-time” coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the L1-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the “fused lasso,” however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems.},
  langid = {english},
  mrnumber = {MR2415737},
  zmnumber = {1378.90064},
  keywords = {convex optimization,Coordinate descent,lasso},
  note = {https://projecteuclid.org/euclid.aoas/1196438020},
  file = {/home/bolker/Documents/zotero_new/storage/A6RT6MVR/Friedman et al. - 2007 - Pathwise coordinate optimization.pdf;/home/bolker/Documents/zotero_new/storage/6PL3GMCT/1196438020.html}
}

@article{friedmanRegularization2010,
  title = {Regularization {{Paths}} for {{Generalized Linear Models}} via {{Coordinate Descent}}},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2010},
  journal = {Journal of statistical software},
  volume = {33},
  number = {1},
  pages = {1--22},
  issn = {1548-7660},
  abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ1 (the lasso), ℓ2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
  pmcid = {PMC2929880},
  pmid = {20808728},
  note = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2929880/},
  file = {/home/bolker/Documents/zotero_new/storage/3P9HUKZH/Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf}
}

@book{jamesintroduction2013,
  title = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  volume = {112},
  publisher = {{Springer}},
  file = {/home/bolker/Documents/zotero_new/storage/KY22P8UZ/James et al. - 2013 - An introduction to statistical learning.pdf;/home/bolker/Documents/zotero_new/storage/AQBFJVSN/10.html}
}

@article{jansonEffective2015,
  title = {Effective {{Degrees}} of {{Freedom}}: {{A Flawed Metaphor}}},
  shorttitle = {Effective {{Degrees}} of {{Freedom}}},
  author = {Janson, Lucas and Fithian, William and Hastie, Trevor},
  year = {2015},
  month = jun,
  journal = {Biometrika},
  volume = {102},
  number = {2},
  eprint = {1312.7851},
  eprinttype = {arxiv},
  pages = {479--485},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asv019},
  abstract = {To most applied statisticians, a fitting procedure's degrees of freedom is synonymous with its model complexity, or its capacity for overfitting to data. In particular, it is often used to parameterize the bias-variance tradeoff in model selection. We argue that, contrary to folk intuition, model complexity and degrees of freedom are not synonymous and may correspond very poorly. We exhibit and theoretically explore various examples of fitting procedures for which degrees of freedom is not monotonic in the model complexity parameter, and can exceed the total dimension of the response space. Even in very simple settings, the degrees of freedom can exceed the dimension of the ambient space by an arbitrarily large amount. We show the degrees of freedom for any non-convex projection method can be unbounded.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,Statistics - Other Statistics},
  note = {http://arxiv.org/abs/1312.7851},
  file = {/home/bolker/Documents/zotero_new/storage/4R9W6EK4/Janson et al. - 2015 - Effective Degrees of Freedom A Flawed Metaphor.pdf;/home/bolker/Documents/zotero_new/storage/IDJCQPKA/1312.html}
}

@article{simonRegularization2011,
  title = {Regularization {{Paths}} for {{Cox}}’s {{Proportional Hazards Model}} via {{Coordinate Descent}}},
  author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2011},
  month = mar,
  journal = {Journal of statistical software},
  volume = {39},
  number = {5},
  pages = {1--13},
  issn = {1548-7660},
  doi = {10.18637/jss.v039.i05},
  abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of ℓ1 and ℓ2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
  pmcid = {PMC4824408},
  pmid = {27065756},
  note = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4824408/},
  file = {/home/bolker/Documents/zotero_new/storage/BN5NERSF/Simon et al. - 2011 - Regularization Paths for Cox’s Proportional Hazard.pdf}
}

@article{zoudegrees2007,
  title = {On the “Degrees of Freedom” of the Lasso},
  author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
  year = {2007},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {35},
  number = {5},
  pages = {2173--2192},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053607000000127},
  abstract = {We study the effective degrees of freedom of the lasso in the framework of Stein’s unbiased risk estimation (SURE). We show that the number of nonzero coefficients is an unbiased estimate for the degrees of freedom of the lasso—a conclusion that requires no special assumption on the predictors. In addition, the unbiased estimator is shown to be asymptotically consistent. With these results on hand, various model selection criteria—Cp, AIC and BIC—are available, which, along with the LARS algorithm, provide a principled and efficient approach to obtaining the optimal lasso fit with the computational effort of a single ordinary least-squares fit.},
  langid = {english},
  mrnumber = {MR2363967},
  zmnumber = {1126.62061},
  keywords = {Degrees of freedom,LARS algorithm,lasso,model selection,SURE,unbiased estimate},
  note = {https://projecteuclid.org/euclid.aos/1194461726},
  file = {/home/bolker/Documents/zotero_new/storage/2BGWHRP2/Zou et al. - 2007 - On the “degrees of freedom” of the lasso.pdf;/home/bolker/Documents/zotero_new/storage/P9AKEXYL/1194461726.html}
}

@book{hastieelements2009,
  title = {The Elements of Statistical Learning Data Mining, Inference, and Prediction},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H},
  year = {2009},
  publisher = {{Springer}},
  address = {{New York}},
  abstract = {"During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics."--Jacket.},
  isbn = {978-0-387-84858-7 0-387-84858-4 978-0-387-84857-0 0-387-84857-6},
  langid = {english},
  note = {http://public.eblib.com/EBLPublic/PublicView.do?ptiID=437866}
}

@article{wittenpenalized2009,
  title = {A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis},
  author = {Witten, Daniela M. and Tibshirani, Robert and Hastie, Trevor},
  year = {2009},
  month = jan,
  journal = {Biostatistics},
  pages = {kxp008},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxp008},
  abstract = {We present a penalized matrix decomposition (PMD), a new framework for computing a rank-K approximation for a matrix. We approximate the matrix X as , where dk, uk, and vk minimize the squared Frobenius norm of X, subject to penalties on uk and vk. This results in a regularized version of the singular value decomposition. Of particular interest is the use of L1-penalties on uk and vk, which yields a decomposition of X using sparse vectors. We show that when the PMD is applied using an L1-penalty on vk but not on uk, a method for sparse principal components results. In fact, this yields an efficient algorithm for the “SCoTLASS” proposal (Jolliffe and others 2003) for obtaining sparse principal components. This method is demonstrated on a publicly available gene expression data set. We also establish connections between the SCoTLASS method for sparse principal component analysis and the method of Zou and others (2006). In addition, we show that when the PMD is applied to a cross-products matrix, it results in a method for penalized canonical correlation analysis (CCA). We apply this penalized CCA method to simulated data and to a genomic data set consisting of gene expression and DNA copy number measurements on the same set of samples.},
  langid = {english},
  pmid = {19377034},
  keywords = {Canonical correlation analysis,DNA copy number,Integrative genomic analysis,L1,Matrix decomposition,Principal component analysis,Sparse principal component analysis,SVD},
  note = {http://biostatistics.oxfordjournals.org/content/early/2009/04/17/biostatistics.kxp008},
  file = {/home/bolker/Documents/zotero_new/storage/8DFHTXWR/Witten et al. - 2009 - A penalized matrix decomposition, with application.pdf;/home/bolker/Documents/zotero_new/storage/9CHTHTAV/biostatistics.kxp008.html}
}

@article{breimanHeuristics1996,
  title = {Heuristics of Instability and Stabilization in Model Selection},
  author = {Breiman, Leo},
  year = {1996},
  journal = {The Annals of Statistics},
  volume = {24},
  number = {6},
  pages = {2350--2383},
  issn = {0090-5364},
  doi = {10.1214/aos/1032181158},
  mrnumber = {1425957},
  note = {https://mathscinet.ams.org/mathscinet-getitem?mr=1425957},
  file = {/home/bolker/Documents/zotero_new/storage/JG3S24MA/Breiman - 1996 - Heuristics of instability and stabilization in mod.pdf;/home/bolker/Documents/zotero_new/storage/B9QY8VE2/mathscinet-getitem.html}
}

@article{breimanStatistical2001,
  title = {Statistical {{Modeling}}: {{The Two Cultures}}},
  shorttitle = {Statistical {{Modeling}}},
  author = {Breiman, Leo},
  year = {2001},
  month = aug,
  journal = {Statistical Science},
  volume = {16},
  number = {3},
  pages = {199--215},
  issn = {08834237},
  annotation = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  note = {http://www.jstor.org/stable/2676681},
  file = {/home/bolker/Documents/zotero_new/storage/46GU69P4/Breiman - 2001 - Statistical Modeling The Two Cultures.pdf;/home/bolker/Documents/zotero_new/storage/4I2ISTJD/2676681.html}
}

@article{breimanSubmodel1992,
  title = {Submodel {{Selection}} and {{Evaluation}} in {{Regression}}. {{The X-Random Case}}},
  author = {Breiman, Leo and Spector, Philip},
  year = {1992},
  journal = {International Statistical Review / Revue Internationale de Statistique},
  volume = {60},
  number = {3},
  pages = {291--319},
  publisher = {{[Wiley, International Statistical Institute (ISI)]}},
  issn = {0306-7734},
  doi = {10.2307/1403680},
  abstract = {Often, in a regression situation with many variables, a sequence of submodels is generated containing fewer variables by using such methods as stepwise addition or deletion of variables, or 'best subsets'. The question is which of this sequence of submodels is 'best', and how can submodel performance be evaluated. This was explored in Breiman (1988) for a fixed X-design. This is a sequel exploring the case of random X-designs. Analytical results are difficult, if not impossible. This study involved an extensive simulation. The basis of the study is the theoretical definition of prediction error (PE) as the expected squared error produced by applying a prediction equation to the distributional universe of (y, x) values. This definition is used throughout to compare various submodels. There can be startling differences between the x-fixed and x-random situations and different PE estimates are appropriate. Non-resampling estimates such as CP, adjusted R2, etc. turn out to be highly biased methods for submodel selection. The two best methods are cross-validation and bootstrap. One surprise is that 5 fold cross-validation (leave out 20\% of the data) is better at submodel selection and evaluation than leave-one-out cross-validation. There are a number of other surprises. /// Dans l'analyse de problèmes de régression à plusieurs variables (indépendantes), on produit souvent une série de sous-modèles constitués d'un sous-ensemble des variables par des méthodes telles que l'addition par étape, le retrait par étape et la méthode du meilleur sous-ensemble. Le problème est de déterminer lequel de ces sous-modèles est le meilleur et d'évaluer sa performance. Ce problème fut exploré dans Breiman (1988) pour le cas d'une matrice X fixe. Dans ce qui suit on considère le cas où la matrice X est aléatoire. La détermination de résultats analytiques est difficile, sinon impossible. Notre étude a utilisé des simulations de grande envergure. Elle se base sur la définition théorique de l'erreur de prédiction (EP) comme étant l'espérance du carré de l'erreur produite en applicant une équation de prédiction à l'univers distributional des valeurs (y, x). La définition est utilisée dans toute l'étude à fin de comparer divers sous-modèles. Il y a une différence étonnante entre le cas où la matrice X est fixée et celui où elle est aléatoire. Différents estimateurs de la EP sont à propos. Les estimateurs n'utilisant pas de ré-échantillonage, tels que le Cp et le R2 ajusté, produisent des méthodes de sélection ayant grand biais. Les deux meilleures méthodes sont la validation croisée et l'autoamorçage. Une surprise est que la validation croisée quintuple est meilleure que la validation croisée tous sauf un. Il y a plusieurs autres résultats surprenants.},
  note = {https://www.jstor.org/stable/1403680}
}

@article{mccormickgiven2021,
  title = {The "given Data" Paradigm Undermines Both Cultures},
  author = {McCormick, Tyler},
  year = {2021},
  month = may,
  journal = {arXiv:2105.12478 [cs, stat]},
  eprint = {2105.12478},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Breiman organizes "Statistical modeling: The two cultures" around a simple visual. Data, to the far right, are compelled into a "black box" with an arrow and then catapulted left by a second arrow, having been transformed into an output. Breiman then posits two interpretations of this visual as encapsulating a distinction between two cultures in statistics. The divide, he argues is about what happens in the "black box." In this comment, I argue for a broader perspective on statistics and, in doing so, elevate questions from "before" and "after" the box as fruitful areas for statistical innovation and practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {http://arxiv.org/abs/2105.12478},
  file = {/home/bolker/Documents/zotero_new/storage/FH534ALZ/McCormick - 2021 - The given data paradigm undermines both cultures.pdf;/home/bolker/Documents/zotero_new/storage/DDEXI6D5/2105.html}
}

@article{meinshausenQuantile,
  title = {Quantile {{Regression Forests}}},
  author = {Meinshausen, Nicolai},
  pages = {17},
  abstract = {Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.},
  langid = {english},
  file = {/home/bolker/Documents/zotero_new/storage/CEFFUZ6I/Meinshausen - Quantile Regression Forests.pdf}
}

@article{millerBreiman2021,
  title = {Breiman's Two Cultures: {{You}} Don't Have to Choose Sides},
  shorttitle = {Breiman's Two Cultures},
  author = {Miller, Andrew C. and Foti, Nicholas J. and Fox, Emily B.},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.12219 [cs, stat]},
  eprint = {2104.12219},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Breiman's classic paper casts data analysis as a choice between two cultures: data modelers and algorithmic modelers. Stated broadly, data modelers use simple, interpretable models with well-understood theoretical properties to analyze data. Algorithmic modelers prioritize predictive accuracy and use more flexible function approximations to analyze data. This dichotomy overlooks a third set of models \$-\$ mechanistic models derived from scientific theories (e.g., ODE/SDE simulators). Mechanistic models encode application-specific scientific knowledge about the data. And while these categories represent extreme points in model space, modern computational and algorithmic tools enable us to interpolate between these points, producing flexible, interpretable, and scientifically-informed hybrids that can enjoy accurate and robust predictions, and resolve issues with data analysis that Breiman describes, such as the Rashomon effect and Occam's dilemma. Challenges still remain in finding an appropriate point in model space, with many choices on how to compose model components and the degree to which each component informs inferences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  note = {http://arxiv.org/abs/2104.12219},
  file = {/home/bolker/Documents/zotero_new/storage/9J4KDYIH/Miller et al. - 2021 - Breiman's two cultures You don't have to choose s.pdf;/home/bolker/Documents/zotero_new/storage/QHRQMC62/2104.html}
}

@article{vanhouwelingenShrinkage2001,
  title = {Shrinkage and {{Penalized Likelihood}} as {{Methods}} to {{Improve Predictive Accuracy}}},
  author = {{van Houwelingen}, J. C},
  year = {2001},
  journal = {Statistica Neerlandica},
  volume = {55},
  number = {1},
  pages = {17--34},
  issn = {0039-0402},
  doi = {10.1111/1467-9574.00154},
  abstract = {A review is given of shrinkage and penalization as tools to improve predictive accuracy of regression models. The James-Stein estimator is taken as starting point. Procedures covered are Pre-test Estimation, the Ridge Regression of Hoerl and Kennard, the Shrinkage Estimators of Copas and Van Houwelingen and Le Cessie, the LASSO of Tibshirani and the Garotte of Breiman. An attempt is made to place all these procedures in a unifying framework of semi-Bayesian methodology. Applications are briefly mentioned, but not amply discussed.},
  keywords = {Garotte,LASSO,Pre-test Estimation,Ridge Regression},
  note = {http://journals.scholarsportal.info/details/00390402/v55i0001/17_saplamtipa.xml},
  file = {/home/bolker/Documents/zotero_new/storage/DAFUHND7/van Houwelingen - 2001 - Shrinkage and Penalized Likelihood as Methods to I.pdf}
}


@article{gelmanReflections2021,
  title = {Reflections on {{Breiman}}'s {{Two Cultures}} of {{Statistical Modeling}}},
  author = {Gelman, Andrew},
  year = {2021},
  journal = {Observational Studies},
  volume = {7},
  number = {1},
  pages = {95--98},
  publisher = {{University of Pennsylvania Press}},
  issn = {2767-3324},
  doi = {10.1353/obs.2021.0025},
  abstract = {In his article on Two Cultures of Statistical Modeling, Leo Breiman argued for an algorithmic approach to statistics, as exemplified by his pathbreaking research on large regularized models that fit data and have good predictive properties but without attempting to capture true underlying structure. I think Breiman was right about the benefits of open-ended predictive methods for complex modern problems. I also discuss some points of disagreement, notably Breiman's dismissal of Bayesian methods, which I think reflected a misunderstanding on his part, in that he did not recognized that Bayesian inference can be viewed as regularized prediction and does not rely on an assumption that the fitted model is true. In retrospect, we can learn both from Breiman's deep foresight and from his occasional oversights.},
  keywords = {algorithms,Bayesian inference,prediction,statistical modeling},
  note = {https://muse.jhu.edu/article/799750},
  file = {/home/bolker/Documents/zotero_new/storage/NCI8H5A2/Gelman - 2021 - Reflections on Breiman's Two Cultures of Statistic.pdf}
}

@article{raperLeo2020,
  title = {Leo {{Breiman}}'s "{{Two Cultures}}"},
  author = {Raper, Simon},
  year = {2020},
  journal = {Significance},
  volume = {17},
  number = {1},
  pages = {34--37},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2020.01357.x},
  abstract = {Simon Raper continues his exploration of key moments in the history of statistics with a close look at Leo Breiman's 2001 critique of the data modelling and algorithmic modelling cultures. Almost 20 years on, Breiman's words are perhaps more relevant than ever},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2020.01357.x},
  note = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2020.01357.x},
  file = {/home/bolker/Documents/zotero_new/storage/LWUBZBRK/Raper - 2020 - Leo Breiman's “Two Cultures”.pdf;/home/bolker/Documents/zotero_new/storage/YT5CS5RB/j.1740-9713.2020.01357.html}
}

@misc{ratzCan2021,
  title = {Can {{QR Decomposition Be Actually Faster}}? {{Schwarz-Rutishauser Algorithm}}},
  shorttitle = {Can {{QR Decomposition Be Actually Faster}}?},
  author = {Ratz, Arthur V.},
  year = {2021},
  month = apr,
  journal = {Medium},
  abstract = {QR Factorization. Schwarz-Rutishauser Algorithm (Explained). Performance Optimization. 100\% Working Code Samples In Python 3.9.x And NumPy},
  howpublished = {https://towardsdatascience.com/can-qr-decomposition-be-actually-faster-schwarz-rutishauser-algorithm-a32c0cde8b9b},
  langid = {english},
  file = {/home/bolker/Zotero/storage/SI6AJ2M3/can-qr-decomposition-be-actually-faster-schwarz-rutishauser-algorithm-a32c0cde8b9b.html}
}

@article{elithWorking2008a,
  title = {A Working Guide to Boosted Regression Trees},
  author = {Elith, J. and Leathwick, J. R. and Hastie, T.},
  year = {2008},
  month = jul,
  journal = {Journal of Animal Ecology},
  volume = {77},
  number = {4},
  pages = {802--813},
  issn = {1365-2656},
  doi = {10.1111/j.1365-2656.2008.01390.x},
  abstract = {1 Ecologists use statistical models for both explanation and prediction, and need techniques that are flexible enough to express typical features of their data, such as nonlinearities and interactions. 2 This study provides a working guide to boosted regression trees (BRT), an ensemble method for fitting statistical models that differs fundamentally from conventional techniques that aim to fit a single parsimonious model. Boosted regression trees combine the strengths of two algorithms: regression trees (models that relate a response to their predictors by recursive binary splits) and boosting (an adaptive method for combining many simple models to give improved predictive performance). The final BRT model can be understood as an additive regression model in which individual terms are simple trees, fitted in a forward, stagewise fashion. 3 Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data. They have no need for prior data transformation or elimination of outliers, can fit complex nonlinear relationships, and automatically handle interaction effects between predictors. Fitting multiple trees in BRT overcomes the biggest drawback of single tree models: their relatively poor predictive performance. Although BRT models are complex, they can be summarized in ways that give powerful ecological insight, and their predictive performance is superior to most traditional modelling methods. 4 The unique features of BRT raise a number of practical issues in model fitting. We demonstrate the practicalities and advantages of using BRT through a distributional analysis of the short-finned eel (Anguilla australis Richardson), a native freshwater fish of New Zealand. We use a data set of over 13 000 sites to illustrate effects of several settings, and then fit and interpret a model using a subset of the data. We provide code and a tutorial to enable the wider use of BRT by ecologists.},
  copyright = {\textcopyright{} 2008 The Authors. Journal compilation \textcopyright{} 2008 British Ecological Society},
  langid = {english},
  keywords = {data mining,machine learning,model averaging,random forests,species distributions},
  file = {/home/bolker/Zotero/storage/G7UUPE4K/Elith et al. - 2008 - A working guide to boosted regression trees.pdf;/home/bolker/Zotero/storage/NNH3GMC4/j.1365-2656.2008.01390.html}
}

@article{bienLasso2013,
  title = {A Lasso for Hierarchical Interactions},
  author = {Bien, Jacob and Taylor, Jonathan and Tibshirani, Robert},
  year = {2013},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {41},
  number = {3},
  pages = {1111--1141},
  issn = {0090-5364},
  doi = {10.1214/13-AOS1096},
  abstract = {We add a set of convex constraints to the lasso to produce sparse interaction models that honor the hierarchy restriction that an interaction only be included in a model if one or both variables are marginally important. We give a precise characterization of the effect of this hierarchy constraint, prove that hierarchy holds with probability one and derive an unbiased estimate for the degrees of freedom of our estimator. A bound on this estimate reveals the amount of fitting ``saved'' by the hierarchy constraint.},
  langid = {english}
}

@article{friedmanPathwise2007,
  title = {Pathwise Coordinate Optimization},
  author = {Friedman, Jerome and Hastie, Trevor and H{\"o}fling, Holger and Tibshirani, Robert},
  year = {2007},
  month = dec,
  journal = {Annals of Applied Statistics},
  volume = {1},
  number = {2},
  pages = {302--332},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/07-AOAS131},
  abstract = {We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the L1-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the ``fused lasso,'' however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems.},
  langid = {english},
  mrnumber = {MR2415737},
  zmnumber = {1378.90064},
  keywords = {convex optimization,Coordinate descent,lasso},
  file = {/home/bolker/Zotero/storage/A6RT6MVR/Friedman et al. - 2007 - Pathwise coordinate optimization.pdf;/home/bolker/Zotero/storage/6PL3GMCT/1196438020.html}
}

@article{wittenPenalized2009,
  title = {A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis},
  author = {Witten, Daniela M. and Tibshirani, Robert and Hastie, Trevor},
  year = {2009},
  month = jan,
  journal = {Biostatistics},
  pages = {kxp008},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxp008},
  abstract = {We present a penalized matrix decomposition (PMD), a new framework for computing a rank-K approximation for a matrix. We approximate the matrix X as , where dk, uk, and vk minimize the squared Frobenius norm of X, subject to penalties on uk and vk. This results in a regularized version of the singular value decomposition. Of particular interest is the use of L1-penalties on uk and vk, which yields a decomposition of X using sparse vectors. We show that when the PMD is applied using an L1-penalty on vk but not on uk, a method for sparse principal components results. In fact, this yields an efficient algorithm for the ``SCoTLASS'' proposal (Jolliffe and others 2003) for obtaining sparse principal components. This method is demonstrated on a publicly available gene expression data set. We also establish connections between the SCoTLASS method for sparse principal component analysis and the method of Zou and others (2006). In addition, we show that when the PMD is applied to a cross-products matrix, it results in a method for penalized canonical correlation analysis (CCA). We apply this penalized CCA method to simulated data and to a genomic data set consisting of gene expression and DNA copy number measurements on the same set of samples.},
  langid = {english},
  pmid = {19377034},
  keywords = {Canonical correlation analysis,DNA copy number,Integrative genomic analysis,L1,Matrix decomposition,Principal component analysis,Sparse principal component analysis,SVD},
  file = {/home/bolker/Zotero/storage/8DFHTXWR/Witten et al. - 2009 - A penalized matrix decomposition, with application.pdf;/home/bolker/Zotero/storage/9CHTHTAV/biostatistics.kxp008.html}
}


@article{hastieRidge2020,
  title = {Ridge {{Regularization}}: {{An Essential Concept}} in {{Data Science}}},
  shorttitle = {Ridge {{Regularization}}},
  author = {Hastie, Trevor},
  year = {2020},
  month = oct,
  journal = {Technometrics},
  volume = {62},
  number = {4},
  pages = {426--433},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2020.1791959},
  abstract = {Ridge or more formally l2 regularization shows up in many areas of statistics and machine learning. It is one of those essential devices that any good data scientist needs to master for their craft. In this brief ridge fest, I have collected together some of the magic and beauty of ridge that my colleagues and I have encountered over the past 40 years in applied statistics.},
  keywords = {Data science,Retrospective,Ridge regression},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2020.1791959},
  file = {/home/bolker/Zotero/storage/732VNZZ9/Hastie - 2020 - Ridge Regularization An Essential Concept in Data.pdf}
}


@book{nazarathyStatistics2021,
  title = {Statistics with {Julia}: Fundamentals for Data Science, Machine Learning and Artificial Intelligence},
  shorttitle = {Statistics with {{Julia}}},
  author = {Nazarathy, Yoni and Klok, Hayden},
  year = {2021},
  series = {Springer {{Series}} in the {{Data Sciences}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-70901-3},
  isbn = {978-3-030-70900-6 978-3-030-70901-3},
  langid = {english}
}

@misc{navarroScience2019,
  title = {Science and Statistics},
  author = {Navarro, Danielle},
  year = {2019},
  month = mar,
  address = {{Aarhus University}},
  abstract = {http://interactingminds.au.dk/events/single-events/artikel/2-day-workshop-open-science-and-reproducibility/},
  langid = {english},
  file = {/home/bolker/Zotero/storage/UX69QA8G/scienceandstatistics.html},
  url = {https://slides.com/djnavarro/scienceandstatistics}
}

@misc{adarValue2015,
  title = {On the Value of Command-Line ``Bullshittery''},
  author = {Adar, Eytan},
  year = {2015},
  month = oct,
  journal = {Medium},
  abstract = {Go read this first for context. Done? Good.},
  url = {https://medium.com/@eytanadar/on-the-value-of-command-line-bullshittery-94dc19ec8c61},
  langid = {english},
  file = {/home/bolker/Zotero/storage/VFLDYT2H/on-the-value-of-command-line-bullshittery-94dc19ec8c61.html}
}
