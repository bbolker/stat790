@article{batesCrossvalidation,
  title = {Cross-Validation: What Does It Estimate and How Well Does It Do It?},
  author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
  pages = {36},
  abstract = {Cross-validation is a widely-used technique to estimate prediction error, but its behavior is complex and not fully understood. Ideally, one would like to think that cross-validation estimates the prediction error for the model at hand, fit to the training data. We prove that this is not the case for the linear model fit by ordinary least squares; rather it estimates the average prediction error of models fit on other unseen training sets drawn from the same population. We further show that this phenomenon occurs for most popular estimates of prediction error, including data splitting, bootstrapping, and Mallow’s Cp. Next, the standard confidence intervals for prediction error derived from cross-validation may have coverage far below the desired level. Because each data point is used for both training and testing, there are correlations among the measured accuracies for each fold, and so the usual estimate of variance is too small. We introduce a nested cross-validation scheme to estimate this variance more accurately, and show empirically that this modification leads to intervals with approximately correct coverage in many examples where traditional cross-validation intervals fail. Lastly, our analysis also shows that when producing confidence intervals for prediction accuracy with simple data splitting, one should not re-fit the model on the combined data, since this invalidates the confidence intervals.},
  langid = {english},
  file = {/home/bolker/Documents/zotero_new/storage/XRCEZVAJ/Bates et al. - Cross-validation what does it estimate and how we.pdf}
}

@article{elithworking2008,
  title = {A Working Guide to Boosted Regression Trees},
  author = {Elith, J. and Leathwick, J. R. and Hastie, T.},
  year = {2008},
  journal = {Journal of Animal Ecology},
  volume = {77},
  number = {4},
  pages = {802--813},
  issn = {1365-2656},
  doi = {10.1111/j.1365-2656.2008.01390.x},
  abstract = {1 Ecologists use statistical models for both explanation and prediction, and need techniques that are flexible enough to express typical features of their data, such as nonlinearities and interactions. 2 This study provides a working guide to boosted regression trees (BRT), an ensemble method for fitting statistical models that differs fundamentally from conventional techniques that aim to fit a single parsimonious model. Boosted regression trees combine the strengths of two algorithms: regression trees (models that relate a response to their predictors by recursive binary splits) and boosting (an adaptive method for combining many simple models to give improved predictive performance). The final BRT model can be understood as an additive regression model in which individual terms are simple trees, fitted in a forward, stagewise fashion. 3 Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data. They have no need for prior data transformation or elimination of outliers, can fit complex nonlinear relationships, and automatically handle interaction effects between predictors. Fitting multiple trees in BRT overcomes the biggest drawback of single tree models: their relatively poor predictive performance. Although BRT models are complex, they can be summarized in ways that give powerful ecological insight, and their predictive performance is superior to most traditional modelling methods. 4 The unique features of BRT raise a number of practical issues in model fitting. We demonstrate the practicalities and advantages of using BRT through a distributional analysis of the short-finned eel (Anguilla australis Richardson), a native freshwater fish of New Zealand. We use a data set of over 13 000 sites to illustrate effects of several settings, and then fit and interpret a model using a subset of the data. We provide code and a tutorial to enable the wider use of BRT by ecologists.},
  langid = {english},
  keywords = {data mining,machine learning,model averaging,random forests,species distributions},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2656.2008.01390.x},
  note = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2656.2008.01390.x},
  file = {/home/bolker/Documents/zotero_new/storage/AJQP9HL5/Elith et al. - 2008 - A working guide to boosted regression trees.pdf;/home/bolker/Documents/zotero_new/storage/83EYN5SA/j.1365-2656.2008.01390.html}
}

@article{friedmanPathwise2007,
  title = {Pathwise Coordinate Optimization},
  author = {Friedman, Jerome and Hastie, Trevor and Höfling, Holger and Tibshirani, Robert},
  year = {2007},
  month = dec,
  journal = {Annals of Applied Statistics},
  volume = {1},
  number = {2},
  pages = {302--332},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/07-AOAS131},
  abstract = {We consider “one-at-a-time” coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the L1-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the “fused lasso,” however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems.},
  langid = {english},
  mrnumber = {MR2415737},
  zmnumber = {1378.90064},
  keywords = {convex optimization,Coordinate descent,lasso},
  note = {https://projecteuclid.org/euclid.aoas/1196438020},
  file = {/home/bolker/Documents/zotero_new/storage/A6RT6MVR/Friedman et al. - 2007 - Pathwise coordinate optimization.pdf;/home/bolker/Documents/zotero_new/storage/6PL3GMCT/1196438020.html}
}

@article{friedmanRegularization2010,
  title = {Regularization {{Paths}} for {{Generalized Linear Models}} via {{Coordinate Descent}}},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2010},
  journal = {Journal of statistical software},
  volume = {33},
  number = {1},
  pages = {1--22},
  issn = {1548-7660},
  abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ1 (the lasso), ℓ2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
  pmcid = {PMC2929880},
  pmid = {20808728},
  note = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2929880/},
  file = {/home/bolker/Documents/zotero_new/storage/3P9HUKZH/Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf}
}

@article{friedmanRegularization2010a,
  title = {Regularization {{Paths}} for {{Generalized Linear Models}} via {{Coordinate Descent}}},
  author = {Friedman, Jerome H. and Hastie, Trevor and Tibshirani, Rob},
  year = {2010},
  month = feb,
  journal = {Journal of Statistical Software},
  volume = {33},
  number = {1},
  pages = {1--22},
  issn = {1548-7660},
  doi = {10.18637/jss.v033.i01},
  copyright = {Copyright (c) 2009 Jerome H. Friedman, Trevor Hastie, Rob Tibshirani},
  langid = {english},
  note = {https://www.jstatsoft.org/index.php/jss/article/view/v033i01},
  file = {/home/bolker/Documents/zotero_new/storage/UD4RHCCA/Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf;/home/bolker/Documents/zotero_new/storage/XGQZRPV3/v033i01.html}
}

@book{jamesintroduction2013,
  title = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  volume = {112},
  publisher = {{Springer}},
  file = {/home/bolker/Documents/zotero_new/storage/KY22P8UZ/James et al. - 2013 - An introduction to statistical learning.pdf;/home/bolker/Documents/zotero_new/storage/AQBFJVSN/10.html}
}

@article{jansonEffective2015,
  title = {Effective {{Degrees}} of {{Freedom}}: {{A Flawed Metaphor}}},
  shorttitle = {Effective {{Degrees}} of {{Freedom}}},
  author = {Janson, Lucas and Fithian, William and Hastie, Trevor},
  year = {2015},
  month = jun,
  journal = {Biometrika},
  volume = {102},
  number = {2},
  eprint = {1312.7851},
  eprinttype = {arxiv},
  pages = {479--485},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asv019},
  abstract = {To most applied statisticians, a fitting procedure's degrees of freedom is synonymous with its model complexity, or its capacity for overfitting to data. In particular, it is often used to parameterize the bias-variance tradeoff in model selection. We argue that, contrary to folk intuition, model complexity and degrees of freedom are not synonymous and may correspond very poorly. We exhibit and theoretically explore various examples of fitting procedures for which degrees of freedom is not monotonic in the model complexity parameter, and can exceed the total dimension of the response space. Even in very simple settings, the degrees of freedom can exceed the dimension of the ambient space by an arbitrarily large amount. We show the degrees of freedom for any non-convex projection method can be unbounded.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,Statistics - Other Statistics},
  note = {http://arxiv.org/abs/1312.7851},
  file = {/home/bolker/Documents/zotero_new/storage/4R9W6EK4/Janson et al. - 2015 - Effective Degrees of Freedom A Flawed Metaphor.pdf;/home/bolker/Documents/zotero_new/storage/IDJCQPKA/1312.html}
}

@article{simonRegularization2011,
  title = {Regularization {{Paths}} for {{Cox}}’s {{Proportional Hazards Model}} via {{Coordinate Descent}}},
  author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2011},
  month = mar,
  journal = {Journal of statistical software},
  volume = {39},
  number = {5},
  pages = {1--13},
  issn = {1548-7660},
  doi = {10.18637/jss.v039.i05},
  abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of ℓ1 and ℓ2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
  pmcid = {PMC4824408},
  pmid = {27065756},
  note = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4824408/},
  file = {/home/bolker/Documents/zotero_new/storage/BN5NERSF/Simon et al. - 2011 - Regularization Paths for Cox’s Proportional Hazard.pdf}
}

@article{zoudegrees2007,
  title = {On the “Degrees of Freedom” of the Lasso},
  author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
  year = {2007},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {35},
  number = {5},
  pages = {2173--2192},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053607000000127},
  abstract = {We study the effective degrees of freedom of the lasso in the framework of Stein’s unbiased risk estimation (SURE). We show that the number of nonzero coefficients is an unbiased estimate for the degrees of freedom of the lasso—a conclusion that requires no special assumption on the predictors. In addition, the unbiased estimator is shown to be asymptotically consistent. With these results on hand, various model selection criteria—Cp, AIC and BIC—are available, which, along with the LARS algorithm, provide a principled and efficient approach to obtaining the optimal lasso fit with the computational effort of a single ordinary least-squares fit.},
  langid = {english},
  mrnumber = {MR2363967},
  zmnumber = {1126.62061},
  keywords = {Degrees of freedom,LARS algorithm,lasso,model selection,SURE,unbiased estimate},
  note = {https://projecteuclid.org/euclid.aos/1194461726},
  file = {/home/bolker/Documents/zotero_new/storage/2BGWHRP2/Zou et al. - 2007 - On the “degrees of freedom” of the lasso.pdf;/home/bolker/Documents/zotero_new/storage/P9AKEXYL/1194461726.html}
}

