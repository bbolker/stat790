@article{batesCrossvalidation,
  title = {Cross-Validation: What Does It Estimate and How Well Does It Do It?},
  author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
  pages = {36},
  abstract = {Cross-validation is a widely-used technique to estimate prediction error, but its behavior is complex and not fully understood. Ideally, one would like to think that cross-validation estimates the prediction error for the model at hand, fit to the training data. We prove that this is not the case for the linear model fit by ordinary least squares; rather it estimates the average prediction error of models fit on other unseen training sets drawn from the same population. We further show that this phenomenon occurs for most popular estimates of prediction error, including data splitting, bootstrapping, and Mallow’s Cp. Next, the standard confidence intervals for prediction error derived from cross-validation may have coverage far below the desired level. Because each data point is used for both training and testing, there are correlations among the measured accuracies for each fold, and so the usual estimate of variance is too small. We introduce a nested cross-validation scheme to estimate this variance more accurately, and show empirically that this modification leads to intervals with approximately correct coverage in many examples where traditional cross-validation intervals fail. Lastly, our analysis also shows that when producing confidence intervals for prediction accuracy with simple data splitting, one should not re-fit the model on the combined data, since this invalidates the confidence intervals.},
  langid = {english},
  file = {/home/bolker/Documents/zotero_new/storage/XRCEZVAJ/Bates et al. - Cross-validation what does it estimate and how we.pdf}
}


@book{woodGeneralized2017,
	series = {{CRC} {Texts} in {Statistical} {Science}},
	title = {Generalized {Additive} {Models}: {An} {Introduction} with {R}},
	url = {https://www.amazon.com/Generalized-Additive-Models-Introduction-Statistical-ebook/dp/B071Z9L5D5/ref=sr_1_1?ie=UTF8&qid=1511887995&sr=8-1&keywords=wood+additive+models},
	urldate = {2017-11-28},
	publisher = {Chapman \& Hall},
	author = {Wood, Simon N.},
	year = {2017},
	file = {Generalized Additive Models\: An Introduction with R, Second Edition (Chapman & Hall/CRC Texts in Statistical Science) 2, Simon N. Wood - Amazon.com:/home/bolker/Documents/zotero_new/storage/Q79P94BB/ref=sr_1_1.html:text/html},
}

@article{elithworking2008,
  title = {A Working Guide to Boosted Regression Trees},
  author = {Elith, J. and Leathwick, J. R. and Hastie, T.},
  year = {2008},
  journal = {Journal of Animal Ecology},
  volume = {77},
  number = {4},
  pages = {802--813},
  issn = {1365-2656},
  doi = {10.1111/j.1365-2656.2008.01390.x},
  abstract = {1 Ecologists use statistical models for both explanation and prediction, and need techniques that are flexible enough to express typical features of their data, such as nonlinearities and interactions. 2 This study provides a working guide to boosted regression trees (BRT), an ensemble method for fitting statistical models that differs fundamentally from conventional techniques that aim to fit a single parsimonious model. Boosted regression trees combine the strengths of two algorithms: regression trees (models that relate a response to their predictors by recursive binary splits) and boosting (an adaptive method for combining many simple models to give improved predictive performance). The final BRT model can be understood as an additive regression model in which individual terms are simple trees, fitted in a forward, stagewise fashion. 3 Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data. They have no need for prior data transformation or elimination of outliers, can fit complex nonlinear relationships, and automatically handle interaction effects between predictors. Fitting multiple trees in BRT overcomes the biggest drawback of single tree models: their relatively poor predictive performance. Although BRT models are complex, they can be summarized in ways that give powerful ecological insight, and their predictive performance is superior to most traditional modelling methods. 4 The unique features of BRT raise a number of practical issues in model fitting. We demonstrate the practicalities and advantages of using BRT through a distributional analysis of the short-finned eel (Anguilla australis Richardson), a native freshwater fish of New Zealand. We use a data set of over 13 000 sites to illustrate effects of several settings, and then fit and interpret a model using a subset of the data. We provide code and a tutorial to enable the wider use of BRT by ecologists.},
  langid = {english},
  keywords = {data mining,machine learning,model averaging,random forests,species distributions},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2656.2008.01390.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2656.2008.01390.x},
  file = {/home/bolker/Documents/zotero_new/storage/AJQP9HL5/Elith et al. - 2008 - A working guide to boosted regression trees.pdf;/home/bolker/Documents/zotero_new/storage/83EYN5SA/j.1365-2656.2008.01390.html}
}

@article{friedmanPathwise2007,
  title = {Pathwise Coordinate Optimization},
  author = {Friedman, Jerome and Hastie, Trevor and Höfling, Holger and Tibshirani, Robert},
  year = {2007},
  month = dec,
  journal = {Annals of Applied Statistics},
  volume = {1},
  number = {2},
  pages = {302--332},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/07-AOAS131},
  abstract = {We consider “one-at-a-time” coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the L1-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the “fused lasso,” however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems.},
  langid = {english},
  mrnumber = {MR2415737},
  zmnumber = {1378.90064},
  keywords = {convex optimization,Coordinate descent,lasso},
  url ={https://projecteuclid.org/euclid.aoas/1196438020},
  file = {/home/bolker/Documents/zotero_new/storage/A6RT6MVR/Friedman et al. - 2007 - Pathwise coordinate optimization.pdf;/home/bolker/Documents/zotero_new/storage/6PL3GMCT/1196438020.html}
}

@article{friedmanRegularization2010,
  title = {Regularization {{Paths}} for {{Generalized Linear Models}} via {{Coordinate Descent}}},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2010},
  journal = {Journal of statistical software},
  volume = {33},
  number = {1},
  pages = {1--22},
  issn = {1548-7660},
  abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ1 (the lasso), ℓ2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
  pmcid = {PMC2929880},
  pmid = {20808728},
  url ={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2929880/},
  file = {/home/bolker/Documents/zotero_new/storage/3P9HUKZH/Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf}
}

@book{jamesintroduction2013,
  title = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  volume = {112},
  publisher = {{Springer}},
  file = {/home/bolker/Documents/zotero_new/storage/KY22P8UZ/James et al. - 2013 - An introduction to statistical learning.pdf;/home/bolker/Documents/zotero_new/storage/AQBFJVSN/10.html}
}

@article{jansonEffective2015,
  title = {Effective {{Degrees}} of {{Freedom}}: {{A Flawed Metaphor}}},
  shorttitle = {Effective {{Degrees}} of {{Freedom}}},
  author = {Janson, Lucas and Fithian, William and Hastie, Trevor},
  year = {2015},
  month = jun,
  journal = {Biometrika},
  volume = {102},
  number = {2},
  eprint = {1312.7851},
  eprinttype = {arxiv},
  pages = {479--485},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asv019},
  abstract = {To most applied statisticians, a fitting procedure's degrees of freedom is synonymous with its model complexity, or its capacity for overfitting to data. In particular, it is often used to parameterize the bias-variance tradeoff in model selection. We argue that, contrary to folk intuition, model complexity and degrees of freedom are not synonymous and may correspond very poorly. We exhibit and theoretically explore various examples of fitting procedures for which degrees of freedom is not monotonic in the model complexity parameter, and can exceed the total dimension of the response space. Even in very simple settings, the degrees of freedom can exceed the dimension of the ambient space by an arbitrarily large amount. We show the degrees of freedom for any non-convex projection method can be unbounded.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,Statistics - Other Statistics},
  url ={http://arxiv.org/abs/1312.7851},
  file = {/home/bolker/Documents/zotero_new/storage/4R9W6EK4/Janson et al. - 2015 - Effective Degrees of Freedom A Flawed Metaphor.pdf;/home/bolker/Documents/zotero_new/storage/IDJCQPKA/1312.html}
}

@article{simonRegularization2011,
  title = {Regularization {{Paths}} for {{Cox}}’s {{Proportional Hazards Model}} via {{Coordinate Descent}}},
  author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2011},
  month = mar,
  journal = {Journal of statistical software},
  volume = {39},
  number = {5},
  pages = {1--13},
  issn = {1548-7660},
  doi = {10.18637/jss.v039.i05},
  abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of ℓ1 and ℓ2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
  pmcid = {PMC4824408},
  pmid = {27065756},
  url ={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4824408/},
  file = {/home/bolker/Documents/zotero_new/storage/BN5NERSF/Simon et al. - 2011 - Regularization Paths for Cox’s Proportional Hazard.pdf}
}

@article{zoudegrees2007,
  title = {On the “Degrees of Freedom” of the Lasso},
  author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
  year = {2007},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {35},
  number = {5},
  pages = {2173--2192},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053607000000127},
  abstract = {We study the effective degrees of freedom of the lasso in the framework of Stein’s unbiased risk estimation (SURE). We show that the number of nonzero coefficients is an unbiased estimate for the degrees of freedom of the lasso—a conclusion that requires no special assumption on the predictors. In addition, the unbiased estimator is shown to be asymptotically consistent. With these results on hand, various model selection criteria—Cp, AIC and BIC—are available, which, along with the LARS algorithm, provide a principled and efficient approach to obtaining the optimal lasso fit with the computational effort of a single ordinary least-squares fit.},
  langid = {english},
  mrnumber = {MR2363967},
  zmnumber = {1126.62061},
  keywords = {Degrees of freedom,LARS algorithm,lasso,model selection,SURE,unbiased estimate},
  url ={https://projecteuclid.org/euclid.aos/1194461726},
  file = {/home/bolker/Documents/zotero_new/storage/2BGWHRP2/Zou et al. - 2007 - On the “degrees of freedom” of the lasso.pdf;/home/bolker/Documents/zotero_new/storage/P9AKEXYL/1194461726.html}
}

@book{hastieelements2009,
  title = {The Elements of Statistical Learning Data Mining, Inference, and Prediction},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H},
  year = {2009},
  publisher = {{Springer}},
  address = {{New York}},
  abstract = {"During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics."--Jacket.},
  isbn = {978-0-387-84858-7 0-387-84858-4 978-0-387-84857-0 0-387-84857-6},
  langid = {english},
  url ={http://public.eblib.com/EBLPublic/PublicView.do?ptiID=437866}
}

@article{wittenpenalized2009,
  title = {A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis},
  author = {Witten, Daniela M. and Tibshirani, Robert and Hastie, Trevor},
  year = {2009},
  month = jan,
  journal = {Biostatistics},
  pages = {kxp008},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxp008},
  abstract = {We present a penalized matrix decomposition (PMD), a new framework for computing a rank-K approximation for a matrix. We approximate the matrix X as , where dk, uk, and vk minimize the squared Frobenius norm of X, subject to penalties on uk and vk. This results in a regularized version of the singular value decomposition. Of particular interest is the use of L1-penalties on uk and vk, which yields a decomposition of X using sparse vectors. We show that when the PMD is applied using an L1-penalty on vk but not on uk, a method for sparse principal components results. In fact, this yields an efficient algorithm for the “SCoTLASS” proposal (Jolliffe and others 2003) for obtaining sparse principal components. This method is demonstrated on a publicly available gene expression data set. We also establish connections between the SCoTLASS method for sparse principal component analysis and the method of Zou and others (2006). In addition, we show that when the PMD is applied to a cross-products matrix, it results in a method for penalized canonical correlation analysis (CCA). We apply this penalized CCA method to simulated data and to a genomic data set consisting of gene expression and DNA copy number measurements on the same set of samples.},
  langid = {english},
  pmid = {19377034},
  keywords = {Canonical correlation analysis,DNA copy number,Integrative genomic analysis,L1,Matrix decomposition,Principal component analysis,Sparse principal component analysis,SVD},
  url ={http://biostatistics.oxfordjournals.org/content/early/2009/04/17/biostatistics.kxp008},
  file = {/home/bolker/Documents/zotero_new/storage/8DFHTXWR/Witten et al. - 2009 - A penalized matrix decomposition, with application.pdf;/home/bolker/Documents/zotero_new/storage/9CHTHTAV/biostatistics.kxp008.html}
}

@article{breimanHeuristics1996,
  title = {Heuristics of Instability and Stabilization in Model Selection},
  author = {Breiman, Leo},
  year = {1996},
  journal = {The Annals of Statistics},
  volume = {24},
  number = {6},
  pages = {2350--2383},
  issn = {0090-5364},
  doi = {10.1214/aos/1032181158},
  mrnumber = {1425957},
  url ={https://mathscinet.ams.org/mathscinet-getitem?mr=1425957},
  file = {/home/bolker/Documents/zotero_new/storage/JG3S24MA/Breiman - 1996 - Heuristics of instability and stabilization in mod.pdf;/home/bolker/Documents/zotero_new/storage/B9QY8VE2/mathscinet-getitem.html}
}

@article{breimanStatistical2001,
  title = {Statistical {{Modeling}}: {{The Two Cultures}}},
  shorttitle = {Statistical {{Modeling}}},
  author = {Breiman, Leo},
  year = {2001},
  month = aug,
  journal = {Statistical Science},
  volume = {16},
  number = {3},
  pages = {199--215},
  issn = {08834237},
  annotation = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  url ={http://www.jstor.org/stable/2676681},
  file = {/home/bolker/Documents/zotero_new/storage/46GU69P4/Breiman - 2001 - Statistical Modeling The Two Cultures.pdf;/home/bolker/Documents/zotero_new/storage/4I2ISTJD/2676681.html}
}

@article{breimanSubmodel1992,
  title = {Submodel {{Selection}} and {{Evaluation}} in {{Regression}}. {{The X-Random Case}}},
  author = {Breiman, Leo and Spector, Philip},
  year = {1992},
  journal = {International Statistical Review / Revue Internationale de Statistique},
  volume = {60},
  number = {3},
  pages = {291--319},
  publisher = {{[Wiley, International Statistical Institute (ISI)]}},
  issn = {0306-7734},
  doi = {10.2307/1403680},
  abstract = {Often, in a regression situation with many variables, a sequence of submodels is generated containing fewer variables by using such methods as stepwise addition or deletion of variables, or 'best subsets'. The question is which of this sequence of submodels is 'best', and how can submodel performance be evaluated. This was explored in Breiman (1988) for a fixed X-design. This is a sequel exploring the case of random X-designs. Analytical results are difficult, if not impossible. This study involved an extensive simulation. The basis of the study is the theoretical definition of prediction error (PE) as the expected squared error produced by applying a prediction equation to the distributional universe of (y, x) values. This definition is used throughout to compare various submodels. There can be startling differences between the x-fixed and x-random situations and different PE estimates are appropriate. Non-resampling estimates such as CP, adjusted R2, etc. turn out to be highly biased methods for submodel selection. The two best methods are cross-validation and bootstrap. One surprise is that 5 fold cross-validation (leave out 20\% of the data) is better at submodel selection and evaluation than leave-one-out cross-validation. There are a number of other surprises. /// Dans l'analyse de problèmes de régression à plusieurs variables (indépendantes), on produit souvent une série de sous-modèles constitués d'un sous-ensemble des variables par des méthodes telles que l'addition par étape, le retrait par étape et la méthode du meilleur sous-ensemble. Le problème est de déterminer lequel de ces sous-modèles est le meilleur et d'évaluer sa performance. Ce problème fut exploré dans Breiman (1988) pour le cas d'une matrice X fixe. Dans ce qui suit on considère le cas où la matrice X est aléatoire. La détermination de résultats analytiques est difficile, sinon impossible. Notre étude a utilisé des simulations de grande envergure. Elle se base sur la définition théorique de l'erreur de prédiction (EP) comme étant l'espérance du carré de l'erreur produite en applicant une équation de prédiction à l'univers distributional des valeurs (y, x). La définition est utilisée dans toute l'étude à fin de comparer divers sous-modèles. Il y a une différence étonnante entre le cas où la matrice X est fixée et celui où elle est aléatoire. Différents estimateurs de la EP sont à propos. Les estimateurs n'utilisant pas de ré-échantillonage, tels que le Cp et le R2 ajusté, produisent des méthodes de sélection ayant grand biais. Les deux meilleures méthodes sont la validation croisée et l'autoamorçage. Une surprise est que la validation croisée quintuple est meilleure que la validation croisée tous sauf un. Il y a plusieurs autres résultats surprenants.},
  url ={https://www.jstor.org/stable/1403680}
}

@article{mccormickgiven2021,
  title = {The "given Data" Paradigm Undermines Both Cultures},
  author = {McCormick, Tyler},
  year = {2021},
  month = may,
  journal = {arXiv:2105.12478 [cs, stat]},
  eprint = {2105.12478},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Breiman organizes "Statistical modeling: The two cultures" around a simple visual. Data, to the far right, are compelled into a "black box" with an arrow and then catapulted left by a second arrow, having been transformed into an output. Breiman then posits two interpretations of this visual as encapsulating a distinction between two cultures in statistics. The divide, he argues is about what happens in the "black box." In this comment, I argue for a broader perspective on statistics and, in doing so, elevate questions from "before" and "after" the box as fruitful areas for statistical innovation and practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  url ={http://arxiv.org/abs/2105.12478},
  file = {/home/bolker/Documents/zotero_new/storage/FH534ALZ/McCormick - 2021 - The given data paradigm undermines both cultures.pdf;/home/bolker/Documents/zotero_new/storage/DDEXI6D5/2105.html}
}

@article{meinshausenQuantile,
  title = {Quantile {{Regression Forests}}},
  author = {Meinshausen, Nicolai},
  pages = {17},
  abstract = {Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.},
  langid = {english},
  file = {/home/bolker/Documents/zotero_new/storage/CEFFUZ6I/Meinshausen - Quantile Regression Forests.pdf}
}

@article{millerBreiman2021,
  title = {Breiman's Two Cultures: {{You}} Don't Have to Choose Sides},
  shorttitle = {Breiman's Two Cultures},
  author = {Miller, Andrew C. and Foti, Nicholas J. and Fox, Emily B.},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.12219 [cs, stat]},
  eprint = {2104.12219},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Breiman's classic paper casts data analysis as a choice between two cultures: data modelers and algorithmic modelers. Stated broadly, data modelers use simple, interpretable models with well-understood theoretical properties to analyze data. Algorithmic modelers prioritize predictive accuracy and use more flexible function approximations to analyze data. This dichotomy overlooks a third set of models \$-\$ mechanistic models derived from scientific theories (e.g., ODE/SDE simulators). Mechanistic models encode application-specific scientific knowledge about the data. And while these categories represent extreme points in model space, modern computational and algorithmic tools enable us to interpolate between these points, producing flexible, interpretable, and scientifically-informed hybrids that can enjoy accurate and robust predictions, and resolve issues with data analysis that Breiman describes, such as the Rashomon effect and Occam's dilemma. Challenges still remain in finding an appropriate point in model space, with many choices on how to compose model components and the degree to which each component informs inferences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  url ={http://arxiv.org/abs/2104.12219},
  file = {/home/bolker/Documents/zotero_new/storage/9J4KDYIH/Miller et al. - 2021 - Breiman's two cultures You don't have to choose s.pdf;/home/bolker/Documents/zotero_new/storage/QHRQMC62/2104.html}
}

@article{vanhouwelingenShrinkage2001,
  title = {Shrinkage and {{Penalized Likelihood}} as {{Methods}} to {{Improve Predictive Accuracy}}},
  author = {{van Houwelingen}, J. C},
  year = {2001},
  journal = {Statistica Neerlandica},
  volume = {55},
  number = {1},
  pages = {17--34},
  issn = {0039-0402},
  doi = {10.1111/1467-9574.00154},
  abstract = {A review is given of shrinkage and penalization as tools to improve predictive accuracy of regression models. The James-Stein estimator is taken as starting point. Procedures covered are Pre-test Estimation, the Ridge Regression of Hoerl and Kennard, the Shrinkage Estimators of Copas and Van Houwelingen and Le Cessie, the LASSO of Tibshirani and the Garotte of Breiman. An attempt is made to place all these procedures in a unifying framework of semi-Bayesian methodology. Applications are briefly mentioned, but not amply discussed.},
  keywords = {Garotte,LASSO,Pre-test Estimation,Ridge Regression},
  url ={http://journals.scholarsportal.info/details/00390402/v55i0001/17_saplamtipa.xml},
  file = {/home/bolker/Documents/zotero_new/storage/DAFUHND7/van Houwelingen - 2001 - Shrinkage and Penalized Likelihood as Methods to I.pdf}
}


@article{gelmanReflections2021,
  title = {Reflections on {{Breiman}}'s {{Two Cultures}} of {{Statistical Modeling}}},
  author = {Gelman, Andrew},
  year = {2021},
  journal = {Observational Studies},
  volume = {7},
  number = {1},
  pages = {95--98},
  publisher = {{University of Pennsylvania Press}},
  issn = {2767-3324},
  doi = {10.1353/obs.2021.0025},
  abstract = {In his article on Two Cultures of Statistical Modeling, Leo Breiman argued for an algorithmic approach to statistics, as exemplified by his pathbreaking research on large regularized models that fit data and have good predictive properties but without attempting to capture true underlying structure. I think Breiman was right about the benefits of open-ended predictive methods for complex modern problems. I also discuss some points of disagreement, notably Breiman's dismissal of Bayesian methods, which I think reflected a misunderstanding on his part, in that he did not recognized that Bayesian inference can be viewed as regularized prediction and does not rely on an assumption that the fitted model is true. In retrospect, we can learn both from Breiman's deep foresight and from his occasional oversights.},
  keywords = {algorithms,Bayesian inference,prediction,statistical modeling},
  url ={https://muse.jhu.edu/article/799750},
  file = {/home/bolker/Documents/zotero_new/storage/NCI8H5A2/Gelman - 2021 - Reflections on Breiman's Two Cultures of Statistic.pdf}
}

@article{raperLeo2020,
  title = {Leo {{Breiman}}'s "{{Two Cultures}}"},
  author = {Raper, Simon},
  year = {2020},
  journal = {Significance},
  volume = {17},
  number = {1},
  pages = {34--37},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2020.01357.x},
  abstract = {Simon Raper continues his exploration of key moments in the history of statistics with a close look at Leo Breiman's 2001 critique of the data modelling and algorithmic modelling cultures. Almost 20 years on, Breiman's words are perhaps more relevant than ever},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2020.01357.x},
  url ={https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2020.01357.x},
  file = {/home/bolker/Documents/zotero_new/storage/LWUBZBRK/Raper - 2020 - Leo Breiman's “Two Cultures”.pdf;/home/bolker/Documents/zotero_new/storage/YT5CS5RB/j.1740-9713.2020.01357.html}
}

@misc{ratzCan2021,
  title = {Can {{QR Decomposition Be Actually Faster}}? {{Schwarz-Rutishauser Algorithm}}},
  shorttitle = {Can {{QR Decomposition Be Actually Faster}}?},
  author = {Ratz, Arthur V.},
  year = {2021},
  month = apr,
  journal = {Medium},
  abstract = {QR Factorization. Schwarz-Rutishauser Algorithm (Explained). Performance Optimization. 100\% Working Code Samples In Python 3.9.x And NumPy},
  howpublished = {https://towardsdatascience.com/can-qr-decomposition-be-actually-faster-schwarz-rutishauser-algorithm-a32c0cde8b9b},
  langid = {english},
  file = {/home/bolker/Zotero/storage/SI6AJ2M3/can-qr-decomposition-be-actually-faster-schwarz-rutishauser-algorithm-a32c0cde8b9b.html}
}

@article{elithWorking2008a,
  title = {A Working Guide to Boosted Regression Trees},
  author = {Elith, J. and Leathwick, J. R. and Hastie, T.},
  year = {2008},
  month = jul,
  journal = {Journal of Animal Ecology},
  volume = {77},
  number = {4},
  pages = {802--813},
  issn = {1365-2656},
  doi = {10.1111/j.1365-2656.2008.01390.x},
  abstract = {1 Ecologists use statistical models for both explanation and prediction, and need techniques that are flexible enough to express typical features of their data, such as nonlinearities and interactions. 2 This study provides a working guide to boosted regression trees (BRT), an ensemble method for fitting statistical models that differs fundamentally from conventional techniques that aim to fit a single parsimonious model. Boosted regression trees combine the strengths of two algorithms: regression trees (models that relate a response to their predictors by recursive binary splits) and boosting (an adaptive method for combining many simple models to give improved predictive performance). The final BRT model can be understood as an additive regression model in which individual terms are simple trees, fitted in a forward, stagewise fashion. 3 Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data. They have no need for prior data transformation or elimination of outliers, can fit complex nonlinear relationships, and automatically handle interaction effects between predictors. Fitting multiple trees in BRT overcomes the biggest drawback of single tree models: their relatively poor predictive performance. Although BRT models are complex, they can be summarized in ways that give powerful ecological insight, and their predictive performance is superior to most traditional modelling methods. 4 The unique features of BRT raise a number of practical issues in model fitting. We demonstrate the practicalities and advantages of using BRT through a distributional analysis of the short-finned eel (Anguilla australis Richardson), a native freshwater fish of New Zealand. We use a data set of over 13 000 sites to illustrate effects of several settings, and then fit and interpret a model using a subset of the data. We provide code and a tutorial to enable the wider use of BRT by ecologists.},
  copyright = {\textcopyright{} 2008 The Authors. Journal compilation \textcopyright{} 2008 British Ecological Society},
  langid = {english},
  keywords = {data mining,machine learning,model averaging,random forests,species distributions},
  file = {/home/bolker/Zotero/storage/G7UUPE4K/Elith et al. - 2008 - A working guide to boosted regression trees.pdf;/home/bolker/Zotero/storage/NNH3GMC4/j.1365-2656.2008.01390.html}
}

@article{bienLasso2013,
  title = {A Lasso for Hierarchical Interactions},
  author = {Bien, Jacob and Taylor, Jonathan and Tibshirani, Robert},
  year = {2013},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {41},
  number = {3},
  pages = {1111--1141},
  issn = {0090-5364},
  doi = {10.1214/13-AOS1096},
  abstract = {We add a set of convex constraints to the lasso to produce sparse interaction models that honor the hierarchy restriction that an interaction only be included in a model if one or both variables are marginally important. We give a precise characterization of the effect of this hierarchy constraint, prove that hierarchy holds with probability one and derive an unbiased estimate for the degrees of freedom of our estimator. A bound on this estimate reveals the amount of fitting ``saved'' by the hierarchy constraint.},
  langid = {english}
}

@article{friedmanPathwise2007,
  title = {Pathwise Coordinate Optimization},
  author = {Friedman, Jerome and Hastie, Trevor and H{\"o}fling, Holger and Tibshirani, Robert},
  year = {2007},
  month = dec,
  journal = {Annals of Applied Statistics},
  volume = {1},
  number = {2},
  pages = {302--332},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/07-AOAS131},
  abstract = {We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the L1-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the ``fused lasso,'' however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems.},
  langid = {english},
  mrnumber = {MR2415737},
  zmnumber = {1378.90064},
  keywords = {convex optimization,Coordinate descent,lasso},
  file = {/home/bolker/Zotero/storage/A6RT6MVR/Friedman et al. - 2007 - Pathwise coordinate optimization.pdf;/home/bolker/Zotero/storage/6PL3GMCT/1196438020.html}
}

@article{wittenPenalized2009,
  title = {A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis},
  author = {Witten, Daniela M. and Tibshirani, Robert and Hastie, Trevor},
  year = {2009},
  month = jan,
  journal = {Biostatistics},
  pages = {kxp008},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxp008},
  abstract = {We present a penalized matrix decomposition (PMD), a new framework for computing a rank-K approximation for a matrix. We approximate the matrix X as , where dk, uk, and vk minimize the squared Frobenius norm of X, subject to penalties on uk and vk. This results in a regularized version of the singular value decomposition. Of particular interest is the use of L1-penalties on uk and vk, which yields a decomposition of X using sparse vectors. We show that when the PMD is applied using an L1-penalty on vk but not on uk, a method for sparse principal components results. In fact, this yields an efficient algorithm for the ``SCoTLASS'' proposal (Jolliffe and others 2003) for obtaining sparse principal components. This method is demonstrated on a publicly available gene expression data set. We also establish connections between the SCoTLASS method for sparse principal component analysis and the method of Zou and others (2006). In addition, we show that when the PMD is applied to a cross-products matrix, it results in a method for penalized canonical correlation analysis (CCA). We apply this penalized CCA method to simulated data and to a genomic data set consisting of gene expression and DNA copy number measurements on the same set of samples.},
  langid = {english},
  pmid = {19377034},
  keywords = {Canonical correlation analysis,DNA copy number,Integrative genomic analysis,L1,Matrix decomposition,Principal component analysis,Sparse principal component analysis,SVD},
  file = {/home/bolker/Zotero/storage/8DFHTXWR/Witten et al. - 2009 - A penalized matrix decomposition, with application.pdf;/home/bolker/Zotero/storage/9CHTHTAV/biostatistics.kxp008.html}
}


@article{hastieRidge2020,
  title = {Ridge {{Regularization}}: {{An Essential Concept}} in {{Data Science}}},
  shorttitle = {Ridge {{Regularization}}},
  author = {Hastie, Trevor},
  year = {2020},
  month = oct,
  journal = {Technometrics},
  volume = {62},
  number = {4},
  pages = {426--433},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2020.1791959},
  abstract = {Ridge or more formally l2 regularization shows up in many areas of statistics and machine learning. It is one of those essential devices that any good data scientist needs to master for their craft. In this brief ridge fest, I have collected together some of the magic and beauty of ridge that my colleagues and I have encountered over the past 40 years in applied statistics.},
  keywords = {Data science,Retrospective,Ridge regression},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2020.1791959},
  file = {/home/bolker/Zotero/storage/732VNZZ9/Hastie - 2020 - Ridge Regularization An Essential Concept in Data.pdf}
}


@book{nazarathyStatistics2021,
  title = {Statistics with {Julia}: Fundamentals for Data Science, Machine Learning and Artificial Intelligence},
  shorttitle = {Statistics with {{Julia}}},
  author = {Nazarathy, Yoni and Klok, Hayden},
  year = {2021},
  series = {Springer {{Series}} in the {{Data Sciences}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-70901-3},
  isbn = {978-3-030-70900-6 978-3-030-70901-3},
  langid = {english}
}

@misc{navarroScience2019,
  title = {Science and Statistics},
  author = {Navarro, Danielle},
  year = {2019},
  month = mar,
  address = {{Aarhus University}},
  abstract = {http://interactingminds.au.dk/events/single-events/artikel/2-day-workshop-open-science-and-reproducibility/},
  langid = {english},
  file = {/home/bolker/Zotero/storage/UX69QA8G/scienceandstatistics.html},
  url = {https://slides.com/djnavarro/scienceandstatistics}
}

@misc{adarValue2015,
  title = {On the Value of Command-Line ``Bullshittery''},
  author = {Adar, Eytan},
  year = {2015},
  month = oct,
  journal = {Medium},
  abstract = {Go read this first for context. Done? Good.},
  url = {https://medium.com/@eytanadar/on-the-value-of-command-line-bullshittery-94dc19ec8c61},
  langid = {english},
  file = {/home/bolker/Zotero/storage/VFLDYT2H/on-the-value-of-command-line-bullshittery-94dc19ec8c61.html}
}

@book{shaliziAdvanced2022,
  title = {Advanced Data Analysis from an Elementary Point of View},
  author = {Shalizi, Cosma Rohilla},
  year = {2022},
  url = {https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/}
}

@misc{lindelovCommon2019,
  title = {Common Statistical Tests Are Linear Models (or: How to Teach Stats)},
  author = {Lindeløv, Jonas Kristoffer},
  year = {2019},
  month = jun,
  url = {https://lindeloev.github.io/tests-as-linear/},
  file = {/home/bolker/Documents/zotero_new/storage/YEWVY7VE/tests-as-linear.html}
}

@article{shaferTutorial2008,
  title = {A Tutorial on Conformal Prediction},
  author = {Shafer, Glenn and Vovk, Vladimir},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  pages = {371--421},
  abstract = {Conformal prediction uses past experience to determine precise levels of confidence in new predictions. Given an error probability ε, together with a method that makes a prediction yˆ of a label y, it produces a set of labels, typically containing yˆ, that also contains y with probability 1 − ε. Conformal prediction can be applied to any method for producing yˆ: a nearest-neighbor method, a support-vector machine, ridge regression, etc.},
  langid = {english},
  url = {https://jmlr.csail.mit.edu/papers/volume9/shafer08a/shafer08a.pdf},
  file = {/home/bolker/Documents/zotero_new/storage/V4UKY2P8/Shafer and Vovk - A Tutorial on Conformal Prediction.pdf}
}

@article{dezeureHighDimensional2015a,
  title = {High-Dimensional Inference: Confidence Intervals, p-values and {R} software hdi},
  shorttitle = {High-{{Dimensional Inference}}},
  author = {Dezeure, Ruben and Bühlmann, Peter and Meier, Lukas and Meinshausen, Nicolai},
  year = {2015},
  month = nov,
  journal = {Statistical Science},
  volume = {30},
  number = {4},
  pages = {533--558},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/15-STS527},
  abstract = {We present a (selective) review of recent frequentist high-dimensional inference methods for constructing ppp-values and confidence intervals in linear and generalized linear models. We include a broad, comparative empirical study which complements the viewpoint from statistical methodology and theory. Furthermore, we introduce and illustrate the R-package hdi which easily allows the use of different methods and supports reproducibility.},
  langid = {english},
  mrnumber = {MR3432840},
  zmnumber = {06946201},
  keywords = {$p$-value,Clustering,confidence interval,generalized linear model,high-dimensional statistical inference,linear model,multiple testing,R-software},
  note = {https://projecteuclid.org/euclid.ss/1449670857},
  file = {/home/bolker/Documents/zotero_new/storage/X6QR9YRM/Dezeure et al. - 2015 - High-Dimensional Inference Confidence Intervals, .pdf;/home/bolker/Documents/zotero_new/storage/4Q8TUABS/1449670857.html}
}

@article{andersonirises1935,
  title = {The Irises of the {{Gaspe}} Peninsula},
  author = {Anderson, Edgar},
  year = {1935},
  journal = {Bull. Am. Iris Soc.},
  volume = {59},
  pages = {2--5},
  note = {https://www.biodiversitylibrary.org/item/270486\#page/344/mode/1up},
  file = {/home/bolker/Documents/zotero_new/storage/ZD6Q5SPC/1571980073972926080.html}
}

@article{andersonSpecies1936,
  title = {The {{Species Problem}} in {{Iris}}},
  author = {Anderson, Edgar},
  year = {1936},
  journal = {Annals of the Missouri Botanical Garden},
  volume = {23},
  number = {3},
  pages = {457--509},
  publisher = {{Missouri Botanical Garden Press}},
  issn = {0026-6493},
  doi = {10.2307/2394164},
  note = {https://www.jstor.org/stable/2394164},
  file = {/home/bolker/Documents/zotero_new/storage/IXF3AU76/Anderson - 1936 - The Species Problem in Iris.pdf}
}

@article{bezdekWill1999,
  title = {Will the Real Iris Data Please Stand Up?},
  author = {Bezdek, J.C. and Keller, J.M. and Krishnapuram, R. and Kuncheva, L.I. and Pal, N.R.},
  year = {1999},
  month = jun,
  journal = {IEEE Transactions on Fuzzy Systems},
  volume = {7},
  number = {3},
  pages = {368--369},
  issn = {1941-0034},
  doi = {10.1109/91.771092},
  abstract = {This correspondence points out several published errors in replicates of the well-known iris data, which was collected by Anderson (1935) but first published by Fisher (1936).},
  keywords = {Books,Computer science,Databases,Face detection,Iris,Machine learning,Mathematics,Pattern recognition,Testing,Writing},
  file = {/home/bolker/Documents/zotero_new/storage/ZLN3I9XM/771092.html}
}

@article{bodmeroutstanding2021,
  title = {The Outstanding Scientist, {R}.{A}. {Fisher}: His Views on Eugenics and Race},
  shorttitle = {The Outstanding Scientist, {{R}}.{{A}}. {{Fisher}}},
  author = {Bodmer, Walter and Bailey, R. A. and Charlesworth, Brian and {Eyre-Walker}, Adam and Farewell, Vernon and Mead, Andrew and Senn, Stephen},
  year = {2021},
  month = apr,
  journal = {Heredity},
  volume = {126},
  number = {4},
  pages = {565--576},
  publisher = {{Nature Publishing Group}},
  issn = {1365-2540},
  doi = {10.1038/s41437-020-00394-6},
  copyright = {2021 The Author(s), under exclusive licence to The Genetics Society},
  langid = {english},
  keywords = {Evolution,Genetics},
  note = {https://www.nature.com/articles/s41437-020-00394-6},
  file = {/home/bolker/Documents/zotero_new/storage/NMQEVV64/Bodmer et al. - 2021 - The outstanding scientist, R.A. Fisher his views .pdf}
}

@article{fisherUse1936,
  title = {The {{Use}} of {{Multiple Measurements}} in {{Taxonomic Problems}}},
  author = {Fisher, R. A.},
  year = {1936},
  journal = {Annals of Eugenics},
  volume = {7},
  number = {2},
  pages = {179--188},
  issn = {2050-1439},
  doi = {10.1111/j.1469-1809.1936.tb02137.x},
  abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
  note = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
  file = {/home/bolker/Documents/zotero_new/storage/ZILQV7TS/Fisher - 1936 - The Use of Multiple Measurements in Taxonomic Prob.pdf;/home/bolker/Documents/zotero_new/storage/CJ6BXN78/j.1469-1809.1936.tb02137.html}
}

@misc{gelmantypical2020,
  title = {The Typical Set and Its Relevance to {{Bayesian}} Computation},
  author = {Gelman, Andrew},
  year = {2020},
  month = aug,
  journal = {Statistical Modeling, Causal Inference, and Social Science},
  howpublished = {https://statmodeling.stat.columbia.edu/2020/08/02/the-typical-set-and-its-relevance-to-bayesian-computation/},
  file = {/home/bolker/Documents/zotero_new/storage/UGQTPXZI/the-typical-set-and-its-relevance-to-bayesian-computation.html}
}

@article{waltersEffects1981,
  title = {Effects of {{Measurement Errors}} on the {{Assessment}} of {{Stock}}–{{Recruitment Relationships}}},
  author = {Walters, Carl J. and Ludwig, Donald},
  year = {1981},
  month = jun,
  journal = {Canadian Journal of Fisheries and Aquatic Sciences},
  volume = {38},
  number = {6},
  pages = {704--710},
  issn = {0706-652X},
  doi = {10.1139/f81-093},
  abstract = {Errors in measuring spawning stocks can have a profound effect on the appearance of stock–recruitment relationships. Large errors make recruitments appear to be independent of spawning stocks. This effect promotes overexploitation rather than simply making the relationship noisier and harder to measure. Efforts to explain variation in recruitment through factors other than spawning stock may be deceptive as well. Much fisheries theory and practice are based on the assumption that recruitment is largely independent of spawning stock; that assumption is not credible except in cases where spawning stocks are known to have been measured accurately.Key words: stock, recruitment, statistics, overexploitation, Les relations stock–recrutement peuvent être profondément influencées par des erreurs de mesures des stocks reproducteurs. De grandes erreurs font que le recrutement semble être indépendant des stocks reproducteurs. Cet effet encourage la surexploitation plutôt que de simplement rendre la relation plus criante et plus difficile à mesurer. Il peut être tout aussi trompeur de faire appel à des facteurs autres que le stock reproducteur pour expliquer la variation du recrutement. La théorie et la pratique des pêches reposent surtout sur l'hypothèse que le recrutement est pour une bonne part indépendant du stock reproducteur; cette hypothèse manque de crédibilité, à moins qu'on ne soit certain que les stocks ont été mesurés avec précision.},
  note = {http://www.nrcresearchpress.com/doi/abs/10.1139/f81-093},
  file = {/home/bolker/Documents/zotero_new/storage/WBG2T75Q/Walters_Ludwig_1981_Effects of Measurement Errors on the Assessment of Stock–Recruitment.pdf;/home/bolker/Documents/zotero_new/storage/T9Q5TZP3/f81-093.html}
}

@misc{bryan_project-oriented_2017,
	title = {Project-oriented workflow},
	url = {https://www.tidyverse.org/blog/2017/12/workflow-vs-script/},
	abstract = {Advice on workflows for developing R scripts. How to think about whether an action belongs in the script or elsewhere.},
	language = {en-us},
	urldate = {2021-01-14},
	journal = {Tidyverse},
	author = {Bryan, Jenny},
	month = dec,
	year = {2017}
}




@inproceedings{jovicreview2015,
	title = {A review of feature selection methods with applications},
	doi = {10.1109/MIPRO.2015.7160458},
	abstract = {Feature selection (FS) methods can be used in data pre-processing to achieve efficient data reduction. This is useful for finding accurate data models. Since exhaustive search for optimal feature subset is infeasible in most cases, many search strategies have been proposed in literature. The usual applications of FS are in classification, clustering, and regression tasks. This review considers most of the commonly used FS techniques. Particular emphasis is on the application aspects. In addition to standard filter, wrapper, and embedded methods, we also provide insight into FS for recent hybrid approaches and other advanced topics.},
	booktitle = {38th {International} {Convention} on {Information} and {Communication} {Technology}, {Electronics} and {Microelectronics} ({MIPRO})},
		author = {Jović, A. and Brkić, K. and Bogunović, N.},
	month = may,
	year = {2015},
	keywords = {Accuracy, Classification algorithms, Clustering algorithms, Filtering algorithms, Information filters, Search problems},
	pages = {1200--1205}
}


@article{pengFeature2005,
	title = {Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy},
	volume = {27},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2005.159},
	abstract = {Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Peng, Hanchuan and Long, Fuhui and Ding, C.},
	month = aug,
	year = {2005},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Algorithm design and analysis, Cancer, classification., Costs, Diversity reception, Index Terms- Feature selection, maximal dependency, maximal relevance, minimal redundancy, mutual information, Mutual information, Pattern classification, Performance analysis, Redundancy, Support vector machine classification, Support vector machines},
	pages = {1226--1238},
	file = {IEEE Xplore Abstract Record:/home/bolker/Documents/zotero_new/storage/9J9HEHAK/1453511.html:text/html},
}


@article{murtaughPerformance2009,
	title = {Performance of several variable-selection methods applied to real ecological data},
	volume = {12},
	issn = {1461023X, 14610248},
	url = {http://doi.wiley.com/10.1111/j.1461-0248.2009.01361.x},
	doi = {10.1111/j.1461-0248.2009.01361.x},
	language = {en},
	number = {10},
	urldate = {2015-07-11},
	journal = {Ecology Letters},
	author = {Murtaugh, Paul A.},
	month = oct,
	year = {2009},
	pages = {1061--1068},
	file = {Murtaugh 2008.pdf:/home/bolker/Documents/zotero_new/storage/FE4V8ES4/Murtaugh 2008.pdf:application/pdf},
}

@article{dahlgrenAlternative2010,
	title = {Alternative regression methods are not considered in {Murtaugh} (2009) or by ecologists in general},
	volume = {13},
	issn = {1461-0248},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1461-0248.2010.01460.x/abstract},
	doi = {10.1111/j.1461-0248.2010.01460.x},
	abstract = {Ecology Letters (2010) 13: E7–E9 
Abstract
Murtaugh (2009) recently illustrated that all subsets variable selection is very similar to stepwise regression. This, however, does not necessarily mean both methods are useful. On the contrary, the same problems with overfitting should apply. Ecologists should, if model building is indeed necessary, consider more reliable regression methods now available.},
	language = {en},
	number = {5},
	urldate = {2016-04-14},
	journal = {Ecology Letters},
	author = {Dahlgren, Johan P.},
	month = may,
	year = {2010},
	keywords = {Shrinkage, Variable selection, AIC, BIC, lasso, all subsets, ridge regression, stepwise multiple regression},
	pages = {E7--E9},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/ZKQHERDI/abstract.html:text/html},
}


@article{blanchetForward2008,
	title = {Forward {Selection} of {Explanatory} {Variables}},
	volume = {89},
	issn = {0012-9658},
	url = {http://www.esajournals.org/doi/abs/10.1890/07-0986.1},
	doi = {10.1890/07-0986.1},
	abstract = {This paper proposes a new way of using forward selection of explanatory variables in regression or canonical redundancy analysis. The classical forward selection method presents two problems: a highly inflated Type I error and an overestimation of the amount of explained variance. Correcting these problems will greatly improve the performance of this very useful method in ecological modeling. To prevent the first problem, we propose a two-step procedure. First, a global test using all explanatory variables is carried out. If, and only if, the global test is significant, one can proceed with forward selection. To prevent overestimation of the explained variance, the forward selection has to be carried out with two stopping criteria: (1) the usual alpha significance level and (2) the adjusted coefficient of multiple determination () calculated using all explanatory variables. When forward selection identifies a variable that brings one or the other criterion over the fixed threshold, that variable is rejected, and the procedure is stopped. This improved method is validated by simulations involving univariate and multivariate response data. An ecological example is presented using data from the Bryce Canyon National Park, Utah, USA.},
	number = {9},
	urldate = {2013-01-03},
	journal = {Ecology},
	author = {Blanchet, F. Guillaume and Legendre, Pierre and Borcard, Daniel},
	month = sep,
	year = {2008},
	keywords = {type I error, simulation study, forward selection, Moran's eigenvector maps (MEM), non-orthogonal explanatory variables, orthogonal explanatory variables, principal coordinates of neighbor matrices (PCNM)},
	pages = {2623--2632},
	file = {ESA Snapshot:/home/bolker/Documents/zotero_new/storage/TEWI8Z5I/showCitFormats.html:text/html},
}



@article{dysonWise2005,
	title = {Wise {Man}},
	issn = {0028-7504},
	url = {https://www.nybooks.com/articles/2005/10/20/wise-man/},
	abstract = {Great scientists come in two varieties, which Isaiah Berlin, quoting the seventh-century-BC poet Archilochus, called foxes and hedgehogs. Foxes know many},
	language = {en},
	urldate = {2023-01-14},
	journal = {New York Review of Books},
	author = {Dyson, Freeman},
	month = oct,
	year = {2005},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/CWBZNU4N/wise-man.html:text/html},
}


@article{stoneAsymptotic1977a,
	title = {An {Asymptotic} {Equivalence} of {Choice} of {Model} by {Cross}-{Validation} and {Akaike}'s {Criterion}},
	volume = {39},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984877},
	abstract = {A logarithmic assessment of the performance of a predicting density is found to lead to asymptotic equivalence of choice of model by cross-validation and Akaike's criterion, when maximum likelihood estimation is used within each model.},
	number = {1},
	urldate = {2023-01-14},
	journal = {J. Royal Stat. Soc. B},
	author = {Stone, M.},
	year = {1977},
	pages = {44--47}
}


@inproceedings{venablesExegeses1998,
	address = {Washington, DC},
	series = {1998 {International} {S}-{PLUS} {User} {Conference}},
	title = {Exegeses on {Linear} {Models}},
	url = {http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf},
	author = {Venables, W. N},
	year = {1998},
}

@misc{atlasQR2013,
	title = {{QR} factorization for ridge regression},
	url = {https://math.stackexchange.com/questions/299481/qr-factorization-for-ridge-regression},
	urldate = {2023-01-15},
	journal = {Mathematics Stack Exchange},
	author = {Atlas},
	month = feb,
	year = {2013},
}


@article{friedmanRegularization2010,
	title = {Regularization {Paths} for {Generalized} {Linear} {Models} via {Coordinate} {Descent}},
	volume = {33},
	issn = {1548-7660},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2929880/},
	abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ1 (the lasso), ℓ2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
	number = {1},
	urldate = {2020-06-25},
	journal = {Journal of statistical software},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
	year = {2010},
	pmid = {20808728},
	pmcid = {PMC2929880},
	pages = {1--22},
	file = {PubMed Central Full Text PDF:/home/bolker/Documents/zotero_new/storage/3P9HUKZH/Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf:application/pdf},
}


@article{hastieRidge2020,
	title = {Ridge {Regularization}: {An} {Essential} {Concept} in {Data} {Science}},
	volume = {62},
	issn = {0040-1706},
	shorttitle = {Ridge {Regularization}},
	url = {https://doi.org/10.1080/00401706.2020.1791959},
	doi = {10.1080/00401706.2020.1791959},
	abstract = {Ridge or more formally l2 regularization shows up in many areas of statistics and machine learning. It is one of those essential devices that any good data scientist needs to master for their craft. In this brief ridge fest, I have collected together some of the magic and beauty of ridge that my colleagues and I have encountered over the past 40 years in applied statistics.},
	number = {4},
	urldate = {2022-11-03},
	journal = {Technometrics},
	author = {Hastie, Trevor},
	month = oct,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2020.1791959},
	keywords = {Data science, Ridge regression, Retrospective},
	pages = {426--433},
	file = {Accepted Version:/home/bolker/Documents/zotero_new/storage/732VNZZ9/Hastie - 2020 - Ridge Regularization An Essential Concept in Data.pdf:application/pdf},
}

@article{giraud-carrierJustification2005,
  title = {Toward a Justification of Meta-Learning: {{Is}} the No Free Lunch Theorem a Show-Stopper?},
  shorttitle = {Toward a Justification of Meta-Learning},
  author = {{Giraud-Carrier}, Christophe and Provost, Foster},
  year = {2005},
  month = jan,
  journal = {Proceedings of the ICML-2005 Workshop on Meta-learning},
  abstract = {We present a preliminary analysis of the fun-damental viability of meta-learning, revisit-ing the No Free Lunch (NFL) theorem. The analysis shows that given some simple and very basic assumptions, the NFL theorem is of little relevance to research in Machine Learning. We augment the basic NFL frame-work to illustrate that the notion of an Ulti-mate Learning Algorithm is well defined. We show that, although cross-validation still is not a viable way to construct general-purpose learning algorithms, meta-learning offers a natural alternative. We still have to pay for our lunch, but the cost is reasonable: the nec-essary fundamental assumptions are ones we all make anyway.},
  file = {/home/bolker/Zotero/storage/GJH45PRV/Giraud-Carrier and Provost - 2005 - Toward a justification of meta-learning Is the no.pdf}
}

@article{wolpertNo1997,
  title = {No Free Lunch Theorems for Optimization},
  author = {Wolpert, D.H. and Macready, W.G.},
  year = {1997},
  month = apr,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {67--82},
  issn = {1941-0026},
  doi = {10.1109/4235.585893},
  abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of "no free lunch" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori "head-to-head" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
  keywords = {Algorithm design and analysis,Bayesian methods,Evolutionary computation,Information theory,Iron,Minimax techniques,Performance analysis,Probability distribution,Simulated annealing},
  file = {/home/bolker/Zotero/storage/NM8P77V4/Wolpert and Macready - 1997 - No free lunch theorems for optimization.pdf}
}

@misc{bremen79Neural2020,
  title = {Neural {{Networks}} ({{Maybe}}) {{Evolved}} to {{Make Adam The Best Optimizer}}},
  author = {{bremen79}},
  year = {2020},
  month = dec,
  journal = {Parameter-free Learning and Optimization Algorithms},
  abstract = {Disclaimer: This post will be a little different than my usual ones. In fact, I won't prove anything and I will just briefly explain some of my conjectures around optimization in deep neural \ldots},
  langid = {english},
  file = {/home/bolker/Zotero/storage/5NGYSYMT/neural-network-maybe-evolved-to-make-adam-the-best-optimizer.html}
}




@article{zhaoDefense2021,
	title = {In Defense of the Indefensible: A Very Naïve Approach to High-Dimensional Inference},
	volume = {36},
	issn = {0883-4237, 2168-8745},
	shorttitle = {In {Defense} of the {Indefensible}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-36/issue-4/In-Defense-of-the-Indefensible--A-Very-Na%c3%afve-Approach/10.1214/20-STS815.full},
	doi = {10.1214/20-STS815},
	abstract = {A great deal of interest has recently focused on conducting inference on the parameters in a high-dimensional linear model. In this paper, we consider a simple and very naïve two-step procedure for this task, in which we (i) fit a lasso model in order to obtain a subset of the variables, and (ii) fit a least squares model on the lasso-selected set. Conventional statistical wisdom tells us that we cannot make use of the standard statistical inference tools for the resulting least squares model (such as confidence intervals and p-values), since we peeked at the data twice: once in running the lasso, and again in fitting the least squares model. However, in this paper, we show that under a certain set of assumptions, with high probability, the set of variables selected by the lasso is identical to the one selected by the noiseless lasso and is hence deterministic. Consequently, the naïve two-step approach can yield asymptotically valid inference. We utilize this finding to develop the naïve confidence interval, which can be used to draw inference on the regression coefficients of the model selected by the lasso, as well as the naïve score test, which can be used to test the hypotheses regarding the full-model regression coefficients.},
	number = {4},
	urldate = {2023-01-23},
	journal = {Statistical Science},
	author = {Zhao, Sen and Witten, Daniela and Shojaie, Ali},
	month = nov,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Confidence interval, Lasso, p-value, Post-selection inference, Significance testing},
	pages = {562--577},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/GGVB6L9Q/Zhao et al. - 2021 - In Defense of the Indefensible A Very Naïve Appro.pdf:application/pdf},
}


@article{yuanModel2006,
	title = {Model selection and estimation in regression with grouped variables},
	volume = {68},
	issn = {1369-7412, 1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00532.x},
	doi = {10.1111/j.1467-9868.2005.00532.x},
	abstract = {We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efﬁcient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
	language = {en},
	number = {1},
	urldate = {2023-01-22},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Yuan, Ming and Lin, Yi},
	month = feb,
	year = {2006},
	pages = {49--67},
	file = {Yuan and Lin - 2006 - Model selection and estimation in regression with .pdf:/home/bolker/Documents/zotero_new/storage/WPB8ARPT/Yuan and Lin - 2006 - Model selection and estimation in regression with .pdf:application/pdf},
}

@misc{mountHow2012,
  title = {How Robust Is Logistic Regression?},
  author = {Mount, John},
  year = {2012},
  month = aug,
  journal = {Win Vector LLC},
  abstract = {Logistic Regression is a popular and effective technique for modeling categorical outcomes as a function of both continuous and categorical variables. The question is: how robust is it? Or: how rob\ldots},
  howpublished = {https://win-vector.com/2012/08/23/how-robust-is-logistic-regression/},
  langid = {american},
  file = {/home/bolker/Zotero/storage/DYBG6VD8/how-robust-is-logistic-regression.html}
}

@article{leeProximal2014,
	title = {Proximal {Newton}-{Type} {Methods} for {Minimizing} {Composite} {Functions}},
	volume = {24},
	issn = {1052-6234, 1095-7189},
	url = {http://epubs.siam.org/doi/10.1137/130921428},
	doi = {10.1137/130921428},
	abstract = {We generalize Newton-type methods for minimizing smooth functions to handle a sum of two convex functions: a smooth function and a nonsmooth function with a simple proximal mapping. We show that the resulting proximal Newton-type methods inherit the desirable convergence behavior of Newton-type methods for minimizing smooth functions, even when search directions are computed inexactly. Many popular methods tailored to problems arising in bioinformatics, signal processing, and statistical learning are special cases of proximal Newton-type methods, and our analysis yields new convergence results for some of these methods.},
	language = {en},
	number = {3},
	urldate = {2023-02-01},
	journal = {SIAM Journal on Optimization},
	author = {Lee, Jason D. and Sun, Yuekai and Saunders, Michael A.},
	month = jan,
	year = {2014},
	pages = {1420--1443},
	file = {Lee et al. - 2014 - Proximal Newton-Type Methods for Minimizing Compos.pdf:/home/bolker/Documents/zotero_new/storage/AI5AKFX5/Lee et al. - 2014 - Proximal Newton-Type Methods for Minimizing Compos.pdf:application/pdf},
}


@article{perperogloureview2019a,
	title = {A review of spline function procedures in {R}},
	volume = {19},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-019-0666-3},
	doi = {10.1186/s12874-019-0666-3},
	abstract = {With progress on both the theoretical and the computational fronts the use of spline modelling has become an established tool in statistical regression analysis. An important issue in spline modelling is the availability of user friendly, well documented software packages. Following the idea of the STRengthening Analytical Thinking for Observational Studies initiative to provide users with guidance documents on the application of statistical methods in observational research, the aim of this article is to provide an overview of the most widely used spline-based techniques and their implementation in R.},
	number = {1},
	urldate = {2023-02-01},
	journal = {BMC Medical Research Methodology},
	author = {Perperoglou, Aris and Sauerbrei, Willi and Abrahamowicz, Michal and Schmid, Matthias},
	month = mar,
	year = {2019},
	keywords = {Functional form of continuous covariates, Multivariable modelling},
	pages = {46},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/6RVM58RB/Perperoglou et al. - 2019 - A review of spline function procedures in R.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/YHYJLI4J/s12874-019-0666-3.html:text/html},
}

@article{golubGeneralized1979,
  title = {Generalized {{Cross-Validation}} as a {{Method}} for {{Choosing}} a {{Good Ridge Parameter}}},
  author = {Golub, Gene H. and Heath, Michael and Wahba, Grace},
  year = {1979},
  month = may,
  journal = {Technometrics},
  volume = {21},
  number = {2},
  pages = {215--223},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1979.10489751},
  abstract = {Consider the ridge estimate ({$\lambda$}) for {$\beta$} in the model unknown, ({$\lambda$}) = (X T X + n{$\lambda$}I)-1 X T y. We study the method of generalized cross-validation (GCV) for choosing a good value for {$\lambda$} from the data. The estimate is the minimizer of V({$\lambda$}) given by where A({$\lambda$}) = X(X T X + n{$\lambda$}I)-1 X T . This estimate is a rotation-invariant version of Allen's PRESS, or ordinary cross-validation. This estimate behaves like a risk improvement estimator, but does not require an estimate of {$\sigma$}2, so can be used when n - p is small, or even if p {$\geq$} 2 n in certain cases. The GCV method can also be used in subset selection and singular value truncation methods for regression, and even to choose from among mixtures of these methods.},
  keywords = {Cross-validation,Ridge parameter,Ridge regression},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1979.10489751},
  file = {/home/bolker/Zotero/storage/4FG7CYQL/Golub et al. - 1979 - Generalized Cross-Validation as a Method for Choos.pdf}
}

@misc{larsenGAM2015,
  title = {{{GAM}}: {{The Predictive Modeling Silver Bullet}} | {{Stitch Fix Technology}} \textendash{} {{Multithreaded}}},
  shorttitle = {{{GAM}}},
  author = {Larsen, Kim},
  year = {2015},
  month = jul,
  journal = {MultiThreaded (StitchFix)},
  abstract = {Imagine that you step into a room of data scientists; the dress code is casual and the scent of strong coffee is hanging in the air. You ask the data scienti...},
  url = {https://multithreaded.stitchfix.com/blog/2015/07/30/gam/},
  file = {/home/bolker/Zotero/storage/5XYN6F7U/gam.html}
}


@article{el-bachirFast,
	title = {Fast {Automatic} {Smoothing} for {Generalized} {Additive} {Models}},
	abstract = {Generalized additive models (GAMs) are regression models wherein parameters of probability distributions depend on input variables through a sum of smooth functions, whose degrees of smoothness are selected by L2 regularization. Such models have become the de-facto standard nonlinear regression models when interpretability and ﬂexibility are required, but reliable and fast methods for automatic smoothing in large data sets are still lacking. We develop a general methodology for automatically learning the optimal degree of L2 regularization for GAMs using an empirical Bayes approach. The smooth functions are penalized by hyper-parameters that are learned simultaneously by maximization of a marginal likelihood using an approximate expectation-maximization algorithm. The latter involves a double Laplace approximation at the E-step, and leads to an eﬃcient M-step. Empirical analysis shows that the resulting algorithm is numerically stable, faster than the best existing methods and achieves state-of-the-art accuracy. For illustration, we apply it to an important and challenging problem in the analysis of extremal data.},
	language = {en},
	author = {El-Bachir, Yousra and Davison, Anthony C},
	file = {El-Bachir and Davison - Fast Automatic Smoothing for Generalized Additive .pdf:/home/bolker/Documents/zotero_new/storage/EDQVMPTY/El-Bachir and Davison - Fast Automatic Smoothing for Generalized Additive .pdf:application/pdf},
}

@article{marraPractical2011,
	title = {Practical variable selection for generalized additive models},
	volume = {55},
	issn = {0167-9473},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947311000491},
	doi = {10.1016/j.csda.2011.02.004},
	abstract = {The problem of variable selection within the class of generalized additive models, when there are many covariates to choose from but the number of predictors is still somewhat smaller than the number of observations, is considered. Two very simple but effective shrinkage methods and an extension of the nonnegative garrote estimator are introduced. The proposals avoid having to use nonparametric testing methods for which there is no general reliable distributional theory. Moreover, component selection is carried out in one single step as opposed to many selection procedures which involve an exhaustive search of all possible models. The empirical performance of the proposed methods is compared to that of some available techniques via an extensive simulation study. The results show under which conditions one method can be preferred over another, hence providing applied researchers with some practical guidelines. The procedures are also illustrated analysing data on plasma beta-carotene levels from a cross-sectional study conducted in the United States.},
	language = {en},
	number = {7},
	urldate = {2023-02-07},
	journal = {Computational Statistics \& Data Analysis},
	author = {Marra, Giampiero and Wood, Simon N.},
	month = jul,
	year = {2011},
	keywords = {Generalized additive model, Nonnegative garrote estimator, Penalized thin plate regression spline, Practical variable selection, Shrinkage smoother},
	pages = {2372--2387},
	file = {ScienceDirect Full Text PDF:/home/bolker/Documents/zotero_new/storage/XAAWNJJ6/Marra and Wood - 2011 - Practical variable selection for generalized addit.pdf:application/pdf;ScienceDirect Snapshot:/home/bolker/Documents/zotero_new/storage/FS7AT22Y/S0167947311000491.html:text/html},
}

@article{woodFast2011,
	title = {Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models},
	volume = {73},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2010.00749.x},
	doi = {10.1111/j.1467-9868.2010.00749.x},
	abstract = {Summary. Recent work by Reiss and Ogden provides a theoretical basis for sometimes preferring restricted maximum likelihood (REML) to generalized cross-validation (GCV) for smoothing parameter selection in semiparametric regression. However, existing REML or marginal likelihood (ML) based methods for semiparametric generalized linear models (GLMs) use iterative REML or ML estimation of the smoothing parameters of working linear approximations to the GLM. Such indirect schemes need not converge and fail to do so in a non-negligible proportion of practical analyses. By contrast, very reliable prediction error criteria smoothing parameter selection methods are available, based on direct optimization of GCV, or related criteria, for the GLM itself. Since such methods directly optimize properly defined functions of the smoothing parameters, they have much more reliable convergence properties. The paper develops the first such method for REML or ML estimation of smoothing parameters. A Laplace approximation is used to obtain an approximate REML or ML for any GLM, which is suitable for efficient direct optimization. This REML or ML criterion requires that Newton–Raphson iteration, rather than Fisher scoring, be used for GLM fitting, and a computationally stable approach to this is proposed. The REML or ML criterion itself is optimized by a Newton method, with the derivatives required obtained by a mixture of implicit differentiation and direct methods. The method will cope with numerical rank deficiency in the fitted model and in fact provides a slight improvement in numerical robustness on the earlier method of Wood for prediction error criteria based smoothness selection. Simulation results suggest that the new REML and ML methods offer some improvement in mean-square error performance relative to GCV or Akaike's information criterion in most cases, without the small number of severe undersmoothing failures to which Akaike's information criterion and GCV are prone. This is achieved at the same computational cost as GCV or Akaike's information criterion. The new approach also eliminates the convergence failures of previous REML- or ML-based approaches for penalized GLMs and usually has lower computational cost than these alternatives. Example applications are presented in adaptive smoothing, scalar on function regression and generalized additive model selection.},
	language = {en},
	number = {1},
	urldate = {2023-02-07},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Wood, Simon N.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2010.00749.x},
	keywords = {Adaptive smoothing, Generalized additive mixed model, Generalized additive model, Generalized cross-validation, Marginal likelihood, Model selection, Penalized generalized linear model, Penalized regression splines, Restricted maximum likelihood, Scalar on function regression, Stable computation},
	pages = {3--36},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/STZE99H9/Wood - 2011 - Fast stable restricted maximum likelihood and marg.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/YBY9GSA9/j.1467-9868.2010.00749.html:text/html},
}

@article{reissSmoothing2009,
	title = {Smoothing parameter selection for a class of semiparametric linear models},
	volume = {71},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2008.00695.x},
	doi = {10.1111/j.1467-9868.2008.00695.x},
	abstract = {Summary. Spline-based approaches to non-parametric and semiparametric regression, as well as to regression of scalar outcomes on functional predictors, entail choosing a parameter controlling the extent to which roughness of the fitted function is penalized. We demonstrate that the equations determining two popular methods for smoothing parameter selection, generalized cross-validation and restricted maximum likelihood, share a similar form that allows us to prove several results which are common to both, and to derive a condition under which they yield identical values. These ideas are illustrated by application of functional principal component regression, a method for regressing scalars on functions, to two chemometric data sets.},
	language = {en},
	number = {2},
	urldate = {2023-02-07},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Reiss, Philip T. and Ogden, R. Todd},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2008.00695.x},
	keywords = {B-splines, Functional linear model, Functional principal component regression, Generalized cross-validation, Linear mixed model, Roughness penalty},
	pages = {505--523},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/IL7LZIWR/Reiss and Todd Ogden - 2009 - Smoothing parameter selection for a class of semip.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/AP2LYYKQ/j.1467-9868.2008.00695.html:text/html},
}

@article{hastieGeneralized1987a,
	title = {Generalized {Additive} {Models}: {Some} {Applications}},
	volume = {82},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Generalized {Additive} {Models}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1987.10478440},
	doi = {10.1080/01621459.1987.10478440},
	language = {en},
	number = {398},
	urldate = {2023-02-07},
	journal = {Journal of the American Statistical Association},
	author = {Hastie, Trevor and Tibshirani, Robert},
	month = jun,
	year = {1987},
	pages = {371--386},
	file = {Hastie and Tibshirani - 1987 - Generalized Additive Models Some Applications.pdf:/home/bolker/Documents/zotero_new/storage/R64VQXE4/Hastie and Tibshirani - 1987 - Generalized Additive Models Some Applications.pdf:application/pdf},
}


@article{breimanEstimating1985,
	title = {Estimating {Optimal} {Transformations} for {Multiple} {Regression} and {Correlation}},
	volume = {80},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1985.10478157},
	doi = {10.1080/01621459.1985.10478157},
	abstract = {In regression analysis the response variable Y and the predictor variables X 1 …, Xp are often replaced by functions θ(Y) and Ø1(X 1), …, Ø p (Xp ). We discuss a procedure for estimating those functions θ and Ø1, …, Ø p that minimize e 2 = E[θ(Y) — Σ Ø j (Xj )]2/var[θ(Y)], given only a sample (yk , xk1 , …, xkp ), 1 ⩽ k ⩽ N and making minimal assumptions concerning the data distribution or the form of the solution functions. For the bivariate case, p = 1, θ and Ø satisfy ρ = p(θ, Ø) = maxθ,Øρ[θ(Y), Ø(X)], where ρ is the product moment correlation coefficient and ρ is the maximal correlation between X and Y. Our procedure thus also provides a method for estimating the maximal correlation between two variables.},
	number = {391},
	urldate = {2023-02-07},
	journal = {Journal of the American Statistical Association},
	author = {Breiman, Leo and Friedman, Jerome H.},
	month = sep,
	year = {1985},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1985.10478157},
	keywords = {ACE, Smoothing},
	pages = {580--598},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/UEE5F44K/Breiman and Friedman - 1985 - Estimating Optimal Transformations for Multiple Re.pdf:application/pdf},
}


@article{eilersFlexible1996,
	title = {Flexible smoothing with {B}-splines and penalties},
	volume = {11},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-11/issue-2/Flexible-smoothing-with-B-splines-and-penalties/10.1214/ss/1038425655.full},
	doi = {10.1214/ss/1038425655},
	abstract = {B-splines are attractive for nonparametric modelling, but choosing the optimal number and positions of knots is a complex task. Equidistant knots can be used, but their small and discrete number allows only limited control over smoothness and fit. We propose to use a relatively large number of knots and a difference penalty on coefficients of adjacent B-splines. We show connections to the familiar spline penalty on the integral of the squared second derivative. A short overview of B-splines, of their construction and of penalized likelihood is presented. We discuss properties of penalized B-splines and propose various criteria for the choice of an optimal penalty parameter. Nonparametric logistic regression, density estimation and scatterplot smoothing are used as examples. Some details of the computations are presented.},
	number = {2},
	urldate = {2023-02-12},
	journal = {Statistical Science},
	author = {Eilers, Paul H. C. and Marx, Brian D.},
	month = may,
	year = {1996},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Density estimation, generalized linear models, nonparametric models, smoothing, splines},
	pages = {89--121},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/YHR93PJA/Eilers and Marx - 1996 - Flexible smoothing with B-splines and penalties.pdf:application/pdf},
}


@article{woodPsplines2017,
	title = {P-splines with derivative based penalties and tensor product smoothing of unevenly distributed data},
	volume = {27},
	issn = {0960-3174, 1573-1375},
	url = {http://arxiv.org/abs/1605.02446},
	doi = {10.1007/s11222-016-9666-x},
	abstract = {The P-splines of Eilers and Marx (1996) combine a B-spline basis with a discrete quadratic penalty on the basis coefﬁcients, to produce a reduced rank spline like smoother. P-splines have three properties that make them very popular as reduced rank smoothers: i) the basis and the penalty are sparse, enabling efﬁcient computation, especially for Bayesian stochastic simulation; ii) it is possible to ﬂexibly ‘mix-and-match’ the order of B-spline basis and penalty, rather than the order of penalty controlling the order of the basis as in spline smoothing; iii) it is very easy to set up the Bspline basis functions and penalties. The discrete penalties are somewhat less interpretable in terms of function shape than the traditional derivative based spline penalties, but tend towards penalties proportional to traditional spline penalties in the limit of large basis size. However part of the point of P-splines is not to use a large basis size. In addition the spline basis functions arise from solving functional optimization problems involving derivative based penalties, so moving to discrete penalties for smoothing may not always be desirable. The purpose of this note is to point out that the three properties of basis-penalty sparsity, mix-and-match penalization and ease of setup are readily obtainable with B-splines subject to derivative based penalization. The penalty setup typically requires a few lines of code, rather than the two lines typically required for P-splines, but this one off disadvantage seems to be the only one associated with using derivative based penalties. As an example application, it is shown how basis-penalty sparsity enables efﬁcient computation with tensor product smoothers of scattered data.},
	language = {en},
	number = {4},
	urldate = {2023-02-12},
	journal = {Statistics and Computing},
	author = {Wood, Simon N.},
	month = jul,
	year = {2017},
	note = {arXiv:1605.02446 [stat]},
	keywords = {Statistics - Computation},
	pages = {985--989},
	file = {Wood - 2017 - P-splines with derivative based penalties and tens.pdf:/home/bolker/Documents/zotero_new/storage/PNFIMTZ5/Wood - 2017 - P-splines with derivative based penalties and tens.pdf:application/pdf},
}

@article{wandPenalized2011,
	title = {Penalized wavelets: {Embedding} wavelets into semiparametric regression},
	volume = {5},
	issn = {1935-7524},
	shorttitle = {Penalized wavelets},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-5/issue-none/Penalized-wavelets-Embedding-wavelets-into-semiparametric-regression/10.1214/11-EJS652.full},
	doi = {10.1214/11-EJS652},
	abstract = {We introduce the concept of penalized wavelets to facilitate seamless embedding of wavelets into semiparametric regression models. In particular, we show that penalized wavelets are analogous to penalized splines; the latter being the established approach to function estimation in semiparametric regression. They differ only in the type of penalization that is appropriate. This fact is not borne out by the existing wavelet literature, where the regression modelling and ﬁtting issues are overshadowed by computational issues such as efﬁciency gains afforded by the Discrete Wavelet Transform and partially obscured by a tendency to work in the wavelet coefﬁcient space. With penalized wavelet structure in place, we then show that ﬁtting and inference can be achieved via the same general approaches used for penalized splines: penalized least squares, maximum likelihood and best prediction within a frequentist mixed model framework, and Markov chain Monte Carlo and mean ﬁeld variational Bayes within a Bayesian framework. Penalized wavelets are also shown have a close relationship with wide data (“p n”) regression and beneﬁt from ongoing research on that topic.},
	language = {en},
	number = {none},
	urldate = {2022-12-28},
	journal = {Electronic Journal of Statistics},
	author = {Wand, M.P. and Ormerod, J.T.},
	month = jan,
	year = {2011},
	file = {Wand and Ormerod - 2011 - Penalized wavelets Embedding wavelets into semipa.pdf:/home/bolker/Documents/zotero_new/storage/L9Y7CTJ2/Wand and Ormerod - 2011 - Penalized wavelets Embedding wavelets into semipa.pdf:application/pdf},
}

@article{donohoWavelet1995,
	title = {Wavelet {Shrinkage}: {Asymptopia}?},
	volume = {57},
	issn = {2517-6161},
	shorttitle = {Wavelet {Shrinkage}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02032.x},
	doi = {10.1111/j.2517-6161.1995.tb02032.x},
	abstract = {Much recent effort has sought asymptotically minimax methods for recovering infinite dimensional objects—curves, densities, spectral densities, images—from noisy data. A now rich and complex body of work develops nearly or exactly minimax estimators for an array of interesting problems. Unfortunately, the results have rarely moved into practice, for a variety of reasons—among them being similarity to known methods, computational intractability and lack of spatial adaptivity. We discuss a method for curve estimation based on n noisy data: translate the empirical wavelet coefficients towards the origin by an amount √(2 log n)σ/√n. The proposal differs from those in current use, is computationally practical and is spatially adaptive; it thus avoids several of the previous objections. Further, the method is nearly minimax both for a wide variety of loss functions—pointwise error, global error measured in Lp-norms, pointwise and global error in estimation of derivatives—and for a wide range of smoothness classes, including standard Holder and Sobolev classes, and bounded variation. This is a much broader near optimality than anything previously proposed: we draw loose parallels with near optimality in robustness and also with the broad near eigenfunction properties of wavelets themselves. Finally, the theory underlying the method is interesting, as it exploits a correspondence between statistical questions and questions of optimal recovery and information-based complexity.},
	language = {en},
	number = {2},
	urldate = {2022-12-26},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Donoho, David L. and Johnstone, Iain M. and Kerkyacharian, Gérard and Picard, Dominique},
	year = {1995},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1995.tb02032.x},
	keywords = {adaptive estimation, besov spaces, density estimation, minimax estimation, nonparametric regression, optimal recovery, spatial adaptation, wavelet orthonormal bases},
	pages = {301--337},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/7UGB74ZK/Donoho et al. - 1995 - Wavelet Shrinkage Asymptopia.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/6KWC8CMX/j.2517-6161.1995.tb02032.html:text/html},
}

@article{abramovichWavelet1998,
	title = {Wavelet thresholding via a {Bayesian} approach},
	volume = {60},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00151},
	doi = {10.1111/1467-9868.00151},
	abstract = {We discuss a Bayesian formalism which gives rise to a type of wavelet threshold estimation in nonparametric regression. A prior distribution is imposed on the wavelet coefficients of the unknown response function, designed to capture the sparseness of wavelet expansion that is common to most applications. For the prior specified, the posterior median yields a thresholding procedure. Our prior model for the underlying function can be adjusted to give functions falling in any specific Besov space. We establish a relationship between the hyperparameters of the prior model and the parameters of those Besov spaces within which realizations from the prior will fall. Such a relationship gives insight into the meaning of the Besov space parameters. Moreover, the relationship established makes it possible in principle to incorporate prior knowledge about the function's regularity properties into the prior model for its wavelet coefficients. However, prior knowledge about a function's regularity properties might be difficult to elicit; with this in mind, we propose a standard choice of prior hyperparameters that works well in our examples. Several simulated examples are used to illustrate our method, and comparisons are made with other thresholding methods. We also present an application to a data set that was collected in an anaesthesiological study.},
	language = {en},
	number = {4},
	urldate = {2022-12-26},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Abramovich, F. and Sapatinas, T. and Silverman, B. W.},
	year = {1998},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00151},
	keywords = {Nonparametric regression, Adaptive estimation, Anaesthetics, Bayes model, Besov spaces, Thresholding, Wavelet transform},
	pages = {725--749},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/NL5QH5UZ/1467-9868.html:text/html},
}

@article{barberReal2004,
	title = {Real nonparametric regression using complex wavelets},
	volume = {66},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2004.B5604.x},
	doi = {10.1111/j.1467-9868.2004.B5604.x},
	abstract = {Summary. Wavelet shrinkage is an effective nonparametric regression technique, especially when the underlying curve has irregular features such as spikes or discontinuities. The basic idea is simple: take the discrete wavelet transform of data consisting of a signal corrupted by noise; shrink or remove the wavelet coefficients to remove the noise; then invert the discrete wavelet transform to form an estimate of the true underlying curve. Various researchers have proposed increasingly sophisticated methods of doing this by using real-valued wavelets. Complex-valued wavelets exist but are rarely used. We propose two new complex-valued wavelet shrinkage techniques: one based on multiwavelet style shrinkage and the other using Bayesian methods. Extensive simulations show that our methods almost always give significantly more accurate estimates than methods based on real-valued wavelets. Further, our multiwavelet style shrinkage method is both simpler and dramatically faster than its competitors. To understand the excellent performance of this method we present a new risk bound on its hard thresholded coefficients.},
	language = {en},
	number = {4},
	urldate = {2022-12-26},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Barber, Stuart and Nason, Guy P.},
	year = {2004},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2004.B5604.x},
	keywords = {Complex normal distribution, Complex-valued wavelets, Curve estimation, Empirical Bayes method, Multiwavelets, Wavelet shrinkage},
	pages = {927--939},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/IZ9AIQ6J/j.1467-9868.2004.B5604.html:text/html},
}


@article{woodThin2003,
	title = {Thin plate regression splines},
	volume = {65},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00374},
	doi = {10.1111/1467-9868.00374},
	abstract = {Summary. I discuss the production of low rank smoothers for d ≥ 1 dimensional data, which can be fitted by regression or penalized regression methods. The smoothers are constructed by a simple transformation and truncation of the basis that arises from the solution of the thin plate spline smoothing problem and are optimal in the sense that the truncation is designed to result in the minimum possible perturbation of the thin plate spline smoothing problem given the dimension of the basis used to construct the smoother. By making use of Lanczos iteration the basis change and truncation are computationally efficient. The smoothers allow the use of approximate thin plate spline models with large data sets, avoid the problems that are associated with ‘knot placement’ that usually complicate modelling with regression splines or penalized regression splines, provide a sensible way of modelling interaction terms in generalized additive models, provide low rank approximations to generalized smoothing spline models, appropriate for use with large data sets, provide a means for incorporating smooth functions of more than one variable into non-linear models and improve the computational efficiency of penalized likelihood models incorporating thin plate splines. Given that the approach produces spline-like models with a sparse basis, it also provides a natural way of incorporating unpenalized spline-like terms in linear and generalized linear models, and these can be treated just like any other model terms from the point of view of model selection, inference and diagnostics.},
	language = {en},
	number = {1},
	urldate = {2023-02-13},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Wood, Simon N.},
	year = {2003},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00374},
	keywords = {Generalized additive model, Regression spline, Thin plate spline},
	pages = {95--114},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/VJKXGIPM/Wood - 2003 - Thin plate regression splines.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/MA6MW9DA/1467-9868.html:text/html},
}
